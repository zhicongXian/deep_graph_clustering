### Starting TaskPrologue of job 2454248 on a0934 at Fri Mar 14 02:54:22 CET 2025
Running on cores 16-31 with governor ondemand
Fri Mar 14 02:54:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:13:00.0 Off |                    0 |
| N/A   34C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

INFO - 03/14/25 02:54:39 - 0:00:00 - {'dataset': 'SeNet', 'task': 'Clustering', 'root_path': './datasets', 'eval_freq': 100, 'exp_iters': 5, 'version': 'run', 'log_path': './results/run/SeNet.log', 'pre_epochs': 1000, 'epochs': 5000, 'height': 3, 'lr_pre': 0.01, 'lr': 0.01, 'w_decay': 0.0, 'decay_rate': 9, 'max_nums': None, 'embed_dim': 32, 'hidden_dim_enc': 64, 'hidden_dim': 64, 'dropout': 0.0, 'nonlin': None, 'temperature': 0.2, 'n_cluster_trials': 5, 't': 1.0, 'r': 2.0, 'patience': 5, 'save_path': 'model.pt', 'use_gpu': True, 'gpu': 0, 'devices': '0,1', 'data_path': './datasets/affinity_matrix_from_senet_sparse_20000.npz', 'label_path': './datasets/senet_label_20000.csv'}
INFO - 03/14/25 02:54:42 - 0:00:03 - 
                                     train iters 0
WARNING - 03/14/25 02:55:39 - 0:01:00 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:55:39 - 0:01:00 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 24, in forward
                                            h = self.linear(x)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 59, in forward
                                            x = torch.cat([time, x_narrow * scale.sqrt()], dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.78 GiB. GPU 0 has a total capacity of 79.25 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 75.00 GiB memory in use. Of the allocated memory 73.06 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:55:39 - 0:01:00 - Added config 93cb35 as new incumbent because there are no incumbents yet.
INFO - 03/14/25 02:55:41 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 02:56:37 - 0:00:58 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:56:37 - 0:00:58 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 24, in forward
                                            h = self.linear(x)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 58, in forward
                                            (x_narrow * x_narrow).sum(dim=-1, keepdim=True).clamp_min(1e-8)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.78 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.51 GiB memory in use. Of the allocated memory 49.29 GiB is allocated by PyTorch, and 23.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:56:39 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 02:56:51 - 0:00:15 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:56:51 - 0:00:15 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.12 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.52 GiB memory in use. Of the allocated memory 57.18 GiB is allocated by PyTorch, and 15.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:56:53 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 02:57:06 - 0:00:14 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:57:06 - 0:00:14 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.12 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.52 GiB memory in use. Of the allocated memory 57.18 GiB is allocated by PyTorch, and 15.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:57:07 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 02:58:02 - 0:00:56 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:58:02 - 0:00:56 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 24, in forward
                                            h = self.linear(x)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 58, in forward
                                            (x_narrow * x_narrow).sum(dim=-1, keepdim=True).clamp_min(1e-8)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.78 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.51 GiB memory in use. Of the allocated memory 49.29 GiB is allocated by PyTorch, and 23.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:58:04 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 02:58:09 - 0:00:07 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:58:09 - 0:00:07 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.52 GiB memory in use. Of the allocated memory 54.97 GiB is allocated by PyTorch, and 18.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:58:12 - 0:00:04 - 
                                     train iters 0
WARNING - 03/14/25 02:58:17 - 0:00:09 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:58:17 - 0:00:09 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.52 GiB memory in use. Of the allocated memory 54.97 GiB is allocated by PyTorch, and 18.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:58:21 - 0:00:04 - 
                                     train iters 0
WARNING - 03/14/25 02:58:26 - 0:00:09 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 02:58:26 - 0:00:09 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.73 GiB is free. Including non-PyTorch memory, this process has 73.52 GiB memory in use. Of the allocated memory 54.97 GiB is allocated by PyTorch, and 18.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 02:58:30 - 0:00:04 - 
                                     train iters 0
INFO - 03/14/25 02:58:43 - 0:00:17 - Epoch 1: train_loss=6.015957832336426
INFO - 03/14/25 02:58:55 - 0:00:30 - Epoch 2: train_loss=1.7015429735183716
INFO - 03/14/25 02:59:09 - 0:00:43 - Epoch 3: train_loss=1.1460579633712769
INFO - 03/14/25 02:59:21 - 0:00:56 - Epoch 4: train_loss=1.146661639213562
INFO - 03/14/25 02:59:34 - 0:01:08 - Epoch 5: train_loss=1.1467843055725098
INFO - 03/14/25 02:59:47 - 0:01:21 - Epoch 6: train_loss=1.1468336582183838
INFO - 03/14/25 02:59:59 - 0:01:34 - Epoch 7: train_loss=1.1468513011932373
INFO - 03/14/25 03:00:12 - 0:01:46 - Epoch 8: train_loss=1.1468608379364014
INFO - 03/14/25 03:00:25 - 0:01:59 - Epoch 9: train_loss=1.1468665599822998
INFO - 03/14/25 03:00:37 - 0:02:12 - Epoch 10: train_loss=1.1468706130981445
INFO - 03/14/25 03:00:50 - 0:02:24 - Epoch 11: train_loss=1.1468734741210938
INFO - 03/14/25 03:01:03 - 0:02:37 - Epoch 12: train_loss=1.1468755006790161
INFO - 03/14/25 03:01:15 - 0:02:50 - Epoch 13: train_loss=1.1468771696090698
INFO - 03/14/25 03:01:28 - 0:03:02 - Epoch 14: train_loss=1.1468783617019653
INFO - 03/14/25 03:01:41 - 0:03:15 - Epoch 15: train_loss=1.1468853950500488
INFO - 03/14/25 03:01:54 - 0:03:28 - Epoch 16: train_loss=1.1468862295150757
INFO - 03/14/25 03:02:06 - 0:03:40 - Epoch 17: train_loss=1.146886944770813
INFO - 03/14/25 03:02:19 - 0:03:53 - Epoch 18: train_loss=1.1468875408172607
INFO - 03/14/25 03:02:32 - 0:04:06 - Epoch 19: train_loss=1.1468878984451294
INFO - 03/14/25 03:02:44 - 0:04:19 - Epoch 20: train_loss=1.1468881368637085
INFO - 03/14/25 03:02:57 - 0:04:31 - Epoch 21: train_loss=1.1468884944915771
INFO - 03/14/25 03:03:10 - 0:04:44 - Epoch 22: train_loss=1.1468830108642578
INFO - 03/14/25 03:03:22 - 0:04:57 - Epoch 23: train_loss=1.146883249282837
INFO - 03/14/25 03:03:35 - 0:05:09 - Epoch 24: train_loss=1.1468894481658936
INFO - 03/14/25 03:03:48 - 0:05:22 - Epoch 25: train_loss=1.146889567375183
INFO - 03/14/25 03:04:00 - 0:05:35 - Epoch 26: train_loss=1.146889567375183
INFO - 03/14/25 03:04:13 - 0:05:47 - Epoch 27: train_loss=1.1468898057937622
INFO - 03/14/25 03:04:26 - 0:06:00 - Epoch 28: train_loss=1.1468898057937622
INFO - 03/14/25 03:04:38 - 0:06:13 - Epoch 29: train_loss=1.1468898057937622
INFO - 03/14/25 03:04:51 - 0:06:25 - Epoch 30: train_loss=1.1468899250030518
INFO - 03/14/25 03:05:04 - 0:06:38 - Epoch 31: train_loss=1.1468901634216309
INFO - 03/14/25 03:05:17 - 0:06:51 - Epoch 32: train_loss=1.1468901634216309
INFO - 03/14/25 03:05:29 - 0:07:03 - Epoch 33: train_loss=1.1468842029571533
INFO - 03/14/25 03:05:42 - 0:07:16 - Epoch 34: train_loss=1.1468901634216309
INFO - 03/14/25 03:05:55 - 0:07:29 - Epoch 35: train_loss=1.1468901634216309
INFO - 03/14/25 03:06:07 - 0:07:42 - Epoch 36: train_loss=1.1468901634216309
INFO - 03/14/25 03:06:20 - 0:07:54 - Epoch 37: train_loss=1.1468900442123413
INFO - 03/14/25 03:06:33 - 0:08:07 - Epoch 38: train_loss=1.1468900442123413
INFO - 03/14/25 03:06:45 - 0:08:20 - Epoch 39: train_loss=1.1468839645385742
INFO - 03/14/25 03:06:58 - 0:08:32 - Epoch 40: train_loss=1.1468899250030518
INFO - 03/14/25 03:07:11 - 0:08:45 - Epoch 41: train_loss=1.1468898057937622
INFO - 03/14/25 03:07:24 - 0:08:58 - Epoch 42: train_loss=1.1468898057937622
INFO - 03/14/25 03:07:36 - 0:09:11 - Epoch 43: train_loss=1.1468838453292847
INFO - 03/14/25 03:07:49 - 0:09:23 - Epoch 44: train_loss=1.1468898057937622
INFO - 03/14/25 03:08:02 - 0:09:36 - Epoch 45: train_loss=1.1468896865844727
INFO - 03/14/25 03:08:14 - 0:09:49 - Epoch 46: train_loss=1.1468836069107056
INFO - 03/14/25 03:08:27 - 0:10:01 - Epoch 47: train_loss=1.146889567375183
INFO - 03/14/25 03:08:40 - 0:10:14 - Epoch 48: train_loss=1.1468836069107056
INFO - 03/14/25 03:08:52 - 0:10:27 - Epoch 49: train_loss=1.1468894481658936
INFO - 03/14/25 03:09:05 - 0:10:39 - Epoch 50: train_loss=1.1468894481658936
INFO - 03/14/25 03:09:18 - 0:10:52 - Epoch 51: train_loss=1.146889328956604
INFO - 03/14/25 03:09:30 - 0:11:05 - Epoch 52: train_loss=1.146889328956604
INFO - 03/14/25 03:09:43 - 0:11:17 - Epoch 53: train_loss=1.1468892097473145
INFO - 03/14/25 03:09:56 - 0:11:30 - Epoch 54: train_loss=1.1468889713287354
INFO - 03/14/25 03:10:08 - 0:11:43 - Epoch 55: train_loss=1.1468889713287354
INFO - 03/14/25 03:10:21 - 0:11:55 - Epoch 56: train_loss=1.1468888521194458
INFO - 03/14/25 03:10:34 - 0:12:08 - Epoch 57: train_loss=1.1468888521194458
INFO - 03/14/25 03:10:47 - 0:12:21 - Epoch 58: train_loss=1.1468887329101562
INFO - 03/14/25 03:10:59 - 0:12:33 - Epoch 59: train_loss=1.1468884944915771
INFO - 03/14/25 03:11:12 - 0:12:46 - Epoch 60: train_loss=1.1468884944915771
INFO - 03/14/25 03:11:25 - 0:12:59 - Epoch 61: train_loss=1.1468883752822876
INFO - 03/14/25 03:11:37 - 0:13:12 - Epoch 62: train_loss=1.1468883752822876
INFO - 03/14/25 03:11:50 - 0:13:24 - Epoch 63: train_loss=1.1468881368637085
INFO - 03/14/25 03:12:03 - 0:13:37 - Epoch 64: train_loss=1.1468881368637085
INFO - 03/14/25 03:12:15 - 0:13:50 - Epoch 65: train_loss=1.1468881368637085
INFO - 03/14/25 03:12:28 - 0:14:02 - Epoch 66: train_loss=1.1468878984451294
INFO - 03/14/25 03:12:41 - 0:14:15 - Epoch 67: train_loss=1.1468819379806519
INFO - 03/14/25 03:12:53 - 0:14:28 - Epoch 68: train_loss=1.1468876600265503
INFO - 03/14/25 03:13:06 - 0:14:40 - Epoch 69: train_loss=1.1468876600265503
INFO - 03/14/25 03:13:19 - 0:14:53 - Epoch 70: train_loss=1.1468875408172607
INFO - 03/14/25 03:13:31 - 0:15:06 - Epoch 71: train_loss=1.1468874216079712
INFO - 03/14/25 03:13:44 - 0:15:18 - Epoch 72: train_loss=1.1468812227249146
INFO - 03/14/25 03:13:57 - 0:15:31 - Epoch 73: train_loss=1.146887183189392
INFO - 03/14/25 03:14:10 - 0:15:44 - Epoch 74: train_loss=1.146887183189392
INFO - 03/14/25 03:14:22 - 0:15:56 - Epoch 75: train_loss=1.146886944770813
INFO - 03/14/25 03:14:35 - 0:16:09 - Epoch 76: train_loss=1.1468868255615234
INFO - 03/14/25 03:14:48 - 0:16:22 - Epoch 77: train_loss=1.1468867063522339
INFO - 03/14/25 03:15:00 - 0:16:35 - Epoch 78: train_loss=1.1468864679336548
INFO - 03/14/25 03:15:13 - 0:16:47 - Epoch 79: train_loss=1.1468864679336548
INFO - 03/14/25 03:15:26 - 0:17:00 - Epoch 80: train_loss=1.1468862295150757
INFO - 03/14/25 03:15:38 - 0:17:13 - Epoch 81: train_loss=1.1468861103057861
INFO - 03/14/25 03:15:51 - 0:17:25 - Epoch 82: train_loss=1.1468859910964966
INFO - 03/14/25 03:16:04 - 0:17:38 - Epoch 83: train_loss=1.146885871887207
INFO - 03/14/25 03:16:17 - 0:17:51 - Epoch 84: train_loss=1.1468857526779175
INFO - 03/14/25 03:16:29 - 0:18:03 - Epoch 85: train_loss=1.1468855142593384
INFO - 03/14/25 03:16:42 - 0:18:16 - Epoch 86: train_loss=1.1468855142593384
INFO - 03/14/25 03:16:55 - 0:18:29 - Epoch 87: train_loss=1.1468852758407593
INFO - 03/14/25 03:17:07 - 0:18:42 - Epoch 88: train_loss=1.1468851566314697
INFO - 03/14/25 03:17:20 - 0:18:54 - Epoch 89: train_loss=1.1468850374221802
INFO - 03/14/25 03:17:33 - 0:19:07 - Epoch 90: train_loss=1.146884799003601
INFO - 03/14/25 03:17:46 - 0:19:20 - Epoch 91: train_loss=1.146884799003601
INFO - 03/14/25 03:17:58 - 0:19:32 - Epoch 92: train_loss=1.146884560585022
INFO - 03/14/25 03:18:11 - 0:19:45 - Epoch 93: train_loss=1.1468844413757324
INFO - 03/14/25 03:18:24 - 0:19:58 - Epoch 94: train_loss=1.1468843221664429
INFO - 03/14/25 03:18:36 - 0:20:11 - Epoch 95: train_loss=1.1468842029571533
INFO - 03/14/25 03:18:49 - 0:20:23 - Epoch 96: train_loss=1.1468838453292847
INFO - 03/14/25 03:19:02 - 0:20:36 - Epoch 97: train_loss=1.1468838453292847
INFO - 03/14/25 03:19:15 - 0:20:49 - Epoch 98: train_loss=1.1468836069107056
INFO - 03/14/25 03:19:27 - 0:21:02 - Epoch 99: train_loss=1.146883487701416
INFO - 03/14/25 03:19:40 - 0:21:14 - Epoch 100: train_loss=1.146883249282837
INFO - 03/14/25 03:19:53 - 0:21:27 - Epoch 101: train_loss=1.1468831300735474
INFO - 03/14/25 03:20:05 - 0:21:40 - Epoch 102: train_loss=1.1468830108642578
INFO - 03/14/25 03:20:18 - 0:21:52 - Epoch 103: train_loss=1.1468827724456787
INFO - 03/14/25 03:20:31 - 0:22:05 - Epoch 104: train_loss=1.1468826532363892
INFO - 03/14/25 03:20:44 - 0:22:18 - Epoch 105: train_loss=1.14688241481781
INFO - 03/14/25 03:20:56 - 0:22:30 - Epoch 106: train_loss=1.146882176399231
INFO - 03/14/25 03:21:09 - 0:22:43 - Epoch 107: train_loss=1.1468820571899414
INFO - 03/14/25 03:21:22 - 0:22:56 - Epoch 108: train_loss=1.1468818187713623
INFO - 03/14/25 03:21:34 - 0:23:09 - Epoch 109: train_loss=1.1468816995620728
INFO - 03/14/25 03:21:47 - 0:23:21 - Epoch 110: train_loss=1.1468814611434937
INFO - 03/14/25 03:22:00 - 0:23:34 - Epoch 111: train_loss=1.1468753814697266
INFO - 03/14/25 03:22:13 - 0:23:47 - Epoch 112: train_loss=1.1468751430511475
INFO - 03/14/25 03:22:25 - 0:24:00 - Epoch 113: train_loss=1.1468749046325684
INFO - 03/14/25 03:22:38 - 0:24:12 - Epoch 114: train_loss=1.1468746662139893
INFO - 03/14/25 03:22:51 - 0:24:25 - Epoch 115: train_loss=1.1468745470046997
INFO - 03/14/25 03:23:03 - 0:24:38 - Epoch 116: train_loss=1.1468743085861206
INFO - 03/14/25 03:23:16 - 0:24:50 - Epoch 117: train_loss=1.1468740701675415
INFO - 03/14/25 03:23:29 - 0:25:03 - Epoch 118: train_loss=1.1468738317489624
INFO - 03/14/25 03:23:42 - 0:25:16 - Epoch 119: train_loss=1.1468735933303833
INFO - 03/14/25 03:23:54 - 0:25:29 - Epoch 120: train_loss=1.1468733549118042
INFO - 03/14/25 03:24:07 - 0:25:41 - Epoch 121: train_loss=1.1468732357025146
INFO - 03/14/25 03:24:20 - 0:25:54 - Epoch 122: train_loss=1.1468729972839355
INFO - 03/14/25 03:24:32 - 0:26:07 - Epoch 123: train_loss=1.1468727588653564
INFO - 03/14/25 03:24:45 - 0:26:19 - Epoch 124: train_loss=1.1468725204467773
INFO - 03/14/25 03:24:58 - 0:26:32 - Epoch 125: train_loss=1.1468722820281982
INFO - 03/14/25 03:25:11 - 0:26:45 - Epoch 126: train_loss=1.1468720436096191
INFO - 03/14/25 03:25:23 - 0:26:57 - Epoch 127: train_loss=1.14687180519104
INFO - 03/14/25 03:25:36 - 0:27:10 - Epoch 128: train_loss=1.146871566772461
INFO - 03/14/25 03:25:49 - 0:27:23 - Epoch 129: train_loss=1.1468713283538818
INFO - 03/14/25 03:26:01 - 0:27:36 - Epoch 130: train_loss=1.1468709707260132
INFO - 03/14/25 03:26:14 - 0:27:48 - Epoch 131: train_loss=1.1468708515167236
INFO - 03/14/25 03:26:27 - 0:28:01 - Epoch 132: train_loss=1.146870493888855
INFO - 03/14/25 03:26:39 - 0:28:14 - Epoch 133: train_loss=1.1468702554702759
INFO - 03/14/25 03:26:52 - 0:28:26 - Epoch 134: train_loss=1.1468698978424072
INFO - 03/14/25 03:27:05 - 0:28:39 - Epoch 135: train_loss=1.1468697786331177
INFO - 03/14/25 03:27:17 - 0:28:52 - Epoch 136: train_loss=1.1468695402145386
INFO - 03/14/25 03:27:30 - 0:29:04 - Epoch 137: train_loss=1.14686918258667
INFO - 03/14/25 03:27:43 - 0:29:17 - Epoch 138: train_loss=1.1468688249588013
INFO - 03/14/25 03:27:56 - 0:29:30 - Epoch 139: train_loss=1.1468685865402222
INFO - 03/14/25 03:28:08 - 0:29:42 - Epoch 140: train_loss=1.146868348121643
INFO - 03/14/25 03:28:21 - 0:29:55 - Epoch 141: train_loss=1.146868109703064
INFO - 03/14/25 03:28:34 - 0:30:08 - Epoch 142: train_loss=1.1468678712844849
INFO - 03/14/25 03:28:46 - 0:30:21 - Epoch 143: train_loss=1.1468675136566162
INFO - 03/14/25 03:28:59 - 0:30:33 - Epoch 144: train_loss=1.1468671560287476
INFO - 03/14/25 03:29:12 - 0:30:46 - Epoch 145: train_loss=1.146866798400879
INFO - 03/14/25 03:29:24 - 0:30:59 - Epoch 146: train_loss=1.1468664407730103
INFO - 03/14/25 03:29:37 - 0:31:11 - Epoch 147: train_loss=1.1468662023544312
INFO - 03/14/25 03:29:50 - 0:31:24 - Epoch 148: train_loss=1.1468658447265625
INFO - 03/14/25 03:30:03 - 0:31:37 - Epoch 149: train_loss=1.1468654870986938
INFO - 03/14/25 03:30:15 - 0:31:49 - Epoch 150: train_loss=1.1468651294708252
INFO - 03/14/25 03:30:28 - 0:32:02 - Epoch 151: train_loss=1.1468647718429565
INFO - 03/14/25 03:30:41 - 0:32:15 - Epoch 152: train_loss=1.1468645334243774
INFO - 03/14/25 03:30:53 - 0:32:28 - Epoch 153: train_loss=1.1468641757965088
INFO - 03/14/25 03:31:06 - 0:32:40 - Epoch 154: train_loss=1.1468638181686401
INFO - 03/14/25 03:31:19 - 0:32:53 - Epoch 155: train_loss=1.1468634605407715
INFO - 03/14/25 03:31:32 - 0:33:06 - Epoch 156: train_loss=1.1468631029129028
INFO - 03/14/25 03:31:44 - 0:33:19 - Epoch 157: train_loss=1.1468627452850342
INFO - 03/14/25 03:31:57 - 0:33:31 - Epoch 158: train_loss=1.146862506866455
INFO - 03/14/25 03:32:10 - 0:33:44 - Epoch 159: train_loss=1.1468620300292969
INFO - 03/14/25 03:32:22 - 0:33:57 - Epoch 160: train_loss=1.1468617916107178
INFO - 03/14/25 03:32:35 - 0:34:09 - Epoch 161: train_loss=1.1468613147735596
INFO - 03/14/25 03:32:48 - 0:34:22 - Epoch 162: train_loss=1.146860957145691
INFO - 03/14/25 03:33:00 - 0:34:35 - Epoch 163: train_loss=1.1468605995178223
INFO - 03/14/25 03:33:13 - 0:34:47 - Epoch 164: train_loss=1.1468602418899536
INFO - 03/14/25 03:33:26 - 0:35:00 - Epoch 165: train_loss=1.1468597650527954
INFO - 03/14/25 03:33:39 - 0:35:13 - Epoch 166: train_loss=1.1468594074249268
INFO - 03/14/25 03:33:51 - 0:35:26 - Epoch 167: train_loss=1.1468589305877686
INFO - 03/14/25 03:34:04 - 0:35:38 - Epoch 168: train_loss=1.1468584537506104
INFO - 03/14/25 03:34:17 - 0:35:51 - Epoch 169: train_loss=1.1468580961227417
INFO - 03/14/25 03:34:29 - 0:36:04 - Epoch 170: train_loss=1.146857738494873
INFO - 03/14/25 03:34:42 - 0:36:16 - Epoch 171: train_loss=1.1468573808670044
INFO - 03/14/25 03:34:55 - 0:36:29 - Epoch 172: train_loss=1.1468567848205566
INFO - 03/14/25 03:35:07 - 0:36:42 - Epoch 173: train_loss=1.146856427192688
INFO - 03/14/25 03:35:20 - 0:36:54 - Epoch 174: train_loss=1.1468558311462402
INFO - 03/14/25 03:35:33 - 0:37:07 - Epoch 175: train_loss=1.1468554735183716
INFO - 03/14/25 03:35:45 - 0:37:20 - Epoch 176: train_loss=1.1468549966812134
INFO - 03/14/25 03:35:58 - 0:37:32 - Epoch 177: train_loss=1.1468544006347656
INFO - 03/14/25 03:36:11 - 0:37:45 - Epoch 178: train_loss=1.146854043006897
INFO - 03/14/25 03:36:24 - 0:37:58 - Epoch 179: train_loss=1.1468534469604492
INFO - 03/14/25 03:36:36 - 0:38:11 - Epoch 180: train_loss=1.1468528509140015
INFO - 03/14/25 03:36:49 - 0:38:23 - Epoch 181: train_loss=1.1468524932861328
INFO - 03/14/25 03:37:02 - 0:38:36 - Epoch 182: train_loss=1.1468520164489746
INFO - 03/14/25 03:37:14 - 0:38:49 - Epoch 183: train_loss=1.1468514204025269
INFO - 03/14/25 03:37:27 - 0:39:01 - Epoch 184: train_loss=1.1468509435653687
INFO - 03/14/25 03:37:40 - 0:39:14 - Epoch 185: train_loss=1.1468504667282104
INFO - 03/14/25 03:37:52 - 0:39:27 - Epoch 186: train_loss=1.1468497514724731
INFO - 03/14/25 03:38:05 - 0:39:39 - Epoch 187: train_loss=1.1468490362167358
INFO - 03/14/25 03:38:18 - 0:39:52 - Epoch 188: train_loss=1.1468486785888672
INFO - 03/14/25 03:38:31 - 0:40:05 - Epoch 189: train_loss=1.1468479633331299
INFO - 03/14/25 03:38:43 - 0:40:18 - Epoch 190: train_loss=1.1468473672866821
INFO - 03/14/25 03:38:56 - 0:40:30 - Epoch 191: train_loss=1.1468467712402344
INFO - 03/14/25 03:39:09 - 0:40:43 - Epoch 192: train_loss=1.1468461751937866
INFO - 03/14/25 03:39:21 - 0:40:56 - Epoch 193: train_loss=1.1468454599380493
INFO - 03/14/25 03:39:34 - 0:41:08 - Epoch 194: train_loss=1.1468448638916016
INFO - 03/14/25 03:39:47 - 0:41:21 - Epoch 195: train_loss=1.1468443870544434
INFO - 03/14/25 03:39:59 - 0:41:34 - Epoch 196: train_loss=1.146843671798706
INFO - 03/14/25 03:40:12 - 0:41:46 - Epoch 197: train_loss=1.1468428373336792
INFO - 03/14/25 03:40:25 - 0:41:59 - Epoch 198: train_loss=1.1468422412872314
INFO - 03/14/25 03:40:37 - 0:42:12 - Epoch 199: train_loss=1.1468414068222046
INFO - 03/14/25 03:40:50 - 0:42:24 - Epoch 200: train_loss=1.1468406915664673
INFO - 03/14/25 03:41:03 - 0:42:37 - Epoch 201: train_loss=1.1468400955200195
INFO - 03/14/25 03:41:16 - 0:42:50 - Epoch 202: train_loss=1.1468393802642822
INFO - 03/14/25 03:41:28 - 0:43:03 - Epoch 203: train_loss=1.146838665008545
INFO - 03/14/25 03:41:41 - 0:43:15 - Epoch 204: train_loss=1.1468377113342285
INFO - 03/14/25 03:41:54 - 0:43:28 - Epoch 205: train_loss=1.1468369960784912
INFO - 03/14/25 03:42:06 - 0:43:41 - Epoch 206: train_loss=1.1468364000320435
INFO - 03/14/25 03:42:19 - 0:43:53 - Epoch 207: train_loss=1.146835446357727
INFO - 03/14/25 03:42:32 - 0:44:06 - Epoch 208: train_loss=1.1468346118927002
INFO - 03/14/25 03:42:44 - 0:44:19 - Epoch 209: train_loss=1.1468336582183838
INFO - 03/14/25 03:42:57 - 0:44:31 - Epoch 210: train_loss=1.1468329429626465
INFO - 03/14/25 03:43:10 - 0:44:44 - Epoch 211: train_loss=1.14683198928833
INFO - 03/14/25 03:43:23 - 0:44:57 - Epoch 212: train_loss=1.1468311548233032
INFO - 03/14/25 03:43:35 - 0:45:10 - Epoch 213: train_loss=1.1468302011489868
INFO - 03/14/25 03:43:48 - 0:45:22 - Epoch 214: train_loss=1.14682936668396
INFO - 03/14/25 03:44:01 - 0:45:35 - Epoch 215: train_loss=1.1468284130096436
INFO - 03/14/25 03:44:13 - 0:45:48 - Epoch 216: train_loss=1.1468274593353271
INFO - 03/14/25 03:44:26 - 0:46:00 - Epoch 217: train_loss=1.1468263864517212
INFO - 03/14/25 03:44:39 - 0:46:13 - Epoch 218: train_loss=1.1468254327774048
INFO - 03/14/25 03:44:52 - 0:46:26 - Epoch 219: train_loss=1.1468243598937988
INFO - 03/14/25 03:45:04 - 0:46:39 - Epoch 220: train_loss=1.1468232870101929
INFO - 03/14/25 03:45:17 - 0:46:51 - Epoch 221: train_loss=1.1468223333358765
INFO - 03/14/25 03:45:30 - 0:47:04 - Epoch 222: train_loss=1.1468212604522705
INFO - 03/14/25 03:45:42 - 0:47:17 - Epoch 223: train_loss=1.1468199491500854
INFO - 03/14/25 03:45:55 - 0:47:29 - Epoch 224: train_loss=1.1468191146850586
INFO - 03/14/25 03:46:08 - 0:47:42 - Epoch 225: train_loss=1.146817684173584
INFO - 03/14/25 03:46:21 - 0:47:55 - Epoch 226: train_loss=1.146816611289978
INFO - 03/14/25 03:46:33 - 0:48:08 - Epoch 227: train_loss=1.146809458732605
INFO - 03/14/25 03:46:46 - 0:48:20 - Epoch 228: train_loss=1.1468082666397095
INFO - 03/14/25 03:46:59 - 0:48:33 - Epoch 229: train_loss=1.1468068361282349
INFO - 03/14/25 03:47:11 - 0:48:46 - Epoch 230: train_loss=1.1468056440353394
INFO - 03/14/25 03:47:24 - 0:48:58 - Epoch 231: train_loss=1.1468042135238647
INFO - 03/14/25 03:47:37 - 0:49:11 - Epoch 232: train_loss=1.1468027830123901
INFO - 03/14/25 03:47:49 - 0:49:24 - Epoch 233: train_loss=1.1468015909194946
INFO - 03/14/25 03:48:02 - 0:49:36 - Epoch 234: train_loss=1.14680016040802
INFO - 03/14/25 03:48:15 - 0:49:49 - Epoch 235: train_loss=1.1467986106872559
INFO - 03/14/25 03:48:28 - 0:50:02 - Epoch 236: train_loss=1.1467971801757812
INFO - 03/14/25 03:48:40 - 0:50:15 - Epoch 237: train_loss=1.146795630455017
INFO - 03/14/25 03:48:53 - 0:50:27 - Epoch 238: train_loss=1.1467939615249634
INFO - 03/14/25 03:49:06 - 0:50:40 - Epoch 239: train_loss=1.1467924118041992
INFO - 03/14/25 03:49:18 - 0:50:53 - Epoch 240: train_loss=1.1467907428741455
INFO - 03/14/25 03:49:31 - 0:51:05 - Epoch 241: train_loss=1.1467889547348022
INFO - 03/14/25 03:49:44 - 0:51:18 - Epoch 242: train_loss=1.1467872858047485
INFO - 03/14/25 03:49:56 - 0:51:31 - Epoch 243: train_loss=1.1467856168746948
INFO - 03/14/25 03:50:09 - 0:51:43 - Epoch 244: train_loss=1.1467835903167725
INFO - 03/14/25 03:50:22 - 0:51:56 - Epoch 245: train_loss=1.146775722503662
INFO - 03/14/25 03:50:34 - 0:52:09 - Epoch 246: train_loss=1.1467738151550293
INFO - 03/14/25 03:50:47 - 0:52:21 - Epoch 247: train_loss=1.1467719078063965
INFO - 03/14/25 03:51:00 - 0:52:34 - Epoch 248: train_loss=1.1467698812484741
INFO - 03/14/25 03:51:13 - 0:52:47 - Epoch 249: train_loss=1.1467677354812622
INFO - 03/14/25 03:51:25 - 0:52:59 - Epoch 250: train_loss=1.1467655897140503
INFO - 03/14/25 03:51:38 - 0:53:12 - Epoch 251: train_loss=1.1467634439468384
INFO - 03/14/25 03:51:51 - 0:53:25 - Epoch 252: train_loss=1.146761178970337
INFO - 03/14/25 03:52:03 - 0:53:38 - Epoch 253: train_loss=1.1467589139938354
INFO - 03/14/25 03:52:16 - 0:53:50 - Epoch 254: train_loss=1.1467565298080444
INFO - 03/14/25 03:52:29 - 0:54:03 - Epoch 255: train_loss=1.1467541456222534
INFO - 03/14/25 03:52:41 - 0:54:16 - Epoch 256: train_loss=1.1467516422271729
INFO - 03/14/25 03:52:54 - 0:54:28 - Epoch 257: train_loss=1.1467490196228027
INFO - 03/14/25 03:53:07 - 0:54:41 - Epoch 258: train_loss=1.1467463970184326
INFO - 03/14/25 03:53:19 - 0:54:54 - Epoch 259: train_loss=1.146743655204773
INFO - 03/14/25 03:53:32 - 0:55:06 - Epoch 260: train_loss=1.1467407941818237
INFO - 03/14/25 03:53:45 - 0:55:19 - Epoch 261: train_loss=1.146737813949585
INFO - 03/14/25 03:53:58 - 0:55:32 - Epoch 262: train_loss=1.1467289924621582
INFO - 03/14/25 03:54:10 - 0:55:44 - Epoch 263: train_loss=1.1467260122299194
INFO - 03/14/25 03:54:23 - 0:55:57 - Epoch 264: train_loss=1.1467227935791016
INFO - 03/14/25 03:54:36 - 0:56:10 - Epoch 265: train_loss=1.1467194557189941
INFO - 03/14/25 03:54:48 - 0:56:23 - Epoch 266: train_loss=1.1467161178588867
INFO - 03/14/25 03:55:01 - 0:56:35 - Epoch 267: train_loss=1.1467127799987793
INFO - 03/14/25 03:55:14 - 0:56:48 - Epoch 268: train_loss=1.1467090845108032
INFO - 03/14/25 03:55:26 - 0:57:01 - Epoch 269: train_loss=1.1467053890228271
INFO - 03/14/25 03:55:39 - 0:57:13 - Epoch 270: train_loss=1.146701693534851
INFO - 03/14/25 03:55:52 - 0:57:26 - Epoch 271: train_loss=1.1466976404190063
INFO - 03/14/25 03:56:04 - 0:57:39 - Epoch 272: train_loss=1.1466937065124512
INFO - 03/14/25 03:56:17 - 0:57:51 - Epoch 273: train_loss=1.1466896533966064
INFO - 03/14/25 03:56:30 - 0:58:04 - Epoch 274: train_loss=1.146685004234314
INFO - 03/14/25 03:56:43 - 0:58:17 - Epoch 275: train_loss=1.1466807126998901
INFO - 03/14/25 03:56:55 - 0:58:30 - Epoch 276: train_loss=1.1466761827468872
INFO - 03/14/25 03:57:08 - 0:58:42 - Epoch 277: train_loss=1.1466712951660156
INFO - 03/14/25 03:57:21 - 0:58:55 - Epoch 278: train_loss=1.1466662883758545
INFO - 03/14/25 03:57:33 - 0:59:08 - Epoch 279: train_loss=1.1466611623764038
INFO - 03/14/25 03:57:46 - 0:59:20 - Epoch 280: train_loss=1.1466498374938965
INFO - 03/14/25 03:57:59 - 0:59:33 - Epoch 281: train_loss=1.1466442346572876
INFO - 03/14/25 03:58:11 - 0:59:46 - Epoch 282: train_loss=1.1466386318206787
INFO - 03/14/25 03:58:24 - 0:59:58 - Epoch 283: train_loss=1.1466327905654907
INFO - 03/14/25 03:58:37 - 1:00:11 - Epoch 284: train_loss=1.1466267108917236
INFO - 03/14/25 03:58:50 - 1:00:24 - Epoch 285: train_loss=1.1466203927993774
INFO - 03/14/25 03:59:02 - 1:00:36 - Epoch 286: train_loss=1.1466138362884521
INFO - 03/14/25 03:59:15 - 1:00:49 - Epoch 287: train_loss=1.1466008424758911
INFO - 03/14/25 03:59:28 - 1:01:02 - Epoch 288: train_loss=1.146593689918518
INFO - 03/14/25 03:59:40 - 1:01:15 - Epoch 289: train_loss=1.1465864181518555
INFO - 03/14/25 03:59:53 - 1:01:27 - Epoch 290: train_loss=1.1465786695480347
INFO - 03/14/25 04:00:06 - 1:01:40 - Epoch 291: train_loss=1.1465706825256348
INFO - 03/14/25 04:00:18 - 1:01:53 - Epoch 292: train_loss=1.1465623378753662
INFO - 03/14/25 04:00:31 - 1:02:05 - Epoch 293: train_loss=1.1465479135513306
INFO - 03/14/25 04:00:44 - 1:02:18 - Epoch 294: train_loss=1.1465389728546143
INFO - 03/14/25 04:00:56 - 1:02:31 - Epoch 295: train_loss=1.1465297937393188
INFO - 03/14/25 04:01:09 - 1:02:43 - Epoch 296: train_loss=1.1465201377868652
INFO - 03/14/25 04:01:22 - 1:02:56 - Epoch 297: train_loss=1.1465044021606445
INFO - 03/14/25 04:01:35 - 1:03:09 - Epoch 298: train_loss=1.1464942693710327
INFO - 03/14/25 04:01:47 - 1:03:21 - Epoch 299: train_loss=1.1464836597442627
INFO - 03/14/25 04:02:00 - 1:03:34 - Epoch 300: train_loss=1.1464725732803345
INFO - 03/14/25 04:02:13 - 1:03:47 - Epoch 301: train_loss=1.1464614868164062
INFO - 03/14/25 04:02:25 - 1:04:00 - Epoch 302: train_loss=1.1464498043060303
INFO - 03/14/25 04:02:38 - 1:04:12 - Epoch 303: train_loss=1.146431803703308
INFO - 03/14/25 04:02:51 - 1:04:25 - Epoch 304: train_loss=1.1464195251464844
INFO - 03/14/25 04:03:03 - 1:04:38 - Epoch 305: train_loss=1.1464067697525024
INFO - 03/14/25 04:03:16 - 1:04:50 - Epoch 306: train_loss=1.1463935375213623
INFO - 03/14/25 04:03:29 - 1:05:03 - Epoch 307: train_loss=1.1463741064071655
INFO - 03/14/25 04:03:41 - 1:05:16 - Epoch 308: train_loss=1.146360158920288
INFO - 03/14/25 04:03:54 - 1:05:28 - Epoch 309: train_loss=1.1463459730148315
INFO - 03/14/25 04:04:07 - 1:05:41 - Epoch 310: train_loss=1.1463253498077393
INFO - 03/14/25 04:04:19 - 1:05:54 - Epoch 311: train_loss=1.1463102102279663
INFO - 03/14/25 04:04:32 - 1:06:06 - Epoch 312: train_loss=1.1462944746017456
INFO - 03/14/25 04:04:45 - 1:06:19 - Epoch 313: train_loss=1.1462783813476562
INFO - 03/14/25 04:04:58 - 1:06:32 - Epoch 314: train_loss=1.1462560892105103
INFO - 03/14/25 04:05:10 - 1:06:45 - Epoch 315: train_loss=1.1462393999099731
INFO - 03/14/25 04:05:23 - 1:06:57 - Epoch 316: train_loss=1.1462160348892212
INFO - 03/14/25 04:05:36 - 1:07:10 - Epoch 317: train_loss=1.1461983919143677
INFO - 03/14/25 04:05:48 - 1:07:23 - Epoch 318: train_loss=1.1461743116378784
INFO - 03/14/25 04:06:01 - 1:07:35 - Epoch 319: train_loss=1.146155595779419
INFO - 03/14/25 04:06:14 - 1:07:48 - Epoch 320: train_loss=1.1461366415023804
INFO - 03/14/25 04:06:26 - 1:08:01 - Epoch 321: train_loss=1.146111011505127
INFO - 03/14/25 04:06:39 - 1:08:13 - Epoch 322: train_loss=1.1460907459259033
INFO - 03/14/25 04:06:52 - 1:08:26 - Epoch 323: train_loss=1.146064281463623
INFO - 03/14/25 04:07:04 - 1:08:39 - Epoch 324: train_loss=1.146037220954895
INFO - 03/14/25 04:07:17 - 1:08:51 - Epoch 325: train_loss=1.1460155248641968
INFO - 03/14/25 04:07:30 - 1:09:04 - Epoch 326: train_loss=1.1459931135177612
INFO - 03/14/25 04:07:43 - 1:09:17 - Epoch 327: train_loss=1.1459643840789795
INFO - 03/14/25 04:07:55 - 1:09:29 - Epoch 328: train_loss=1.1459349393844604
INFO - 03/14/25 04:08:08 - 1:09:42 - Epoch 329: train_loss=1.1459046602249146
INFO - 03/14/25 04:08:21 - 1:09:55 - Epoch 330: train_loss=1.1458799839019775
INFO - 03/14/25 04:08:33 - 1:10:08 - Epoch 331: train_loss=1.1458483934402466
INFO - 03/14/25 04:08:46 - 1:10:20 - Epoch 332: train_loss=1.1458162069320679
INFO - 03/14/25 04:08:59 - 1:10:33 - Epoch 333: train_loss=1.1457833051681519
INFO - 03/14/25 04:09:11 - 1:10:46 - Epoch 334: train_loss=1.1457493305206299
INFO - 03/14/25 04:09:24 - 1:10:58 - Epoch 335: train_loss=1.1457208395004272
INFO - 03/14/25 04:09:37 - 1:11:11 - Epoch 336: train_loss=1.1456794738769531
INFO - 03/14/25 04:09:49 - 1:11:24 - Epoch 337: train_loss=1.1456429958343506
INFO - 03/14/25 04:10:02 - 1:11:36 - Epoch 338: train_loss=1.1456059217453003
INFO - 03/14/25 04:10:15 - 1:11:49 - Epoch 339: train_loss=1.1455676555633545
INFO - 03/14/25 04:10:27 - 1:12:02 - Epoch 340: train_loss=1.1455281972885132
INFO - 03/14/25 04:10:40 - 1:12:14 - Epoch 341: train_loss=1.1454880237579346
INFO - 03/14/25 04:10:53 - 1:12:27 - Epoch 342: train_loss=1.1454402208328247
INFO - 03/14/25 04:11:06 - 1:12:40 - Epoch 343: train_loss=1.1453973054885864
INFO - 03/14/25 04:11:18 - 1:12:52 - Epoch 344: train_loss=1.1453471183776855
INFO - 03/14/25 04:11:31 - 1:13:05 - Epoch 345: train_loss=1.1453012228012085
INFO - 03/14/25 04:11:44 - 1:13:18 - Epoch 346: train_loss=1.1452420949935913
INFO - 03/14/25 04:11:56 - 1:13:31 - Epoch 347: train_loss=1.145193099975586
INFO - 03/14/25 04:12:09 - 1:13:43 - Epoch 348: train_loss=1.1451365947723389
INFO - 03/14/25 04:12:22 - 1:13:56 - Epoch 349: train_loss=1.1450779438018799
INFO - 03/14/25 04:12:34 - 1:14:09 - Epoch 350: train_loss=1.145011305809021
INFO - 03/14/25 04:12:47 - 1:14:21 - Epoch 351: train_loss=1.1449488401412964
INFO - 03/14/25 04:13:00 - 1:14:34 - Epoch 352: train_loss=1.1448776721954346
INFO - 03/14/25 04:13:12 - 1:14:47 - Epoch 353: train_loss=1.1448042392730713
INFO - 03/14/25 04:13:25 - 1:14:59 - Epoch 354: train_loss=1.144728183746338
INFO - 03/14/25 04:13:38 - 1:15:12 - Epoch 355: train_loss=1.1446491479873657
INFO - 03/14/25 04:13:50 - 1:15:25 - Epoch 356: train_loss=1.144560694694519
INFO - 03/14/25 04:14:03 - 1:15:37 - Epoch 357: train_loss=1.1444690227508545
INFO - 03/14/25 04:14:16 - 1:15:50 - Epoch 358: train_loss=1.1443673372268677
INFO - 03/14/25 04:14:29 - 1:16:03 - Epoch 359: train_loss=1.1442677974700928
INFO - 03/14/25 04:14:41 - 1:16:16 - Epoch 360: train_loss=1.1441516876220703
INFO - 03/14/25 04:14:54 - 1:16:28 - Epoch 361: train_loss=1.1440304517745972
INFO - 03/14/25 04:15:07 - 1:16:41 - Epoch 362: train_loss=1.1439036130905151
INFO - 03/14/25 04:15:19 - 1:16:54 - Epoch 363: train_loss=1.143764615058899
INFO - 03/14/25 04:15:32 - 1:17:06 - Epoch 364: train_loss=1.1436126232147217
INFO - 03/14/25 04:15:45 - 1:17:19 - Epoch 365: train_loss=1.143446683883667
INFO - 03/14/25 04:15:57 - 1:17:32 - Epoch 366: train_loss=1.143265724182129
INFO - 03/14/25 04:16:10 - 1:17:44 - Epoch 367: train_loss=1.1430683135986328
INFO - 03/14/25 04:16:23 - 1:17:57 - Epoch 368: train_loss=1.1428470611572266
INFO - 03/14/25 04:16:36 - 1:18:10 - Epoch 369: train_loss=1.1425997018814087
INFO - 03/14/25 04:16:48 - 1:18:22 - Epoch 370: train_loss=1.1423239707946777
INFO - 03/14/25 04:17:01 - 1:18:35 - Epoch 371: train_loss=1.1420165300369263
INFO - 03/14/25 04:17:14 - 1:18:48 - Epoch 372: train_loss=1.1416610479354858
INFO - 03/14/25 04:17:26 - 1:19:01 - Epoch 373: train_loss=1.141251802444458
INFO - 03/14/25 04:17:39 - 1:19:13 - Epoch 374: train_loss=1.1407684087753296
INFO - 03/14/25 04:17:52 - 1:19:26 - Epoch 375: train_loss=1.1401941776275635
INFO - 03/14/25 04:18:04 - 1:19:39 - Epoch 376: train_loss=1.139487862586975
INFO - 03/14/25 04:18:17 - 1:19:51 - Epoch 377: train_loss=1.1386065483093262
INFO - 03/14/25 04:18:30 - 1:20:04 - Epoch 378: train_loss=1.137473225593567
INFO - 03/14/25 04:18:42 - 1:20:17 - Epoch 379: train_loss=1.1359409093856812
INFO - 03/14/25 04:18:55 - 1:20:29 - Epoch 380: train_loss=1.1337403059005737
INFO - 03/14/25 04:19:08 - 1:20:42 - Epoch 381: train_loss=1.1303948163986206
INFO - 03/14/25 04:19:20 - 1:20:55 - Epoch 382: train_loss=1.1251049041748047
INFO - 03/14/25 04:19:33 - 1:21:07 - Epoch 383: train_loss=1.1228077411651611
INFO - 03/14/25 04:19:46 - 1:21:20 - Epoch 384: train_loss=1.1265109777450562
INFO - 03/14/25 04:19:59 - 1:21:33 - Epoch 385: train_loss=1.1284654140472412
INFO - 03/14/25 04:20:11 - 1:21:46 - Epoch 386: train_loss=1.1287055015563965
INFO - 03/14/25 04:20:24 - 1:21:58 - Epoch 387: train_loss=1.1275136470794678
INFO - 03/14/25 04:20:37 - 1:22:11 - Epoch 388: train_loss=1.1247177124023438
INFO - 03/14/25 04:20:49 - 1:22:24 - Epoch 389: train_loss=1.1209611892700195
INFO - 03/14/25 04:21:02 - 1:22:36 - Epoch 390: train_loss=1.1220067739486694
INFO - 03/14/25 04:21:15 - 1:22:49 - Epoch 391: train_loss=1.124025821685791
INFO - 03/14/25 04:21:27 - 1:23:02 - Epoch 392: train_loss=1.1241767406463623
INFO - 03/14/25 04:21:40 - 1:23:14 - Epoch 393: train_loss=1.1227716207504272
INFO - 03/14/25 04:21:53 - 1:23:27 - Epoch 394: train_loss=1.1201739311218262
INFO - 03/14/25 04:22:06 - 1:23:40 - Epoch 395: train_loss=1.1201729774475098
INFO - 03/14/25 04:22:18 - 1:23:52 - Epoch 396: train_loss=1.1219335794448853
INFO - 03/14/25 04:22:31 - 1:24:05 - Epoch 397: train_loss=1.1215084791183472
INFO - 03/14/25 04:22:44 - 1:24:18 - Epoch 398: train_loss=1.1188819408416748
INFO - 03/14/25 04:22:56 - 1:24:31 - Epoch 399: train_loss=1.1197154521942139
INFO - 03/14/25 04:23:09 - 1:24:43 - Epoch 400: train_loss=1.1211341619491577
INFO - 03/14/25 04:23:22 - 1:24:56 - Epoch 401: train_loss=1.1204396486282349
INFO - 03/14/25 04:23:34 - 1:25:09 - Epoch 402: train_loss=1.1186285018920898
INFO - 03/14/25 04:23:47 - 1:25:21 - Epoch 403: train_loss=1.118783950805664
INFO - 03/14/25 04:24:00 - 1:25:34 - Epoch 404: train_loss=1.1197935342788696
INFO - 03/14/25 04:24:12 - 1:25:47 - Epoch 405: train_loss=1.1191984415054321
INFO - 03/14/25 04:24:25 - 1:25:59 - Epoch 406: train_loss=1.117767572402954
INFO - 03/14/25 04:24:38 - 1:26:12 - Epoch 407: train_loss=1.1187763214111328
INFO - 03/14/25 04:24:51 - 1:26:25 - Epoch 408: train_loss=1.1192692518234253
INFO - 03/14/25 04:25:03 - 1:26:38 - Epoch 409: train_loss=1.1182198524475098
INFO - 03/14/25 04:25:16 - 1:26:50 - Epoch 410: train_loss=1.1176670789718628
INFO - 03/14/25 04:25:29 - 1:27:03 - Epoch 411: train_loss=1.1187100410461426
INFO - 03/14/25 04:25:41 - 1:27:16 - Epoch 412: train_loss=1.1179571151733398
INFO - 03/14/25 04:25:54 - 1:27:28 - Epoch 413: train_loss=1.1177324056625366
INFO - 03/14/25 04:26:07 - 1:27:41 - Epoch 414: train_loss=1.1183799505233765
INFO - 03/14/25 04:26:19 - 1:27:54 - Epoch 415: train_loss=1.1178058385849
INFO - 03/14/25 04:26:32 - 1:28:06 - Epoch 416: train_loss=1.117569088935852
INFO - 03/14/25 04:26:45 - 1:28:19 - Epoch 417: train_loss=1.1184325218200684
INFO - 03/14/25 04:26:58 - 1:28:32 - Epoch 418: train_loss=1.1178702116012573
INFO - 03/14/25 04:27:10 - 1:28:44 - Epoch 419: train_loss=1.1179348230361938
INFO - 03/14/25 04:27:23 - 1:28:57 - Epoch 420: train_loss=1.1177124977111816
INFO - 03/14/25 04:27:36 - 1:29:10 - Epoch 421: train_loss=1.1180555820465088
INFO - 03/14/25 04:27:48 - 1:29:23 - Epoch 422: train_loss=1.117155909538269
INFO - 03/14/25 04:28:01 - 1:29:35 - Epoch 423: train_loss=1.1184026002883911
INFO - 03/14/25 04:28:14 - 1:29:48 - Epoch 424: train_loss=1.1179133653640747
INFO - 03/14/25 04:28:26 - 1:30:01 - Epoch 425: train_loss=1.117978572845459
INFO - 03/14/25 04:28:39 - 1:30:13 - Epoch 426: train_loss=1.1176871061325073
INFO - 03/14/25 04:28:52 - 1:30:26 - Epoch 427: train_loss=1.1180952787399292
INFO - 03/14/25 04:29:04 - 1:30:39 - Epoch 428: train_loss=1.1176806688308716
INFO - 03/14/25 04:29:17 - 1:30:51 - Epoch 429: train_loss=1.1178933382034302
INFO - 03/14/25 04:29:30 - 1:31:04 - Epoch 430: train_loss=1.1177518367767334
INFO - 03/14/25 04:29:43 - 1:31:17 - Epoch 431: train_loss=1.1175907850265503
INFO - 03/14/25 04:29:55 - 1:31:30 - Epoch 432: train_loss=1.117501974105835
INFO - 03/14/25 04:30:08 - 1:31:42 - Epoch 433: train_loss=1.1174110174179077
INFO - 03/14/25 04:30:21 - 1:31:55 - Epoch 434: train_loss=1.1174944639205933
INFO - 03/14/25 04:30:33 - 1:32:08 - Epoch 435: train_loss=1.1169850826263428
INFO - 03/14/25 04:30:46 - 1:32:20 - Epoch 436: train_loss=1.117864727973938
INFO - 03/14/25 04:30:59 - 1:32:33 - Epoch 437: train_loss=1.1169763803482056
INFO - 03/14/25 04:31:11 - 1:32:46 - Epoch 438: train_loss=1.118610143661499
INFO - 03/14/25 04:31:24 - 1:32:58 - Epoch 439: train_loss=1.118606448173523
INFO - 03/14/25 04:31:37 - 1:33:11 - Epoch 440: train_loss=1.116965413093567
INFO - 03/14/25 04:31:50 - 1:33:24 - Epoch 441: train_loss=1.1180990934371948
INFO - 03/14/25 04:32:02 - 1:33:36 - Epoch 442: train_loss=1.1175440549850464
INFO - 03/14/25 04:32:15 - 1:33:49 - Epoch 443: train_loss=1.1177679300308228
INFO - 03/14/25 04:32:28 - 1:34:02 - Epoch 444: train_loss=1.1176179647445679
INFO - 03/14/25 04:32:40 - 1:34:15 - Epoch 445: train_loss=1.117363691329956
INFO - 03/14/25 04:32:53 - 1:34:27 - Epoch 446: train_loss=1.117607593536377
INFO - 03/14/25 04:33:06 - 1:34:40 - Epoch 447: train_loss=1.1170594692230225
INFO - 03/14/25 04:33:18 - 1:34:53 - Epoch 448: train_loss=1.1178175210952759
INFO - 03/14/25 04:33:31 - 1:35:05 - Epoch 449: train_loss=1.1175988912582397
INFO - 03/14/25 04:33:44 - 1:35:18 - Epoch 450: train_loss=1.117253303527832
INFO - 03/14/25 04:33:56 - 1:35:31 - Epoch 451: train_loss=1.1175894737243652
INFO - 03/14/25 04:34:09 - 1:35:43 - Epoch 452: train_loss=1.117038607597351
INFO - 03/14/25 04:34:22 - 1:35:56 - Epoch 453: train_loss=1.1178696155548096
INFO - 03/14/25 04:34:35 - 1:36:09 - Epoch 454: train_loss=1.1175020933151245
INFO - 03/14/25 04:34:47 - 1:36:21 - Epoch 455: train_loss=1.1175763607025146
INFO - 03/14/25 04:35:00 - 1:36:34 - Epoch 456: train_loss=1.1174103021621704
INFO - 03/14/25 04:35:13 - 1:36:47 - Epoch 457: train_loss=1.1174912452697754
INFO - 03/14/25 04:35:25 - 1:37:00 - Epoch 458: train_loss=1.117127776145935
INFO - 03/14/25 04:35:38 - 1:37:12 - Epoch 459: train_loss=1.1174834966659546
INFO - 03/14/25 04:35:51 - 1:37:25 - Epoch 460: train_loss=1.1170107126235962
INFO - 03/14/25 04:36:03 - 1:37:38 - Epoch 461: train_loss=1.1176323890686035
INFO - 03/14/25 04:36:16 - 1:37:50 - Epoch 462: train_loss=1.1171131134033203
INFO - 03/14/25 04:36:29 - 1:38:03 - Epoch 463: train_loss=1.1177682876586914
INFO - 03/14/25 04:36:42 - 1:38:16 - Epoch 464: train_loss=1.1176906824111938
INFO - 03/14/25 04:36:54 - 1:38:28 - Epoch 465: train_loss=1.1172003746032715
INFO - 03/14/25 04:37:07 - 1:38:41 - Epoch 466: train_loss=1.1172895431518555
INFO - 03/14/25 04:37:20 - 1:38:54 - Epoch 467: train_loss=1.1170969009399414
INFO - 03/14/25 04:37:32 - 1:39:07 - Epoch 468: train_loss=1.1169853210449219
INFO - 03/14/25 04:37:45 - 1:39:19 - Epoch 469: train_loss=1.117090106010437
INFO - 03/14/25 04:37:58 - 1:39:32 - Epoch 470: train_loss=1.1167261600494385
INFO - 03/14/25 04:38:10 - 1:39:45 - Epoch 471: train_loss=1.1174439191818237
INFO - 03/14/25 04:38:23 - 1:39:57 - Epoch 472: train_loss=1.1169707775115967
INFO - 03/14/25 04:38:36 - 1:40:10 - Epoch 473: train_loss=1.117661714553833
INFO - 03/14/25 04:38:49 - 1:40:23 - Epoch 474: train_loss=1.1175142526626587
INFO - 03/14/25 04:39:01 - 1:40:35 - Epoch 475: train_loss=1.1169606447219849
INFO - 03/14/25 04:39:14 - 1:40:48 - Epoch 476: train_loss=1.1172564029693604
INFO - 03/14/25 04:39:27 - 1:41:01 - Epoch 477: train_loss=1.116702675819397
INFO - 03/14/25 04:39:39 - 1:41:14 - Epoch 478: train_loss=1.1174981594085693
INFO - 03/14/25 04:39:52 - 1:41:26 - Epoch 479: train_loss=1.1169487237930298
INFO - 03/14/25 04:40:05 - 1:41:39 - Epoch 480: train_loss=1.117709755897522
INFO - 03/14/25 04:40:17 - 1:41:52 - Epoch 481: train_loss=1.1177091598510742
INFO - 03/14/25 04:40:30 - 1:42:04 - Epoch 482: train_loss=1.1168184280395508
INFO - 03/14/25 04:40:43 - 1:42:17 - Epoch 483: train_loss=1.1173206567764282
INFO - 03/14/25 04:40:56 - 1:42:30 - Epoch 484: train_loss=1.116929292678833
INFO - 03/14/25 04:41:08 - 1:42:43 - Epoch 485: train_loss=1.1172302961349487
INFO - 03/14/25 04:41:21 - 1:42:55 - Epoch 486: train_loss=1.1169272661209106
INFO - 03/14/25 04:41:34 - 1:43:08 - Epoch 487: train_loss=1.1172209978103638
INFO - 03/14/25 04:41:46 - 1:43:21 - Epoch 488: train_loss=1.117215871810913
INFO - 03/14/25 04:41:59 - 1:43:33 - Epoch 489: train_loss=1.1169143915176392
INFO - 03/14/25 04:42:12 - 1:43:46 - Epoch 490: train_loss=1.1170202493667603
INFO - 03/14/25 04:42:24 - 1:43:59 - Epoch 491: train_loss=1.1166563034057617
INFO - 03/14/25 04:42:37 - 1:44:11 - Epoch 492: train_loss=1.1172034740447998
INFO - 03/14/25 04:42:50 - 1:44:24 - Epoch 493: train_loss=1.116783618927002
INFO - 03/14/25 04:43:03 - 1:44:37 - Epoch 494: train_loss=1.117199182510376
INFO - 03/14/25 04:43:15 - 1:44:50 - Epoch 495: train_loss=1.1170027256011963
INFO - 03/14/25 04:43:28 - 1:45:02 - Epoch 496: train_loss=1.1170024871826172
INFO - 03/14/25 04:43:41 - 1:45:15 - Epoch 497: train_loss=1.1168893575668335
INFO - 03/14/25 04:43:53 - 1:45:28 - Epoch 498: train_loss=1.1168866157531738
INFO - 03/14/25 04:44:06 - 1:45:40 - Epoch 499: train_loss=1.1167656183242798
INFO - 03/14/25 04:44:19 - 1:45:53 - Epoch 500: train_loss=1.116762638092041
INFO - 03/14/25 04:44:31 - 1:46:06 - Epoch 501: train_loss=1.1167566776275635
INFO - 03/14/25 04:44:44 - 1:46:18 - Epoch 502: train_loss=1.1166210174560547
INFO - 03/14/25 04:44:57 - 1:46:31 - Epoch 503: train_loss=1.1167526245117188
INFO - 03/14/25 04:45:09 - 1:46:44 - Epoch 504: train_loss=1.1164562702178955
INFO - 03/14/25 04:45:22 - 1:46:56 - Epoch 505: train_loss=1.1170685291290283
INFO - 03/14/25 04:45:35 - 1:47:09 - Epoch 506: train_loss=1.1167407035827637
INFO - 03/14/25 04:45:48 - 1:47:22 - Epoch 507: train_loss=1.1170635223388672
INFO - 03/14/25 04:46:00 - 1:47:34 - Epoch 508: train_loss=1.1168557405471802
INFO - 03/14/25 04:46:13 - 1:47:47 - Epoch 509: train_loss=1.116852045059204
INFO - 03/14/25 04:46:26 - 1:48:00 - Epoch 510: train_loss=1.116844654083252
INFO - 03/14/25 04:46:38 - 1:48:13 - Epoch 511: train_loss=1.116843819618225
INFO - 03/14/25 04:46:51 - 1:48:25 - Epoch 512: train_loss=1.1165893077850342
INFO - 03/14/25 04:47:04 - 1:48:38 - Epoch 513: train_loss=1.1168358325958252
INFO - 03/14/25 04:47:16 - 1:48:51 - Epoch 514: train_loss=1.1164244413375854
INFO - 03/14/25 04:47:29 - 1:49:03 - Epoch 515: train_loss=1.1170392036437988
INFO - 03/14/25 04:47:42 - 1:49:16 - Epoch 516: train_loss=1.1167103052139282
INFO - 03/14/25 04:47:54 - 1:49:29 - Epoch 517: train_loss=1.1170315742492676
INFO - 03/14/25 04:48:07 - 1:49:41 - Epoch 518: train_loss=1.116929054260254
INFO - 03/14/25 04:48:20 - 1:49:54 - Epoch 519: train_loss=1.1168171167373657
INFO - 03/14/25 04:48:32 - 1:50:07 - Epoch 520: train_loss=1.116815447807312
INFO - 03/14/25 04:48:45 - 1:50:19 - Epoch 521: train_loss=1.1166927814483643
INFO - 03/14/25 04:48:58 - 1:50:32 - Epoch 522: train_loss=1.116557240486145
INFO - 03/14/25 04:49:10 - 1:50:45 - Epoch 523: train_loss=1.1166861057281494
INFO - 03/14/25 04:49:23 - 1:50:57 - Epoch 524: train_loss=1.1163939237594604
INFO - 03/14/25 04:49:36 - 1:51:10 - Epoch 525: train_loss=1.1168009042739868
INFO - 03/14/25 04:49:49 - 1:51:23 - Epoch 526: train_loss=1.1165426969528198
INFO - 03/14/25 04:50:01 - 1:51:36 - Epoch 527: train_loss=1.1169987916946411
INFO - 03/14/25 04:50:14 - 1:51:48 - Epoch 528: train_loss=1.1168971061706543
INFO - 03/14/25 04:50:27 - 1:52:01 - Epoch 529: train_loss=1.1165335178375244
INFO - 03/14/25 04:50:39 - 1:52:14 - Epoch 530: train_loss=1.1166636943817139
INFO - 03/14/25 04:50:52 - 1:52:26 - Epoch 531: train_loss=1.1163697242736816
INFO - 03/14/25 04:51:05 - 1:52:39 - Epoch 532: train_loss=1.116775631904602
INFO - 03/14/25 04:51:17 - 1:52:52 - Epoch 533: train_loss=1.1163617372512817
INFO - 03/14/25 04:51:30 - 1:53:04 - Epoch 534: train_loss=1.1170706748962402
INFO - 03/14/25 04:51:43 - 1:53:17 - Epoch 535: train_loss=1.1169723272323608
INFO - 03/14/25 04:51:56 - 1:53:30 - Epoch 536: train_loss=1.1165106296539307
INFO - 03/14/25 04:52:08 - 1:53:42 - Epoch 537: train_loss=1.1166410446166992
INFO - 03/14/25 04:52:21 - 1:53:55 - Epoch 538: train_loss=1.116346836090088
INFO - 03/14/25 04:52:34 - 1:54:08 - Epoch 539: train_loss=1.1167527437210083
INFO - 03/14/25 04:52:46 - 1:54:20 - Epoch 540: train_loss=1.1163384914398193
INFO - 03/14/25 04:52:59 - 1:54:33 - Epoch 541: train_loss=1.1170475482940674
INFO - 03/14/25 04:53:12 - 1:54:46 - Epoch 542: train_loss=1.116949200630188
INFO - 03/14/25 04:53:24 - 1:54:59 - Epoch 543: train_loss=1.116619348526001
INFO - 03/14/25 04:53:37 - 1:55:11 - Epoch 544: train_loss=1.1167340278625488
INFO - 03/14/25 04:53:50 - 1:55:24 - Epoch 545: train_loss=1.1164813041687012
INFO - 03/14/25 04:54:02 - 1:55:37 - Epoch 546: train_loss=1.1164765357971191
INFO - 03/14/25 04:54:15 - 1:55:49 - Epoch 547: train_loss=1.1163173913955688
INFO - 03/14/25 04:54:28 - 1:56:02 - Epoch 548: train_loss=1.116721272468567
INFO - 03/14/25 04:54:40 - 1:56:15 - Epoch 549: train_loss=1.116599440574646
INFO - 03/14/25 04:54:53 - 1:56:27 - Epoch 550: train_loss=1.116463541984558
INFO - 03/14/25 04:55:06 - 1:56:40 - Epoch 551: train_loss=1.1164581775665283
INFO - 03/14/25 04:55:19 - 1:56:53 - Epoch 552: train_loss=1.1162996292114258
INFO - 03/14/25 04:55:31 - 1:57:05 - Epoch 553: train_loss=1.1164523363113403
INFO - 03/14/25 04:55:44 - 1:57:18 - Epoch 554: train_loss=1.116287350654602
INFO - 03/14/25 04:55:57 - 1:57:31 - Epoch 555: train_loss=1.1165798902511597
INFO - 03/14/25 04:56:09 - 1:57:44 - Epoch 556: train_loss=1.1162803173065186
INFO - 03/14/25 04:56:22 - 1:57:56 - Epoch 557: train_loss=1.1166894435882568
INFO - 03/14/25 04:56:35 - 1:58:09 - Epoch 558: train_loss=1.1164324283599854
INFO - 03/14/25 04:56:47 - 1:58:22 - Epoch 559: train_loss=1.1166801452636719
INFO - 03/14/25 04:57:00 - 1:58:34 - Epoch 560: train_loss=1.116560459136963
INFO - 03/14/25 04:57:13 - 1:58:47 - Epoch 561: train_loss=1.1164212226867676
INFO - 03/14/25 04:57:25 - 1:59:00 - Epoch 562: train_loss=1.1164164543151855
INFO - 03/14/25 04:57:38 - 1:59:12 - Epoch 563: train_loss=1.1164121627807617
INFO - 03/14/25 04:57:51 - 1:59:25 - Epoch 564: train_loss=1.1162501573562622
INFO - 03/14/25 04:58:03 - 1:59:38 - Epoch 565: train_loss=1.1164041757583618
INFO - 03/14/25 04:58:16 - 1:59:50 - Epoch 566: train_loss=1.1162395477294922
INFO - 03/14/25 04:58:29 - 2:00:03 - Epoch 567: train_loss=1.1163954734802246
INFO - 03/14/25 04:58:42 - 2:00:16 - Epoch 568: train_loss=1.1160266399383545
INFO - 03/14/25 04:58:54 - 2:00:29 - Epoch 569: train_loss=1.1167479753494263
INFO - 03/14/25 04:59:07 - 2:00:41 - Epoch 570: train_loss=1.116633415222168
INFO - 03/14/25 04:59:20 - 2:00:54 - Epoch 571: train_loss=1.1162179708480835
INFO - 03/14/25 04:59:32 - 2:01:07 - Epoch 572: train_loss=1.1162140369415283
INFO - 03/14/25 04:59:45 - 2:01:19 - Epoch 573: train_loss=1.1162110567092896
INFO - 03/14/25 04:59:58 - 2:01:32 - Epoch 574: train_loss=1.1162008047103882
INFO - 03/14/25 05:00:10 - 2:01:45 - Epoch 575: train_loss=1.116199254989624
INFO - 03/14/25 05:00:23 - 2:01:57 - Epoch 576: train_loss=1.115987777709961
INFO - 03/14/25 05:00:36 - 2:02:10 - Epoch 577: train_loss=1.1163461208343506
INFO - 03/14/25 05:00:48 - 2:02:23 - Epoch 578: train_loss=1.1159770488739014
INFO - 03/14/25 05:01:01 - 2:02:35 - Epoch 579: train_loss=1.1165910959243774
INFO - 03/14/25 05:01:14 - 2:02:48 - Epoch 580: train_loss=1.116581678390503
INFO - 03/14/25 05:01:27 - 2:03:01 - Epoch 581: train_loss=1.1161655187606812
INFO - 03/14/25 05:01:39 - 2:03:13 - Epoch 582: train_loss=1.1161643266677856
INFO - 03/14/25 05:01:52 - 2:03:26 - Epoch 583: train_loss=1.116155743598938
INFO - 03/14/25 05:02:05 - 2:03:39 - Epoch 584: train_loss=1.116151213645935
INFO - 03/14/25 05:02:17 - 2:03:52 - Epoch 585: train_loss=1.1161442995071411
INFO - 03/14/25 05:02:30 - 2:04:04 - Epoch 586: train_loss=1.1161390542984009
INFO - 03/14/25 05:02:43 - 2:04:17 - Epoch 587: train_loss=1.1161324977874756
INFO - 03/14/25 05:02:55 - 2:04:30 - Epoch 588: train_loss=1.116126537322998
INFO - 03/14/25 05:03:08 - 2:04:42 - Epoch 589: train_loss=1.1159183979034424
INFO - 03/14/25 05:03:21 - 2:04:55 - Epoch 590: train_loss=1.1161161661148071
INFO - 03/14/25 05:03:33 - 2:05:08 - Epoch 591: train_loss=1.1161093711853027
INFO - 03/14/25 05:03:46 - 2:05:20 - Epoch 592: train_loss=1.1161059141159058
INFO - 03/14/25 05:03:59 - 2:05:33 - Epoch 593: train_loss=1.1161035299301147
INFO - 03/14/25 05:04:11 - 2:05:46 - Epoch 594: train_loss=1.1165133714675903
INFO - 03/14/25 05:04:24 - 2:05:58 - Epoch 595: train_loss=1.116089105606079
INFO - 03/14/25 05:04:37 - 2:06:11 - Epoch 596: train_loss=1.1167083978652954
INFO - 03/14/25 05:04:50 - 2:06:24 - Epoch 597: train_loss=1.1160855293273926
INFO - 03/14/25 05:05:02 - 2:06:36 - Epoch 598: train_loss=1.1160807609558105
INFO - 03/14/25 05:05:15 - 2:06:49 - Epoch 599: train_loss=1.1164873838424683
INFO - 03/14/25 05:05:28 - 2:07:02 - Epoch 600: train_loss=1.1158698797225952
INFO - 03/14/25 05:05:40 - 2:07:15 - Epoch 601: train_loss=1.1166993379592896
INFO - 03/14/25 05:05:53 - 2:07:27 - Epoch 602: train_loss=1.1164923906326294
INFO - 03/14/25 05:06:06 - 2:07:40 - Epoch 603: train_loss=1.1160732507705688
INFO - 03/14/25 05:06:18 - 2:07:53 - Epoch 604: train_loss=1.1162291765213013
INFO - 03/14/25 05:06:31 - 2:08:05 - Epoch 605: train_loss=1.1162316799163818
INFO - 03/14/25 05:06:44 - 2:08:18 - Epoch 606: train_loss=1.1160683631896973
INFO - 03/14/25 05:06:56 - 2:08:31 - Epoch 607: train_loss=1.1160683631896973
INFO - 03/14/25 05:07:09 - 2:08:43 - Epoch 608: train_loss=1.115859866142273
INFO - 03/14/25 05:07:22 - 2:08:56 - Epoch 609: train_loss=1.1162221431732178
INFO - 03/14/25 05:07:34 - 2:09:09 - Epoch 610: train_loss=1.1160579919815063
INFO - 03/14/25 05:07:47 - 2:09:21 - Epoch 611: train_loss=1.11605703830719
INFO - 03/14/25 05:08:00 - 2:09:34 - Epoch 612: train_loss=1.1160500049591064
INFO - 03/14/25 05:08:12 - 2:09:47 - Epoch 613: train_loss=1.1160473823547363
INFO - 03/14/25 05:08:25 - 2:09:59 - Epoch 614: train_loss=1.116044282913208
INFO - 03/14/25 05:08:38 - 2:10:12 - Epoch 615: train_loss=1.1158336400985718
INFO - 03/14/25 05:08:51 - 2:10:25 - Epoch 616: train_loss=1.11603581905365
INFO - 03/14/25 05:09:03 - 2:10:37 - Epoch 617: train_loss=1.1158249378204346
INFO - 03/14/25 05:09:16 - 2:10:50 - Epoch 618: train_loss=1.1160269975662231
INFO - 03/14/25 05:09:29 - 2:11:03 - Epoch 619: train_loss=1.1158150434494019
INFO - 03/14/25 05:09:41 - 2:11:16 - Epoch 620: train_loss=1.11601984500885
INFO - 03/14/25 05:09:54 - 2:11:28 - Epoch 621: train_loss=1.1158075332641602
INFO - 03/14/25 05:10:07 - 2:11:41 - Epoch 622: train_loss=1.1160099506378174
INFO - 03/14/25 05:10:19 - 2:11:54 - Epoch 623: train_loss=1.115799903869629
INFO - 03/14/25 05:10:32 - 2:12:06 - Epoch 624: train_loss=1.116004467010498
INFO - 03/14/25 05:10:45 - 2:12:19 - Epoch 625: train_loss=1.1159956455230713
INFO - 03/14/25 05:10:57 - 2:12:32 - Epoch 626: train_loss=1.1159915924072266
INFO - 03/14/25 05:11:10 - 2:12:44 - Epoch 627: train_loss=1.1159887313842773
INFO - 03/14/25 05:11:23 - 2:12:57 - Epoch 628: train_loss=1.1159839630126953
INFO - 03/14/25 05:11:35 - 2:13:10 - Epoch 629: train_loss=1.1159800291061401
INFO - 03/14/25 05:11:48 - 2:13:22 - Epoch 630: train_loss=1.1159756183624268
INFO - 03/14/25 05:12:01 - 2:13:35 - Epoch 631: train_loss=1.115769386291504
INFO - 03/14/25 05:12:14 - 2:13:48 - Epoch 632: train_loss=1.1159690618515015
INFO - 03/14/25 05:12:26 - 2:14:01 - Epoch 633: train_loss=1.115761160850525
INFO - 03/14/25 05:12:39 - 2:14:13 - Epoch 634: train_loss=1.115963339805603
INFO - 03/14/25 05:12:52 - 2:14:26 - Epoch 635: train_loss=1.115754246711731
INFO - 03/14/25 05:13:04 - 2:14:39 - Epoch 636: train_loss=1.1159577369689941
INFO - 03/14/25 05:13:17 - 2:14:51 - Epoch 637: train_loss=1.1157485246658325
INFO - 03/14/25 05:13:30 - 2:15:04 - Epoch 638: train_loss=1.1161079406738281
INFO - 03/14/25 05:13:42 - 2:15:17 - Epoch 639: train_loss=1.116105318069458
INFO - 03/14/25 05:13:55 - 2:15:29 - Epoch 640: train_loss=1.1157371997833252
INFO - 03/14/25 05:14:08 - 2:15:42 - Epoch 641: train_loss=1.1159405708312988
INFO - 03/14/25 05:14:20 - 2:15:55 - Epoch 642: train_loss=1.1157305240631104
INFO - 03/14/25 05:14:33 - 2:16:07 - Epoch 643: train_loss=1.1159344911575317
INFO - 03/14/25 05:14:46 - 2:16:20 - Epoch 644: train_loss=1.1157255172729492
INFO - 03/14/25 05:14:59 - 2:16:33 - Epoch 645: train_loss=1.1159294843673706
INFO - 03/14/25 05:15:11 - 2:16:45 - Epoch 646: train_loss=1.1159242391586304
INFO - 03/14/25 05:15:24 - 2:16:58 - Epoch 647: train_loss=1.1159191131591797
INFO - 03/14/25 05:15:37 - 2:17:11 - Epoch 648: train_loss=1.1159186363220215
INFO - 03/14/25 05:15:49 - 2:17:24 - Epoch 649: train_loss=1.1157091856002808
INFO - 03/14/25 05:16:02 - 2:17:36 - Epoch 650: train_loss=1.1159113645553589
INFO - 03/14/25 05:16:15 - 2:17:49 - Epoch 651: train_loss=1.11570405960083
INFO - 03/14/25 05:16:27 - 2:18:02 - Epoch 652: train_loss=1.115905523300171
INFO - 03/14/25 05:16:40 - 2:18:14 - Epoch 653: train_loss=1.115902304649353
INFO - 03/14/25 05:16:53 - 2:18:27 - Epoch 654: train_loss=1.1156940460205078
INFO - 03/14/25 05:17:05 - 2:18:40 - Epoch 655: train_loss=1.1158950328826904
INFO - 03/14/25 05:17:18 - 2:18:52 - Epoch 656: train_loss=1.115687370300293
INFO - 03/14/25 05:17:31 - 2:19:05 - Epoch 657: train_loss=1.1158887147903442
INFO - 03/14/25 05:17:43 - 2:19:18 - Epoch 658: train_loss=1.1156821250915527
INFO - 03/14/25 05:17:56 - 2:19:30 - Epoch 659: train_loss=1.1158826351165771
INFO - 03/14/25 05:18:09 - 2:19:43 - Epoch 660: train_loss=1.1158802509307861
INFO - 03/14/25 05:18:22 - 2:19:56 - Epoch 661: train_loss=1.11567223072052
INFO - 03/14/25 05:18:34 - 2:20:09 - Epoch 662: train_loss=1.1158736944198608
INFO - 03/14/25 05:18:47 - 2:20:21 - Epoch 663: train_loss=1.1156662702560425
INFO - 03/14/25 05:19:00 - 2:20:34 - Epoch 664: train_loss=1.115869164466858
INFO - 03/14/25 05:19:12 - 2:20:47 - Epoch 665: train_loss=1.115864634513855
INFO - 03/14/25 05:19:25 - 2:20:59 - Epoch 666: train_loss=1.115658164024353
INFO - 03/14/25 05:19:38 - 2:21:12 - Epoch 667: train_loss=1.115858793258667
INFO - 03/14/25 05:19:50 - 2:21:25 - Epoch 668: train_loss=1.1156518459320068
INFO - 03/14/25 05:20:03 - 2:21:37 - Epoch 669: train_loss=1.1158562898635864
INFO - 03/14/25 05:20:16 - 2:21:50 - Epoch 670: train_loss=1.1156476736068726
INFO - 03/14/25 05:20:29 - 2:22:03 - Epoch 671: train_loss=1.1160075664520264
INFO - 03/14/25 05:20:41 - 2:22:15 - Epoch 672: train_loss=1.115849256515503
INFO - 03/14/25 05:20:54 - 2:22:28 - Epoch 673: train_loss=1.1158448457717896
INFO - 03/14/25 05:21:07 - 2:22:41 - Epoch 674: train_loss=1.1158428192138672
INFO - 03/14/25 05:21:19 - 2:22:54 - Epoch 675: train_loss=1.1158407926559448
INFO - 03/14/25 05:21:32 - 2:23:06 - Epoch 676: train_loss=1.1156320571899414
INFO - 03/14/25 05:21:45 - 2:23:19 - Epoch 677: train_loss=1.1158381700515747
INFO - 03/14/25 05:21:57 - 2:23:32 - Epoch 678: train_loss=1.1156270503997803
INFO - 03/14/25 05:22:10 - 2:23:44 - Epoch 679: train_loss=1.1161227226257324
INFO - 03/14/25 05:22:23 - 2:23:57 - Epoch 680: train_loss=1.1159892082214355
INFO - 03/14/25 05:22:35 - 2:24:10 - Epoch 681: train_loss=1.1156210899353027
INFO - 03/14/25 05:22:48 - 2:24:22 - Epoch 682: train_loss=1.115826964378357
INFO - 03/14/25 05:23:01 - 2:24:35 - Epoch 683: train_loss=1.1158263683319092
INFO - 03/14/25 05:23:14 - 2:24:48 - Epoch 684: train_loss=1.1158207654953003
INFO - 03/14/25 05:23:26 - 2:25:00 - Epoch 685: train_loss=1.115613341331482
INFO - 03/14/25 05:23:39 - 2:25:13 - Epoch 686: train_loss=1.1158190965652466
INFO - 03/14/25 05:23:52 - 2:25:26 - Epoch 687: train_loss=1.1158148050308228
INFO - 03/14/25 05:24:04 - 2:25:39 - Epoch 688: train_loss=1.1156095266342163
INFO - 03/14/25 05:24:17 - 2:25:51 - Epoch 689: train_loss=1.1156086921691895
INFO - 03/14/25 05:24:30 - 2:26:04 - Epoch 690: train_loss=1.115607500076294
INFO - 03/14/25 05:24:42 - 2:26:17 - Epoch 691: train_loss=1.1159723997116089
INFO - 03/14/25 05:24:55 - 2:26:29 - Epoch 692: train_loss=1.1158092021942139
INFO - 03/14/25 05:25:08 - 2:26:42 - Epoch 693: train_loss=1.1159685850143433
INFO - 03/14/25 05:25:21 - 2:26:55 - Epoch 694: train_loss=1.1156034469604492
INFO - 03/14/25 05:25:33 - 2:27:08 - Epoch 695: train_loss=1.115970492362976
INFO - 03/14/25 05:25:46 - 2:27:20 - Epoch 696: train_loss=1.1158080101013184
INFO - 03/14/25 05:25:59 - 2:27:33 - Epoch 697: train_loss=1.115808367729187
INFO - 03/14/25 05:26:11 - 2:27:46 - Epoch 698: train_loss=1.1158056259155273
INFO - 03/14/25 05:26:24 - 2:27:58 - Epoch 699: train_loss=1.115808129310608
INFO - 03/14/25 05:26:37 - 2:28:11 - Epoch 700: train_loss=1.1158052682876587
INFO - 03/14/25 05:26:49 - 2:28:24 - Epoch 701: train_loss=1.1155999898910522
INFO - 03/14/25 05:27:02 - 2:28:36 - Epoch 702: train_loss=1.1158047914505005
INFO - 03/14/25 05:27:15 - 2:28:49 - Epoch 703: train_loss=1.1156002283096313
INFO - 03/14/25 05:27:27 - 2:29:02 - Epoch 704: train_loss=1.115803837776184
INFO - 03/14/25 05:27:40 - 2:29:14 - Epoch 705: train_loss=1.115798830986023
INFO - 03/14/25 05:27:53 - 2:29:27 - Epoch 706: train_loss=1.115800142288208
INFO - 03/14/25 05:28:06 - 2:29:40 - Epoch 707: train_loss=1.1157983541488647
INFO - 03/14/25 05:28:18 - 2:29:52 - Epoch 708: train_loss=1.1155914068222046
INFO - 03/14/25 05:28:31 - 2:30:05 - Epoch 709: train_loss=1.1155893802642822
INFO - 03/14/25 05:28:44 - 2:30:18 - Epoch 710: train_loss=1.1155887842178345
INFO - 03/14/25 05:28:56 - 2:30:31 - Epoch 711: train_loss=1.115585446357727
INFO - 03/14/25 05:29:09 - 2:30:43 - Epoch 712: train_loss=1.1155836582183838
INFO - 03/14/25 05:29:22 - 2:30:56 - Epoch 713: train_loss=1.1150926351547241
INFO - 03/14/25 05:29:34 - 2:31:09 - Epoch 714: train_loss=1.1157883405685425
INFO - 03/14/25 05:29:47 - 2:31:21 - Epoch 715: train_loss=1.115087866783142
INFO - 03/14/25 05:30:00 - 2:31:34 - Epoch 716: train_loss=1.11619234085083
INFO - 03/14/25 05:30:13 - 2:31:47 - Epoch 717: train_loss=1.1157782077789307
INFO - 03/14/25 05:30:25 - 2:32:00 - Epoch 718: train_loss=1.1159379482269287
INFO - 03/14/25 05:30:38 - 2:32:12 - Epoch 719: train_loss=1.1159355640411377
INFO - 03/14/25 05:30:51 - 2:32:25 - Epoch 720: train_loss=1.1157714128494263
INFO - 03/14/25 05:31:03 - 2:32:38 - Epoch 721: train_loss=1.1157740354537964
INFO - 03/14/25 05:31:16 - 2:32:50 - Epoch 722: train_loss=1.1155648231506348
INFO - 03/14/25 05:31:29 - 2:33:03 - Epoch 723: train_loss=1.1157677173614502
INFO - 03/14/25 05:31:41 - 2:33:16 - Epoch 724: train_loss=1.115561604499817
INFO - 03/14/25 05:31:54 - 2:33:28 - Epoch 725: train_loss=1.1157652139663696
INFO - 03/14/25 05:32:07 - 2:33:41 - Epoch 726: train_loss=1.1157610416412354
INFO - 03/14/25 05:32:20 - 2:33:54 - Epoch 727: train_loss=1.1157597303390503
INFO - 03/14/25 05:32:32 - 2:34:07 - Epoch 728: train_loss=1.1157584190368652
INFO - 03/14/25 05:32:45 - 2:34:19 - Epoch 729: train_loss=1.115552544593811
INFO - 03/14/25 05:32:58 - 2:34:32 - Epoch 730: train_loss=1.1155518293380737
INFO - 03/14/25 05:33:10 - 2:34:45 - Epoch 731: train_loss=1.1155483722686768
INFO - 03/14/25 05:33:23 - 2:34:57 - Epoch 732: train_loss=1.1155484914779663
INFO - 03/14/25 05:33:36 - 2:35:10 - Epoch 733: train_loss=1.1155447959899902
INFO - 03/14/25 05:33:48 - 2:35:23 - Epoch 734: train_loss=1.1155438423156738
INFO - 03/14/25 05:34:01 - 2:35:35 - Epoch 735: train_loss=1.1155424118041992
INFO - 03/14/25 05:34:14 - 2:35:48 - Epoch 736: train_loss=1.1150511503219604
INFO - 03/14/25 05:34:27 - 2:36:01 - Epoch 737: train_loss=1.1157443523406982
INFO - 03/14/25 05:34:39 - 2:36:14 - Epoch 738: train_loss=1.1150472164154053
INFO - 03/14/25 05:34:52 - 2:36:26 - Epoch 739: train_loss=1.1157441139221191
INFO - 03/14/25 05:35:05 - 2:36:39 - Epoch 740: train_loss=1.115044116973877
INFO - 03/14/25 05:35:17 - 2:36:52 - Epoch 741: train_loss=1.1158998012542725
INFO - 03/14/25 05:35:30 - 2:37:04 - Epoch 742: train_loss=1.1150414943695068
INFO - 03/14/25 05:35:43 - 2:37:17 - Epoch 743: train_loss=1.115739107131958
INFO - 03/14/25 05:35:55 - 2:37:30 - Epoch 744: train_loss=1.115038275718689
INFO - 03/14/25 05:36:08 - 2:37:42 - Epoch 745: train_loss=1.1162517070770264
INFO - 03/14/25 05:36:21 - 2:37:55 - Epoch 746: train_loss=1.1157310009002686
INFO - 03/14/25 05:36:34 - 2:38:08 - Epoch 747: train_loss=1.1161415576934814
INFO - 03/14/25 05:36:46 - 2:38:21 - Epoch 748: train_loss=1.116348147392273
INFO - 03/14/25 05:36:59 - 2:38:33 - Epoch 749: train_loss=1.1155203580856323
INFO - 03/14/25 05:37:12 - 2:38:46 - Epoch 750: train_loss=1.1162481307983398
INFO - 03/14/25 05:37:24 - 2:38:59 - Epoch 751: train_loss=1.1163439750671387
INFO - 03/14/25 05:37:37 - 2:39:11 - Epoch 752: train_loss=1.11551833152771
INFO - 03/14/25 05:37:50 - 2:39:24 - Epoch 753: train_loss=1.116137981414795
INFO - 03/14/25 05:38:02 - 2:39:37 - Epoch 754: train_loss=1.116245150566101
INFO - 03/14/25 05:38:15 - 2:39:49 - Epoch 755: train_loss=1.1157206296920776
INFO - 03/14/25 05:38:28 - 2:40:02 - Epoch 756: train_loss=1.1160141229629517
INFO - 03/14/25 05:38:40 - 2:40:15 - Epoch 757: train_loss=1.1161335706710815
INFO - 03/14/25 05:38:53 - 2:40:27 - Epoch 758: train_loss=1.1157177686691284
INFO - 03/14/25 05:39:06 - 2:40:40 - Epoch 759: train_loss=1.1158790588378906
INFO - 03/14/25 05:39:19 - 2:40:53 - Epoch 760: train_loss=1.1160123348236084
INFO - 03/14/25 05:39:31 - 2:41:05 - Epoch 761: train_loss=1.1155110597610474
INFO - 03/14/25 05:39:44 - 2:41:18 - Epoch 762: train_loss=1.1158767938613892
INFO - 03/14/25 05:39:57 - 2:41:31 - Epoch 763: train_loss=1.1158744096755981
INFO - 03/14/25 05:40:09 - 2:41:44 - Epoch 764: train_loss=1.115507960319519
INFO - 03/14/25 05:40:22 - 2:41:56 - Epoch 765: train_loss=1.115715742111206
INFO - 03/14/25 05:40:35 - 2:42:09 - Epoch 766: train_loss=1.115506649017334
INFO - 03/14/25 05:40:47 - 2:42:22 - Epoch 767: train_loss=1.115710735321045
INFO - 03/14/25 05:41:00 - 2:42:34 - Epoch 768: train_loss=1.1157095432281494
INFO - 03/14/25 05:41:13 - 2:42:47 - Epoch 769: train_loss=1.1150137186050415
INFO - 03/14/25 05:41:26 - 2:43:00 - Epoch 770: train_loss=1.1161189079284668
INFO - 03/14/25 05:41:38 - 2:43:13 - Epoch 771: train_loss=1.115709662437439
INFO - 03/14/25 05:41:51 - 2:43:25 - Epoch 772: train_loss=1.1157026290893555
INFO - 03/14/25 05:42:04 - 2:43:38 - Epoch 773: train_loss=1.1159963607788086
INFO - 03/14/25 05:42:16 - 2:43:51 - Epoch 774: train_loss=1.1154972314834595
INFO - 03/14/25 05:42:29 - 2:44:03 - Epoch 775: train_loss=1.1158612966537476
INFO - 03/14/25 05:42:42 - 2:44:16 - Epoch 776: train_loss=1.1161130666732788
INFO - 03/14/25 05:42:54 - 2:44:29 - Epoch 777: train_loss=1.115493655204773
INFO - 03/14/25 05:43:07 - 2:44:41 - Epoch 778: train_loss=1.1159902811050415
INFO - 03/14/25 05:43:20 - 2:44:54 - Epoch 779: train_loss=1.1162190437316895
INFO - 03/14/25 05:43:33 - 2:45:07 - Epoch 780: train_loss=1.115695834159851
INFO - 03/14/25 05:43:45 - 2:45:20 - Epoch 781: train_loss=1.1156984567642212
INFO - 03/14/25 05:43:58 - 2:45:32 - Epoch 782: train_loss=1.115990161895752
INFO - 03/14/25 05:44:11 - 2:45:45 - Epoch 783: train_loss=1.1154879331588745
INFO - 03/14/25 05:44:23 - 2:45:58 - Epoch 784: train_loss=1.1158525943756104
INFO - 03/14/25 05:44:36 - 2:46:10 - Epoch 785: train_loss=1.1161035299301147
INFO - 03/14/25 05:44:49 - 2:46:23 - Epoch 786: train_loss=1.1154855489730835
INFO - 03/14/25 05:45:01 - 2:46:36 - Epoch 787: train_loss=1.1158521175384521
INFO - 03/14/25 05:45:14 - 2:46:48 - Epoch 788: train_loss=1.1161046028137207
INFO - 03/14/25 05:45:27 - 2:47:01 - Epoch 789: train_loss=1.1154839992523193
INFO - 03/14/25 05:45:39 - 2:47:14 - Epoch 790: train_loss=1.1159809827804565
INFO - 03/14/25 05:45:52 - 2:47:26 - Epoch 791: train_loss=1.1163990497589111
INFO - 03/14/25 05:46:05 - 2:47:39 - Epoch 792: train_loss=1.1159791946411133
INFO - 03/14/25 05:46:18 - 2:47:52 - Epoch 793: train_loss=1.115479826927185
INFO - 03/14/25 05:46:30 - 2:48:04 - Epoch 794: train_loss=1.1158461570739746
INFO - 03/14/25 05:46:43 - 2:48:17 - Epoch 795: train_loss=1.1156843900680542
INFO - 03/14/25 05:46:56 - 2:48:30 - Epoch 796: train_loss=1.1154775619506836
INFO - 03/14/25 05:47:08 - 2:48:43 - Epoch 797: train_loss=1.1156821250915527
INFO - 03/14/25 05:47:21 - 2:48:55 - Epoch 798: train_loss=1.1149868965148926
INFO - 03/14/25 05:47:34 - 2:49:08 - Epoch 799: train_loss=1.1156798601150513
INFO - 03/14/25 05:47:46 - 2:49:21 - Epoch 800: train_loss=1.1154725551605225
INFO - 03/14/25 05:47:59 - 2:49:33 - Epoch 801: train_loss=1.1154718399047852
INFO - 03/14/25 05:48:12 - 2:49:46 - Epoch 802: train_loss=1.1154701709747314
INFO - 03/14/25 05:48:24 - 2:49:59 - Epoch 803: train_loss=1.1149797439575195
INFO - 03/14/25 05:48:37 - 2:50:11 - Epoch 804: train_loss=1.1158338785171509
INFO - 03/14/25 05:48:50 - 2:50:24 - Epoch 805: train_loss=1.1156708002090454
INFO - 03/14/25 05:49:03 - 2:50:37 - Epoch 806: train_loss=1.1154648065567017
INFO - 03/14/25 05:49:15 - 2:50:50 - Epoch 807: train_loss=1.115667462348938
INFO - 03/14/25 05:49:28 - 2:51:02 - Epoch 808: train_loss=1.1149723529815674
INFO - 03/14/25 05:49:41 - 2:51:15 - Epoch 809: train_loss=1.1156657934188843
INFO - 03/14/25 05:49:53 - 2:51:28 - Epoch 810: train_loss=1.1149702072143555
INFO - 03/14/25 05:50:06 - 2:51:40 - Epoch 811: train_loss=1.1154576539993286
INFO - 03/14/25 05:50:19 - 2:51:53 - Epoch 812: train_loss=1.1149674654006958
INFO - 03/14/25 05:50:31 - 2:52:06 - Epoch 813: train_loss=1.1156642436981201
INFO - 03/14/25 05:50:44 - 2:52:18 - Epoch 814: train_loss=1.1149641275405884
INFO - 03/14/25 05:50:57 - 2:52:31 - Epoch 815: train_loss=1.1156601905822754
INFO - 03/14/25 05:51:10 - 2:52:44 - Epoch 816: train_loss=1.115452766418457
INFO - 03/14/25 05:51:22 - 2:52:57 - Epoch 817: train_loss=1.1154495477676392
INFO - 03/14/25 05:51:35 - 2:53:09 - Epoch 818: train_loss=1.115450382232666
INFO - 03/14/25 05:51:48 - 2:53:22 - Epoch 819: train_loss=1.115450143814087
INFO - 03/14/25 05:52:00 - 2:53:35 - Epoch 820: train_loss=1.1149582862854004
INFO - 03/14/25 05:52:13 - 2:53:47 - Epoch 821: train_loss=1.1156569719314575
INFO - 03/14/25 05:52:26 - 2:54:00 - Epoch 822: train_loss=1.1149561405181885
INFO - 03/14/25 05:52:38 - 2:54:13 - Epoch 823: train_loss=1.115654706954956
INFO - 03/14/25 05:52:51 - 2:54:25 - Epoch 824: train_loss=1.1149561405181885
INFO - 03/14/25 05:53:04 - 2:54:38 - Epoch 825: train_loss=1.1156500577926636
INFO - 03/14/25 05:53:16 - 2:54:51 - Epoch 826: train_loss=1.1149547100067139
INFO - 03/14/25 05:53:29 - 2:55:03 - Epoch 827: train_loss=1.1154452562332153
INFO - 03/14/25 05:53:42 - 2:55:16 - Epoch 828: train_loss=1.1149531602859497
INFO - 03/14/25 05:53:55 - 2:55:29 - Epoch 829: train_loss=1.115651249885559
INFO - 03/14/25 05:54:07 - 2:55:42 - Epoch 830: train_loss=1.114951729774475
INFO - 03/14/25 05:54:20 - 2:55:54 - Epoch 831: train_loss=1.1156483888626099
INFO - 03/14/25 05:54:33 - 2:56:07 - Epoch 832: train_loss=1.1149506568908691
INFO - 03/14/25 05:54:45 - 2:56:20 - Epoch 833: train_loss=1.115440011024475
INFO - 03/14/25 05:54:58 - 2:56:32 - Epoch 834: train_loss=1.1149483919143677
INFO - 03/14/25 05:55:11 - 2:56:45 - Epoch 835: train_loss=1.1156418323516846
INFO - 03/14/25 05:55:23 - 2:56:58 - Epoch 836: train_loss=1.1149464845657349
INFO - 03/14/25 05:55:36 - 2:57:10 - Epoch 837: train_loss=1.1154354810714722
INFO - 03/14/25 05:55:49 - 2:57:23 - Epoch 838: train_loss=1.114944577217102
INFO - 03/14/25 05:56:01 - 2:57:36 - Epoch 839: train_loss=1.1156426668167114
INFO - 03/14/25 05:56:14 - 2:57:48 - Epoch 840: train_loss=1.1149414777755737
INFO - 03/14/25 05:56:27 - 2:58:01 - Epoch 841: train_loss=1.1156374216079712
INFO - 03/14/25 05:56:40 - 2:58:14 - Epoch 842: train_loss=1.1149396896362305
INFO - 03/14/25 05:56:52 - 2:58:26 - Epoch 843: train_loss=1.1154296398162842
INFO - 03/14/25 05:57:05 - 2:58:39 - Epoch 844: train_loss=1.1149369478225708
INFO - 03/14/25 05:57:18 - 2:58:52 - Epoch 845: train_loss=1.1157907247543335
INFO - 03/14/25 05:57:30 - 2:59:05 - Epoch 846: train_loss=1.1154240369796753
INFO - 03/14/25 05:57:43 - 2:59:17 - Epoch 847: train_loss=1.1156339645385742
INFO - 03/14/25 05:57:56 - 2:59:30 - Epoch 848: train_loss=1.1156284809112549
INFO - 03/14/25 05:58:08 - 2:59:43 - Epoch 849: train_loss=1.1156262159347534
INFO - 03/14/25 05:58:21 - 2:59:55 - Epoch 850: train_loss=1.1156270503997803
INFO - 03/14/25 05:58:34 - 3:00:08 - Epoch 851: train_loss=1.1154216527938843
INFO - 03/14/25 05:58:46 - 3:00:21 - Epoch 852: train_loss=1.1154205799102783
INFO - 03/14/25 05:58:59 - 3:00:33 - Epoch 853: train_loss=1.115622878074646
INFO - 03/14/25 05:59:12 - 3:00:46 - Epoch 854: train_loss=1.1154179573059082
INFO - 03/14/25 05:59:25 - 3:00:59 - Epoch 855: train_loss=1.115622639656067
INFO - 03/14/25 05:59:37 - 3:01:12 - Epoch 856: train_loss=1.1154179573059082
INFO - 03/14/25 05:59:50 - 3:01:24 - Epoch 857: train_loss=1.1156219244003296
INFO - 03/14/25 06:00:03 - 3:01:37 - Epoch 858: train_loss=1.1156206130981445
INFO - 03/14/25 06:00:15 - 3:01:50 - Epoch 859: train_loss=1.1154145002365112
INFO - 03/14/25 06:00:28 - 3:02:02 - Epoch 860: train_loss=1.1154142618179321
INFO - 03/14/25 06:00:41 - 3:02:15 - Epoch 861: train_loss=1.1154133081436157
INFO - 03/14/25 06:00:54 - 3:02:28 - Epoch 862: train_loss=1.1149224042892456
INFO - 03/14/25 06:01:06 - 3:02:40 - Epoch 863: train_loss=1.1156162023544312
INFO - 03/14/25 06:01:19 - 3:02:53 - Epoch 864: train_loss=1.1149194240570068
INFO - 03/14/25 06:01:32 - 3:03:06 - Epoch 865: train_loss=1.1154099702835083
INFO - 03/14/25 06:01:44 - 3:03:19 - Epoch 866: train_loss=1.1149179935455322
INFO - 03/14/25 06:01:57 - 3:03:31 - Epoch 867: train_loss=1.115618348121643
INFO - 03/14/25 06:02:10 - 3:03:44 - Epoch 868: train_loss=1.114915132522583
INFO - 03/14/25 06:02:22 - 3:03:57 - Epoch 869: train_loss=1.1156132221221924
INFO - 03/14/25 06:02:35 - 3:04:09 - Epoch 870: train_loss=1.114915132522583
INFO - 03/14/25 06:02:48 - 3:04:22 - Epoch 871: train_loss=1.1149142980575562
INFO - 03/14/25 06:03:01 - 3:04:35 - Epoch 872: train_loss=1.115608811378479
INFO - 03/14/25 06:03:13 - 3:04:48 - Epoch 873: train_loss=1.1149119138717651
INFO - 03/14/25 06:03:26 - 3:05:00 - Epoch 874: train_loss=1.1156072616577148
INFO - 03/14/25 06:03:39 - 3:05:13 - Epoch 875: train_loss=1.1149104833602905
INFO - 03/14/25 06:03:51 - 3:05:26 - Epoch 876: train_loss=1.1153998374938965
INFO - 03/14/25 06:04:04 - 3:05:38 - Epoch 877: train_loss=1.1149084568023682
INFO - 03/14/25 06:04:17 - 3:05:51 - Epoch 878: train_loss=1.1156047582626343
INFO - 03/14/25 06:04:29 - 3:06:04 - Epoch 879: train_loss=1.1149060726165771
INFO - 03/14/25 06:04:42 - 3:06:16 - Epoch 880: train_loss=1.1155999898910522
INFO - 03/14/25 06:04:55 - 3:06:29 - Epoch 881: train_loss=1.1149042844772339
INFO - 03/14/25 06:05:08 - 3:06:42 - Epoch 882: train_loss=1.1149039268493652
INFO - 03/14/25 06:05:20 - 3:06:55 - Epoch 883: train_loss=1.1155974864959717
INFO - 03/14/25 06:05:33 - 3:07:07 - Epoch 884: train_loss=1.1149003505706787
INFO - 03/14/25 06:05:46 - 3:07:20 - Epoch 885: train_loss=1.1155961751937866
INFO - 03/14/25 06:05:58 - 3:07:33 - Epoch 886: train_loss=1.114898681640625
INFO - 03/14/25 06:06:11 - 3:07:45 - Epoch 887: train_loss=1.115387201309204
INFO - 03/14/25 06:06:24 - 3:07:58 - Epoch 888: train_loss=1.1148960590362549
INFO - 03/14/25 06:06:36 - 3:08:11 - Epoch 889: train_loss=1.1155893802642822
INFO - 03/14/25 06:06:49 - 3:08:23 - Epoch 890: train_loss=1.114893913269043
INFO - 03/14/25 06:07:02 - 3:08:36 - Epoch 891: train_loss=1.1153829097747803
INFO - 03/14/25 06:07:15 - 3:08:49 - Epoch 892: train_loss=1.1148922443389893
INFO - 03/14/25 06:07:27 - 3:09:01 - Epoch 893: train_loss=1.1155871152877808
INFO - 03/14/25 06:07:40 - 3:09:14 - Epoch 894: train_loss=1.1148897409439087
INFO - 03/14/25 06:07:53 - 3:09:27 - Epoch 895: train_loss=1.1155866384506226
INFO - 03/14/25 06:08:05 - 3:09:40 - Epoch 896: train_loss=1.1148881912231445
INFO - 03/14/25 06:08:18 - 3:09:52 - Epoch 897: train_loss=1.1153790950775146
INFO - 03/14/25 06:08:31 - 3:10:05 - Epoch 898: train_loss=1.1148873567581177
INFO - 03/14/25 06:08:43 - 3:10:18 - Epoch 899: train_loss=1.115587830543518
INFO - 03/14/25 06:08:56 - 3:10:30 - Epoch 900: train_loss=1.114885926246643
INFO - 03/14/25 06:09:09 - 3:10:43 - Epoch 901: train_loss=1.1155834197998047
INFO - 03/14/25 06:09:22 - 3:10:56 - Epoch 902: train_loss=1.1148862838745117
INFO - 03/14/25 06:09:34 - 3:11:08 - Epoch 903: train_loss=1.1148874759674072
INFO - 03/14/25 06:09:47 - 3:11:21 - Epoch 904: train_loss=1.1155840158462524
INFO - 03/14/25 06:10:00 - 3:11:34 - Epoch 905: train_loss=1.1148855686187744
INFO - 03/14/25 06:10:12 - 3:11:47 - Epoch 906: train_loss=1.1155834197998047
INFO - 03/14/25 06:10:25 - 3:11:59 - Epoch 907: train_loss=1.1148855686187744
INFO - 03/14/25 06:10:38 - 3:12:12 - Epoch 908: train_loss=1.1153755187988281
INFO - 03/14/25 06:10:50 - 3:12:25 - Epoch 909: train_loss=1.114884614944458
INFO - 03/14/25 06:11:03 - 3:12:37 - Epoch 910: train_loss=1.115581750869751
INFO - 03/14/25 06:11:16 - 3:12:50 - Epoch 911: train_loss=1.1148830652236938
INFO - 03/14/25 06:11:28 - 3:13:03 - Epoch 912: train_loss=1.1155767440795898
INFO - 03/14/25 06:11:41 - 3:13:15 - Epoch 913: train_loss=1.1148818731307983
INFO - 03/14/25 06:11:54 - 3:13:28 - Epoch 914: train_loss=1.1148815155029297
INFO - 03/14/25 06:12:08 - 3:13:41 - Epoch 915: train_loss=1.1155747175216675
INFO - 03/14/25 06:12:22 - 3:13:56 - Epoch 916: train_loss=1.1148781776428223
INFO - 03/14/25 06:12:35 - 3:14:09 - Epoch 917: train_loss=1.115573525428772
INFO - 03/14/25 06:12:47 - 3:14:22 - Epoch 918: train_loss=1.1148765087127686
INFO - 03/14/25 06:13:00 - 3:14:34 - Epoch 919: train_loss=1.1153664588928223
INFO - 03/14/25 06:13:13 - 3:14:47 - Epoch 920: train_loss=1.1148744821548462
INFO - 03/14/25 06:13:25 - 3:15:00 - Epoch 921: train_loss=1.1155730485916138
INFO - 03/14/25 06:13:38 - 3:15:12 - Epoch 922: train_loss=1.1148719787597656
INFO - 03/14/25 06:13:51 - 3:15:25 - Epoch 923: train_loss=1.1155678033828735
INFO - 03/14/25 06:14:04 - 3:15:38 - Epoch 924: train_loss=1.114870309829712
INFO - 03/14/25 06:14:16 - 3:15:50 - Epoch 925: train_loss=1.1153600215911865
INFO - 03/14/25 06:14:29 - 3:16:03 - Epoch 926: train_loss=1.1148678064346313
INFO - 03/14/25 06:14:42 - 3:16:16 - Epoch 927: train_loss=1.1155625581741333
INFO - 03/14/25 06:14:54 - 3:16:29 - Epoch 928: train_loss=1.1148663759231567
INFO - 03/14/25 06:15:07 - 3:16:41 - Epoch 929: train_loss=1.1153546571731567
INFO - 03/14/25 06:15:20 - 3:16:54 - Epoch 930: train_loss=1.1148642301559448
INFO - 03/14/25 06:15:32 - 3:17:07 - Epoch 931: train_loss=1.1155604124069214
INFO - 03/14/25 06:15:45 - 3:17:19 - Epoch 932: train_loss=1.1148619651794434
INFO - 03/14/25 06:15:58 - 3:17:32 - Epoch 933: train_loss=1.1153521537780762
INFO - 03/14/25 06:16:10 - 3:17:45 - Epoch 934: train_loss=1.1148605346679688
INFO - 03/14/25 06:16:23 - 3:17:57 - Epoch 935: train_loss=1.1157147884368896
INFO - 03/14/25 06:16:36 - 3:18:10 - Epoch 936: train_loss=1.114858865737915
INFO - 03/14/25 06:16:49 - 3:18:23 - Epoch 937: train_loss=1.1148580312728882
INFO - 03/14/25 06:17:01 - 3:18:36 - Epoch 938: train_loss=1.1155544519424438
INFO - 03/14/25 06:17:14 - 3:18:48 - Epoch 939: train_loss=1.1148552894592285
INFO - 03/14/25 06:17:27 - 3:19:01 - Epoch 940: train_loss=1.1155532598495483
INFO - 03/14/25 06:17:39 - 3:19:14 - Epoch 941: train_loss=1.1148537397384644
INFO - 03/14/25 06:17:52 - 3:19:26 - Epoch 942: train_loss=1.115343689918518
INFO - 03/14/25 06:18:05 - 3:19:39 - Epoch 943: train_loss=1.1148518323898315
INFO - 03/14/25 06:18:17 - 3:19:52 - Epoch 944: train_loss=1.115546703338623
INFO - 03/14/25 06:18:30 - 3:20:04 - Epoch 945: train_loss=1.114850401878357
INFO - 03/14/25 06:18:43 - 3:20:17 - Epoch 946: train_loss=1.1153405904769897
INFO - 03/14/25 06:18:56 - 3:20:30 - Epoch 947: train_loss=1.11484956741333
INFO - 03/14/25 06:19:08 - 3:20:43 - Epoch 948: train_loss=1.1155506372451782
INFO - 03/14/25 06:19:21 - 3:20:55 - Epoch 949: train_loss=1.114847183227539
INFO - 03/14/25 06:19:34 - 3:21:08 - Epoch 950: train_loss=1.1155503988265991
INFO - 03/14/25 06:19:46 - 3:21:21 - Epoch 951: train_loss=1.1148470640182495
INFO - 03/14/25 06:19:59 - 3:21:33 - Epoch 952: train_loss=1.1155469417572021
INFO - 03/14/25 06:20:12 - 3:21:46 - Epoch 953: train_loss=1.1148478984832764
INFO - 03/14/25 06:20:25 - 3:21:59 - Epoch 954: train_loss=1.1153388023376465
INFO - 03/14/25 06:20:37 - 3:22:11 - Epoch 955: train_loss=1.1148483753204346
INFO - 03/14/25 06:20:50 - 3:22:24 - Epoch 956: train_loss=1.1155449151992798
INFO - 03/14/25 06:21:03 - 3:22:37 - Epoch 957: train_loss=1.114848017692566
INFO - 03/14/25 06:21:15 - 3:22:50 - Epoch 958: train_loss=1.1155449151992798
INFO - 03/14/25 06:21:28 - 3:23:02 - Epoch 959: train_loss=1.1148476600646973
INFO - 03/14/25 06:21:41 - 3:23:15 - Epoch 960: train_loss=1.115337610244751
INFO - 03/14/25 06:21:53 - 3:23:28 - Epoch 961: train_loss=1.1148463487625122
INFO - 03/14/25 06:22:06 - 3:23:40 - Epoch 962: train_loss=1.1155413389205933
INFO - 03/14/25 06:22:19 - 3:23:53 - Epoch 963: train_loss=1.1148449182510376
INFO - 03/14/25 06:22:32 - 3:24:06 - Epoch 964: train_loss=1.1153346300125122
INFO - 03/14/25 06:22:44 - 3:24:18 - Epoch 965: train_loss=1.1148433685302734
INFO - 03/14/25 06:22:57 - 3:24:31 - Epoch 966: train_loss=1.1155372858047485
INFO - 03/14/25 06:23:10 - 3:24:44 - Epoch 967: train_loss=1.1148412227630615
INFO - 03/14/25 06:23:22 - 3:24:57 - Epoch 968: train_loss=1.1153310537338257
INFO - 03/14/25 06:23:35 - 3:25:09 - Epoch 969: train_loss=1.1148396730422974
INFO - 03/14/25 06:23:48 - 3:25:22 - Epoch 970: train_loss=1.1155352592468262
INFO - 03/14/25 06:24:00 - 3:25:35 - Epoch 971: train_loss=1.1148371696472168
INFO - 03/14/25 06:24:13 - 3:25:47 - Epoch 972: train_loss=1.1155308485031128
INFO - 03/14/25 06:24:26 - 3:26:00 - Epoch 973: train_loss=1.1148356199264526
INFO - 03/14/25 06:24:39 - 3:26:13 - Epoch 974: train_loss=1.1148346662521362
INFO - 03/14/25 06:24:51 - 3:26:26 - Epoch 975: train_loss=1.115324854850769
INFO - 03/14/25 06:25:04 - 3:26:38 - Epoch 976: train_loss=1.115322232246399
INFO - 03/14/25 06:25:17 - 3:26:51 - Epoch 977: train_loss=1.1148321628570557
INFO - 03/14/25 06:25:29 - 3:27:04 - Epoch 978: train_loss=1.1148306131362915
INFO - 03/14/25 06:25:42 - 3:27:16 - Epoch 979: train_loss=1.114829421043396
INFO - 03/14/25 06:25:55 - 3:27:29 - Epoch 980: train_loss=1.1148285865783691
INFO - 03/14/25 06:26:08 - 3:27:42 - Epoch 981: train_loss=1.1148279905319214
INFO - 03/14/25 06:26:20 - 3:27:55 - Epoch 982: train_loss=1.114827275276184
INFO - 03/14/25 06:26:33 - 3:28:07 - Epoch 983: train_loss=1.1153148412704468
INFO - 03/14/25 06:26:46 - 3:28:20 - Epoch 984: train_loss=1.1153146028518677
INFO - 03/14/25 06:26:58 - 3:28:33 - Epoch 985: train_loss=1.1148231029510498
INFO - 03/14/25 06:27:11 - 3:28:45 - Epoch 986: train_loss=1.1148228645324707
INFO - 03/14/25 06:27:24 - 3:28:58 - Epoch 987: train_loss=1.1155154705047607
INFO - 03/14/25 06:27:36 - 3:29:11 - Epoch 988: train_loss=1.114819884300232
INFO - 03/14/25 06:27:49 - 3:29:23 - Epoch 989: train_loss=1.1155158281326294
INFO - 03/14/25 06:28:02 - 3:29:36 - Epoch 990: train_loss=1.1148182153701782
INFO - 03/14/25 06:28:15 - 3:29:49 - Epoch 991: train_loss=1.115512490272522
INFO - 03/14/25 06:28:27 - 3:30:01 - Epoch 992: train_loss=1.1148165464401245
INFO - 03/14/25 06:28:40 - 3:30:14 - Epoch 993: train_loss=1.1153075695037842
INFO - 03/14/25 06:28:53 - 3:30:27 - Epoch 994: train_loss=1.114815592765808
INFO - 03/14/25 06:29:05 - 3:30:40 - Epoch 995: train_loss=1.1155158281326294
INFO - 03/14/25 06:29:18 - 3:30:52 - Epoch 996: train_loss=1.1148145198822021
INFO - 03/14/25 06:29:31 - 3:31:05 - Epoch 997: train_loss=1.1156764030456543
INFO - 03/14/25 06:29:43 - 3:31:18 - Epoch 998: train_loss=1.1153051853179932
INFO - 03/14/25 06:29:56 - 3:31:30 - Epoch 999: train_loss=1.1156766414642334
INFO - 03/14/25 06:29:56 - 3:31:30 - --------------------------Training Start-------------------------
WARNING - 03/14/25 06:30:08 - 3:31:43 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:30:08 - 3:31:43 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 94, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 89, in loss
                                            weight_sum = torch.einsum('en, e->n', ass_i * ass_j, weight)  # (N_k, )
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.24 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 61.57 GiB is allocated by PyTorch, and 15.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:30:10 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:30:17 - 0:00:09 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:30:17 - 0:00:09 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 158, in forward
                                            adj = gumbel_sigmoid(adj, tau=self.temperature)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/utils/utils.py", line 70, in gumbel_sigmoid
                                            gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.89 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 63.76 GiB is allocated by PyTorch, and 13.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:30:19 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:30:26 - 0:00:09 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:30:26 - 0:00:09 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 158, in forward
                                            adj = gumbel_sigmoid(adj, tau=self.temperature)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/utils/utils.py", line 70, in gumbel_sigmoid
                                            gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.89 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 63.76 GiB is allocated by PyTorch, and 13.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:30:28 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:30:39 - 0:00:13 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:30:39 - 0:00:13 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 60.19 GiB is allocated by PyTorch, and 16.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:30:41 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:30:52 - 0:00:13 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:30:52 - 0:00:13 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 60.19 GiB is allocated by PyTorch, and 16.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:30:54 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:06 - 0:00:13 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:06 - 0:00:13 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 60.19 GiB is allocated by PyTorch, and 16.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:08 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:08 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:08 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 25, in forward
                                            h = self.agg(h, edge_index)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 101, in forward
                                            support_t = torch.matmul(adj, x)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.41 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 50.22 GiB is allocated by PyTorch, and 26.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:10 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:10 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:10 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 25, in forward
                                            h = self.agg(h, edge_index)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 101, in forward
                                            support_t = torch.matmul(adj, x)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.41 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 50.22 GiB is allocated by PyTorch, and 26.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:12 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:12 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:12 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 25, in forward
                                            h = self.agg(h, edge_index)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 101, in forward
                                            support_t = torch.matmul(adj, x)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.41 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 50.22 GiB is allocated by PyTorch, and 26.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:14 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:23 - 0:00:11 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:23 - 0:00:11 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.55 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 59.85 GiB is allocated by PyTorch, and 17.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:25 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:34 - 0:00:11 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:34 - 0:00:11 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.55 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 59.85 GiB is allocated by PyTorch, and 17.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:36 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:31:45 - 0:00:11 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:31:45 - 0:00:11 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.55 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 59.85 GiB is allocated by PyTorch, and 17.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:31:47 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:32:50 - 0:01:06 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:32:50 - 0:01:06 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 137, in forward
                                            logits = torch.log_softmax(ass, dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 67.52 GiB is allocated by PyTorch, and 9.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:32:55 - 0:00:05 - 
                                     train iters 0
WARNING - 03/14/25 06:33:59 - 0:01:09 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:33:59 - 0:01:09 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 137, in forward
                                            logits = torch.log_softmax(ass, dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 67.52 GiB is allocated by PyTorch, and 9.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:34:04 - 0:00:05 - 
                                     train iters 0
WARNING - 03/14/25 06:35:08 - 0:01:09 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:35:08 - 0:01:09 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 137, in forward
                                            logits = torch.log_softmax(ass, dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 67.52 GiB is allocated by PyTorch, and 9.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:35:13 - 0:00:05 - 
                                     train iters 0
WARNING - 03/14/25 06:35:22 - 0:00:14 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:35:22 - 0:00:14 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.15 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 63.46 GiB is allocated by PyTorch, and 13.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:35:24 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:35:33 - 0:00:11 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:35:33 - 0:00:11 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.15 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 63.46 GiB is allocated by PyTorch, and 13.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:35:35 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:35:44 - 0:00:11 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:35:44 - 0:00:11 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.15 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 63.46 GiB is allocated by PyTorch, and 13.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:35:46 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:35:47 - 0:00:03 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:35:47 - 0:00:03 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.47 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 67.58 GiB is allocated by PyTorch, and 9.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:35:53 - 0:00:06 - 
                                     train iters 0
WARNING - 03/14/25 06:35:53 - 0:00:07 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:35:53 - 0:00:07 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.47 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 67.58 GiB is allocated by PyTorch, and 9.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:35:59 - 0:00:06 - 
                                     train iters 0
WARNING - 03/14/25 06:36:00 - 0:00:07 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:36:00 - 0:00:07 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.47 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 67.58 GiB is allocated by PyTorch, and 9.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:36:06 - 0:00:06 - 
                                     train iters 0
WARNING - 03/14/25 06:36:16 - 0:00:16 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:36:16 - 0:00:16 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.99 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 64.89 GiB is allocated by PyTorch, and 12.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:36:18 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:36:28 - 0:00:12 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:36:28 - 0:00:12 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.99 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 64.89 GiB is allocated by PyTorch, and 12.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:36:30 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:36:40 - 0:00:12 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:36:40 - 0:00:12 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 157, in forward
                                            adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.99 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 64.89 GiB is allocated by PyTorch, and 12.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:36:42 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:36:59 - 0:00:19 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:36:59 - 0:00:19 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 24, in forward
                                            h = self.linear(x)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 59, in forward
                                            x = torch.cat([time, x_narrow * scale.sqrt()], dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.80 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 42.52 GiB is allocated by PyTorch, and 34.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:37:01 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:37:18 - 0:00:19 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:37:18 - 0:00:19 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 24, in forward
                                            h = self.linear(x)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 59, in forward
                                            x = torch.cat([time, x_narrow * scale.sqrt()], dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.80 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 42.52 GiB is allocated by PyTorch, and 34.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:37:20 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:37:37 - 0:00:19 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:37:37 - 0:00:19 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 151, in forward
                                            ass = self.assignor(x, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 129, in forward
                                            ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 24, in forward
                                            h = self.linear(x)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 59, in forward
                                            x = torch.cat([time, x_narrow * scale.sqrt()], dim=-1)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.80 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 42.52 GiB is allocated by PyTorch, and 34.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:37:39 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:37:39 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:37:39 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 59.78 GiB is allocated by PyTorch, and 17.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:37:43 - 0:00:04 - 
                                     train iters 0
WARNING - 03/14/25 06:37:43 - 0:00:05 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:37:43 - 0:00:05 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 59.78 GiB is allocated by PyTorch, and 17.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:37:48 - 0:00:04 - 
                                     train iters 0
WARNING - 03/14/25 06:37:48 - 0:00:05 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:37:48 - 0:00:05 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 156, in forward
                                            adj = ass.exp().t() @ adj @ ass.exp()
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 59.78 GiB is allocated by PyTorch, and 17.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:37:52 - 0:00:04 - 
                                     train iters 0
WARNING - 03/14/25 06:37:59 - 0:00:11 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:37:59 - 0:00:11 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 158, in forward
                                            adj = gumbel_sigmoid(adj, tau=self.temperature)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/utils/utils.py", line 70, in gumbel_sigmoid
                                            gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.02 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 58.74 GiB is allocated by PyTorch, and 18.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:38:01 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:38:07 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:38:07 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 158, in forward
                                            adj = gumbel_sigmoid(adj, tau=self.temperature)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/utils/utils.py", line 70, in gumbel_sigmoid
                                            gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.02 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 58.74 GiB is allocated by PyTorch, and 18.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:38:09 - 0:00:02 - 
                                     train iters 0
WARNING - 03/14/25 06:38:15 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/14/25 06:38:15 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 84, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device, pretrain=True)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 65, in loss
                                            embeddings, clu_mat = self.encoder(features, adj)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/l_se_net.py", line 55, in forward
                                            z, edge, ass = layer(z, edge)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
                                            return self._call_impl(*args, **kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
                                            return forward_call(*args, **kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/layers.py", line 158, in forward
                                            adj = gumbel_sigmoid(adj, tau=self.temperature)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/utils/utils.py", line 70, in gumbel_sigmoid
                                            gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.02 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 58.74 GiB is allocated by PyTorch, and 18.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/14/25 06:38:17 - 0:00:02 - 
                                     train iters 0
/var/tmp/slurmd_spool/job2454248/slurm_script: line 14: 1776295 Killed                  python3 ./main_with_smac.py --dataset=SeNet --data_path=./datasets/affinity_matrix_from_senet_sparse_20000.npz --label_path=./datasets/senet_label_20000.csv >> output_main_lsenet_hpo.txt
slurmstepd: error: Detected 1 oom_kill event in StepId=2454248.batch. Some of the step tasks have been OOM Killed.
=== JOB_STATISTICS ===
=== current date     : Fri Mar 14 07:09:58 CET 2025
= Job-ID             : 2454248 on alex
= Job-Name           : lsenet_hpo
= Job-Command        : /home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/fau_alex_job_script_hpo.sh
= Initial workdir    : /home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering
= Queue/Partition    : a100
= Slurm account      : v100dd with QOS=normal
= Features           : a100_80
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 04:15:37
= Total RAM usage    : 215.6 GiB of assigned  GiB (%)
= Node list          : a0934
= Subm/Elig/Start/End: 2025-03-14T02:54:08 / 2025-03-14T02:54:08 / 2025-03-14T02:54:21 / 2025-03-14T07:09:58
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           30.1G   104.9G   209.7G        N/A      51K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-80GB, 00000000:13:00.0, 1776295, 81 %, 16 %, 80798 MiB, 15330791 ms
