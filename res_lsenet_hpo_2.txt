### Starting TaskPrologue of job 2448128 on a0931 at Tue Mar 11 09:33:27 CET 2025
Running on cores 0-15 with governor ondemand
Tue Mar 11 09:33:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0E:00.0 Off |                    0 |
| N/A   38C    P0             68W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

INFO - 03/11/25 09:33:44 - 0:00:00 - {'dataset': 'SeNet', 'task': 'Clustering', 'root_path': './datasets', 'eval_freq': 100, 'exp_iters': 5, 'version': 'run', 'log_path': './results/run/SeNet.log', 'pre_epochs': 200, 'epochs': 1500, 'height': 5, 'lr_pre': 0.01, 'lr': 0.01, 'w_decay': 0.0, 'decay_rate': 9, 'max_nums': None, 'embed_dim': 32, 'hidden_dim_enc': 64, 'hidden_dim': 64, 'dropout': 0.0, 'nonlin': None, 'temperature': 0.2, 'n_cluster_trials': 5, 't': 1.0, 'r': 2.0, 'patience': 5, 'save_path': 'model.pt', 'use_gpu': True, 'gpu': 0, 'devices': '0,1', 'data_path': './datasets/affinity_matrix_from_senet_sparse_20000.npz', 'label_path': './datasets/senet_label_20000.csv'}
INFO - 03/11/25 09:33:46 - 0:00:02 - 
                                     train iters 0
INFO - 03/11/25 09:34:01 - 0:00:17 - Epoch 1: train_loss=6.013265132904053
INFO - 03/11/25 09:34:16 - 0:00:31 - Epoch 2: train_loss=1.3114103078842163
INFO - 03/11/25 09:34:30 - 0:00:46 - Epoch 3: train_loss=1.1434791088104248
INFO - 03/11/25 09:34:45 - 0:01:01 - Epoch 4: train_loss=1.1373153924942017
INFO - 03/11/25 09:34:59 - 0:01:15 - Epoch 5: train_loss=1.1401708126068115
INFO - 03/11/25 09:35:14 - 0:01:29 - Epoch 6: train_loss=1.1331027746200562
INFO - 03/11/25 09:35:28 - 0:01:44 - Epoch 7: train_loss=1.1284171342849731
INFO - 03/11/25 09:35:42 - 0:01:58 - Epoch 8: train_loss=1.1310831308364868
INFO - 03/11/25 09:35:57 - 0:02:13 - Epoch 9: train_loss=1.1329115629196167
INFO - 03/11/25 09:36:11 - 0:02:27 - Epoch 10: train_loss=1.1287683248519897
INFO - 03/11/25 09:36:25 - 0:02:41 - Epoch 11: train_loss=1.124741554260254
INFO - 03/11/25 09:36:40 - 0:02:56 - Epoch 12: train_loss=1.1268088817596436
INFO - 03/11/25 09:36:54 - 0:03:10 - Epoch 13: train_loss=1.1263504028320312
INFO - 03/11/25 09:37:09 - 0:03:25 - Epoch 14: train_loss=1.12267005443573
INFO - 03/11/25 09:37:23 - 0:03:39 - Epoch 15: train_loss=1.1235361099243164
INFO - 03/11/25 09:37:37 - 0:03:53 - Epoch 16: train_loss=1.1235450506210327
INFO - 03/11/25 09:37:52 - 0:04:08 - Epoch 17: train_loss=1.121907114982605
INFO - 03/11/25 09:38:06 - 0:04:22 - Epoch 18: train_loss=1.1217098236083984
INFO - 03/11/25 09:38:21 - 0:04:37 - Epoch 19: train_loss=1.1253328323364258
INFO - 03/11/25 09:38:35 - 0:04:51 - Epoch 20: train_loss=1.1238442659378052
INFO - 03/11/25 09:38:49 - 0:05:05 - Epoch 21: train_loss=1.1227728128433228
INFO - 03/11/25 09:39:04 - 0:05:20 - Epoch 22: train_loss=1.120365858078003
INFO - 03/11/25 09:39:18 - 0:05:34 - Epoch 23: train_loss=1.1215249300003052
INFO - 03/11/25 09:39:32 - 0:05:48 - Epoch 24: train_loss=1.1225606203079224
INFO - 03/11/25 09:39:47 - 0:06:03 - Epoch 25: train_loss=1.1217705011367798
INFO - 03/11/25 09:40:01 - 0:06:17 - Epoch 26: train_loss=1.119802713394165
INFO - 03/11/25 09:40:16 - 0:06:32 - Epoch 27: train_loss=1.119356632232666
INFO - 03/11/25 09:40:30 - 0:06:46 - Epoch 28: train_loss=1.1194909811019897
INFO - 03/11/25 09:40:44 - 0:07:00 - Epoch 29: train_loss=1.1199548244476318
INFO - 03/11/25 09:40:59 - 0:07:15 - Epoch 30: train_loss=1.119736671447754
INFO - 03/11/25 09:41:13 - 0:07:29 - Epoch 31: train_loss=1.1185661554336548
INFO - 03/11/25 09:41:28 - 0:07:44 - Epoch 32: train_loss=1.1186045408248901
INFO - 03/11/25 09:41:42 - 0:07:58 - Epoch 33: train_loss=1.1189864873886108
INFO - 03/11/25 09:41:56 - 0:08:12 - Epoch 34: train_loss=1.1183586120605469
INFO - 03/11/25 09:42:11 - 0:08:27 - Epoch 35: train_loss=1.1179640293121338
INFO - 03/11/25 09:42:25 - 0:08:41 - Epoch 36: train_loss=1.1181365251541138
INFO - 03/11/25 09:42:39 - 0:08:55 - Epoch 37: train_loss=1.118140697479248
INFO - 03/11/25 09:42:54 - 0:09:10 - Epoch 38: train_loss=1.117668628692627
INFO - 03/11/25 09:43:08 - 0:09:24 - Epoch 39: train_loss=1.1178817749023438
INFO - 03/11/25 09:43:23 - 0:09:39 - Epoch 40: train_loss=1.1187258958816528
INFO - 03/11/25 09:43:37 - 0:09:53 - Epoch 41: train_loss=1.1181282997131348
INFO - 03/11/25 09:43:51 - 0:10:07 - Epoch 42: train_loss=1.118391752243042
INFO - 03/11/25 09:44:06 - 0:10:22 - Epoch 43: train_loss=1.1175247430801392
INFO - 03/11/25 09:44:20 - 0:10:36 - Epoch 44: train_loss=1.1181342601776123
INFO - 03/11/25 09:44:35 - 0:10:51 - Epoch 45: train_loss=1.1176772117614746
INFO - 03/11/25 09:44:49 - 0:11:05 - Epoch 46: train_loss=1.117964744567871
INFO - 03/11/25 09:45:03 - 0:11:19 - Epoch 47: train_loss=1.1176581382751465
INFO - 03/11/25 09:45:18 - 0:11:34 - Epoch 48: train_loss=1.1178703308105469
INFO - 03/11/25 09:45:32 - 0:11:48 - Epoch 49: train_loss=1.1177647113800049
INFO - 03/11/25 09:45:46 - 0:12:02 - Epoch 50: train_loss=1.1175211668014526
INFO - 03/11/25 09:46:01 - 0:12:17 - Epoch 51: train_loss=1.1175192594528198
INFO - 03/11/25 09:46:15 - 0:12:31 - Epoch 52: train_loss=1.1173768043518066
INFO - 03/11/25 09:46:30 - 0:12:46 - Epoch 53: train_loss=1.1173568964004517
INFO - 03/11/25 09:46:44 - 0:13:00 - Epoch 54: train_loss=1.1173444986343384
INFO - 03/11/25 09:46:58 - 0:13:14 - Epoch 55: train_loss=1.1171703338623047
INFO - 03/11/25 09:47:13 - 0:13:29 - Epoch 56: train_loss=1.117163896560669
INFO - 03/11/25 09:47:27 - 0:13:43 - Epoch 57: train_loss=1.1173115968704224
INFO - 03/11/25 09:47:42 - 0:13:57 - Epoch 58: train_loss=1.1172882318496704
INFO - 03/11/25 09:47:56 - 0:14:12 - Epoch 59: train_loss=1.1171247959136963
INFO - 03/11/25 09:48:10 - 0:14:26 - Epoch 60: train_loss=1.1171060800552368
INFO - 03/11/25 09:48:25 - 0:14:41 - Epoch 61: train_loss=1.1171029806137085
INFO - 03/11/25 09:48:39 - 0:14:55 - Epoch 62: train_loss=1.1168880462646484
INFO - 03/11/25 09:48:53 - 0:15:09 - Epoch 63: train_loss=1.1172382831573486
INFO - 03/11/25 09:49:08 - 0:15:24 - Epoch 64: train_loss=1.1170567274093628
INFO - 03/11/25 09:49:22 - 0:15:38 - Epoch 65: train_loss=1.1174870729446411
INFO - 03/11/25 09:49:37 - 0:15:53 - Epoch 66: train_loss=1.117068886756897
INFO - 03/11/25 09:49:51 - 0:16:07 - Epoch 67: train_loss=1.1175718307495117
INFO - 03/11/25 09:50:05 - 0:16:21 - Epoch 68: train_loss=1.1173386573791504
INFO - 03/11/25 09:50:20 - 0:16:36 - Epoch 69: train_loss=1.1172046661376953
INFO - 03/11/25 09:50:34 - 0:16:50 - Epoch 70: train_loss=1.117334246635437
INFO - 03/11/25 09:50:48 - 0:17:04 - Epoch 71: train_loss=1.1171839237213135
INFO - 03/11/25 09:51:03 - 0:17:19 - Epoch 72: train_loss=1.1170107126235962
INFO - 03/11/25 09:51:17 - 0:17:33 - Epoch 73: train_loss=1.117419958114624
INFO - 03/11/25 09:51:32 - 0:17:48 - Epoch 74: train_loss=1.1171691417694092
INFO - 03/11/25 09:51:46 - 0:18:02 - Epoch 75: train_loss=1.1171627044677734
INFO - 03/11/25 09:52:00 - 0:18:16 - Epoch 76: train_loss=1.1169898509979248
INFO - 03/11/25 09:52:15 - 0:18:31 - Epoch 77: train_loss=1.117408037185669
INFO - 03/11/25 09:52:29 - 0:18:45 - Epoch 78: train_loss=1.1172764301300049
INFO - 03/11/25 09:52:43 - 0:18:59 - Epoch 79: train_loss=1.1171298027038574
INFO - 03/11/25 09:52:58 - 0:19:14 - Epoch 80: train_loss=1.117119550704956
INFO - 03/11/25 09:53:12 - 0:19:28 - Epoch 81: train_loss=1.1169543266296387
INFO - 03/11/25 09:53:27 - 0:19:43 - Epoch 82: train_loss=1.1169384717941284
INFO - 03/11/25 09:53:41 - 0:19:57 - Epoch 83: train_loss=1.1172417402267456
INFO - 03/11/25 09:53:55 - 0:20:11 - Epoch 84: train_loss=1.1169260740280151
INFO - 03/11/25 09:54:10 - 0:20:26 - Epoch 85: train_loss=1.117448329925537
INFO - 03/11/25 09:54:24 - 0:20:40 - Epoch 86: train_loss=1.1170909404754639
INFO - 03/11/25 09:54:39 - 0:20:54 - Epoch 87: train_loss=1.1172317266464233
INFO - 03/11/25 09:54:53 - 0:21:09 - Epoch 88: train_loss=1.1172292232513428
INFO - 03/11/25 09:55:07 - 0:21:23 - Epoch 89: train_loss=1.1170854568481445
INFO - 03/11/25 09:55:22 - 0:21:38 - Epoch 90: train_loss=1.1170706748962402
INFO - 03/11/25 09:55:36 - 0:21:52 - Epoch 91: train_loss=1.1170536279678345
INFO - 03/11/25 09:55:50 - 0:22:06 - Epoch 92: train_loss=1.1168973445892334
INFO - 03/11/25 09:56:05 - 0:22:21 - Epoch 93: train_loss=1.1170634031295776
INFO - 03/11/25 09:56:19 - 0:22:35 - Epoch 94: train_loss=1.1166791915893555
INFO - 03/11/25 09:56:34 - 0:22:49 - Epoch 95: train_loss=1.117299199104309
INFO - 03/11/25 09:56:48 - 0:23:04 - Epoch 96: train_loss=1.1168808937072754
INFO - 03/11/25 09:57:02 - 0:23:18 - Epoch 97: train_loss=1.1173065900802612
INFO - 03/11/25 09:57:17 - 0:23:33 - Epoch 98: train_loss=1.1170352697372437
INFO - 03/11/25 09:57:31 - 0:23:47 - Epoch 99: train_loss=1.117154598236084
INFO - 03/11/25 09:57:45 - 0:24:01 - Epoch 100: train_loss=1.117018222808838
INFO - 03/11/25 09:58:00 - 0:24:16 - Epoch 101: train_loss=1.1170239448547363
INFO - 03/11/25 09:58:14 - 0:24:30 - Epoch 102: train_loss=1.117006778717041
INFO - 03/11/25 09:58:29 - 0:24:45 - Epoch 103: train_loss=1.116990089416504
INFO - 03/11/25 09:58:43 - 0:24:59 - Epoch 104: train_loss=1.1169894933700562
INFO - 03/11/25 09:58:57 - 0:25:13 - Epoch 105: train_loss=1.1169947385787964
INFO - 03/11/25 09:59:12 - 0:25:28 - Epoch 106: train_loss=1.1169795989990234
INFO - 03/11/25 09:59:26 - 0:25:42 - Epoch 107: train_loss=1.116987943649292
INFO - 03/11/25 09:59:40 - 0:25:56 - Epoch 108: train_loss=1.116991639137268
INFO - 03/11/25 09:59:55 - 0:26:11 - Epoch 109: train_loss=1.116835117340088
INFO - 03/11/25 10:00:09 - 0:26:25 - Epoch 110: train_loss=1.1168133020401
INFO - 03/11/25 10:00:24 - 0:26:40 - Epoch 111: train_loss=1.1168022155761719
INFO - 03/11/25 10:00:38 - 0:26:54 - Epoch 112: train_loss=1.1168022155761719
INFO - 03/11/25 10:00:52 - 0:27:08 - Epoch 113: train_loss=1.1167998313903809
INFO - 03/11/25 10:01:07 - 0:27:23 - Epoch 114: train_loss=1.1167891025543213
INFO - 03/11/25 10:01:21 - 0:27:37 - Epoch 115: train_loss=1.116781234741211
INFO - 03/11/25 10:01:35 - 0:27:51 - Epoch 116: train_loss=1.1167867183685303
INFO - 03/11/25 10:01:50 - 0:28:06 - Epoch 117: train_loss=1.1165670156478882
INFO - 03/11/25 10:02:04 - 0:28:20 - Epoch 118: train_loss=1.1167733669281006
INFO - 03/11/25 10:02:19 - 0:28:35 - Epoch 119: train_loss=1.1165544986724854
INFO - 03/11/25 10:02:33 - 0:28:49 - Epoch 120: train_loss=1.1169137954711914
INFO - 03/11/25 10:02:47 - 0:29:03 - Epoch 121: train_loss=1.1165313720703125
INFO - 03/11/25 10:03:02 - 0:29:18 - Epoch 122: train_loss=1.116739273071289
INFO - 03/11/25 10:03:16 - 0:29:32 - Epoch 123: train_loss=1.116519808769226
INFO - 03/11/25 10:03:30 - 0:29:46 - Epoch 124: train_loss=1.1168887615203857
INFO - 03/11/25 10:03:45 - 0:30:01 - Epoch 125: train_loss=1.1167138814926147
INFO - 03/11/25 10:03:59 - 0:30:15 - Epoch 126: train_loss=1.1167062520980835
INFO - 03/11/25 10:04:14 - 0:30:30 - Epoch 127: train_loss=1.11670982837677
INFO - 03/11/25 10:04:28 - 0:30:44 - Epoch 128: train_loss=1.116485595703125
INFO - 03/11/25 10:04:42 - 0:30:58 - Epoch 129: train_loss=1.1172313690185547
INFO - 03/11/25 10:04:57 - 0:31:13 - Epoch 130: train_loss=1.1166836023330688
INFO - 03/11/25 10:05:11 - 0:31:27 - Epoch 131: train_loss=1.1175123453140259
INFO - 03/11/25 10:05:25 - 0:31:41 - Epoch 132: train_loss=1.1172269582748413
INFO - 03/11/25 10:05:40 - 0:31:56 - Epoch 133: train_loss=1.1172196865081787
INFO - 03/11/25 10:05:54 - 0:32:10 - Epoch 134: train_loss=1.1169759035110474
INFO - 03/11/25 10:06:09 - 0:32:25 - Epoch 135: train_loss=1.117209792137146
INFO - 03/11/25 10:06:23 - 0:32:39 - Epoch 136: train_loss=1.11683988571167
INFO - 03/11/25 10:06:37 - 0:32:53 - Epoch 137: train_loss=1.1171019077301025
INFO - 03/11/25 10:06:52 - 0:33:08 - Epoch 138: train_loss=1.1168220043182373
INFO - 03/11/25 10:07:06 - 0:33:22 - Epoch 139: train_loss=1.1169664859771729
INFO - 03/11/25 10:07:20 - 0:33:36 - Epoch 140: train_loss=1.117453932762146
INFO - 03/11/25 10:07:35 - 0:33:51 - Epoch 141: train_loss=1.117347240447998
INFO - 03/11/25 10:07:49 - 0:34:05 - Epoch 142: train_loss=1.116686224937439
INFO - 03/11/25 10:08:04 - 0:34:20 - Epoch 143: train_loss=1.1168701648712158
INFO - 03/11/25 10:08:18 - 0:34:34 - Epoch 144: train_loss=1.1167198419570923
INFO - 03/11/25 10:08:32 - 0:34:48 - Epoch 145: train_loss=1.1172337532043457
INFO - 03/11/25 10:08:47 - 0:35:03 - Epoch 146: train_loss=1.1171997785568237
INFO - 03/11/25 10:09:01 - 0:35:17 - Epoch 147: train_loss=1.116607904434204
INFO - 03/11/25 10:09:15 - 0:35:31 - Epoch 148: train_loss=1.1172665357589722
INFO - 03/11/25 10:09:30 - 0:35:46 - Epoch 149: train_loss=1.1171083450317383
INFO - 03/11/25 10:09:44 - 0:36:00 - Epoch 150: train_loss=1.1169241666793823
INFO - 03/11/25 10:09:59 - 0:36:15 - Epoch 151: train_loss=1.1169096231460571
INFO - 03/11/25 10:10:13 - 0:36:29 - Epoch 152: train_loss=1.1169013977050781
INFO - 03/11/25 10:10:27 - 0:36:43 - Epoch 153: train_loss=1.1167502403259277
INFO - 03/11/25 10:10:42 - 0:36:58 - Epoch 154: train_loss=1.1169190406799316
INFO - 03/11/25 10:10:56 - 0:37:12 - Epoch 155: train_loss=1.1169027090072632
INFO - 03/11/25 10:11:10 - 0:37:26 - Epoch 156: train_loss=1.116729974746704
INFO - 03/11/25 10:11:25 - 0:37:41 - Epoch 157: train_loss=1.116728663444519
INFO - 03/11/25 10:11:39 - 0:37:55 - Epoch 158: train_loss=1.1167367696762085
INFO - 03/11/25 10:11:54 - 0:38:09 - Epoch 159: train_loss=1.1165187358856201
INFO - 03/11/25 10:12:08 - 0:38:24 - Epoch 160: train_loss=1.116922378540039
INFO - 03/11/25 10:12:22 - 0:38:38 - Epoch 161: train_loss=1.1167384386062622
INFO - 03/11/25 10:12:37 - 0:38:53 - Epoch 162: train_loss=1.116858959197998
INFO - 03/11/25 10:12:51 - 0:39:07 - Epoch 163: train_loss=1.1166844367980957
INFO - 03/11/25 10:13:05 - 0:39:21 - Epoch 164: train_loss=1.116860270500183
INFO - 03/11/25 10:13:20 - 0:39:36 - Epoch 165: train_loss=1.1166847944259644
INFO - 03/11/25 10:13:34 - 0:39:50 - Epoch 166: train_loss=1.1166832447052002
INFO - 03/11/25 10:13:48 - 0:40:04 - Epoch 167: train_loss=1.1164588928222656
INFO - 03/11/25 10:14:03 - 0:40:19 - Epoch 168: train_loss=1.1166644096374512
INFO - 03/11/25 10:14:17 - 0:40:33 - Epoch 169: train_loss=1.1164453029632568
INFO - 03/11/25 10:14:32 - 0:40:48 - Epoch 170: train_loss=1.1168172359466553
INFO - 03/11/25 10:14:46 - 0:41:02 - Epoch 171: train_loss=1.116652250289917
INFO - 03/11/25 10:15:00 - 0:41:16 - Epoch 172: train_loss=1.1166425943374634
INFO - 03/11/25 10:15:15 - 0:41:31 - Epoch 173: train_loss=1.116423487663269
INFO - 03/11/25 10:15:29 - 0:41:45 - Epoch 174: train_loss=1.1166224479675293
INFO - 03/11/25 10:15:43 - 0:41:59 - Epoch 175: train_loss=1.1164042949676514
INFO - 03/11/25 10:15:58 - 0:42:14 - Epoch 176: train_loss=1.1169145107269287
INFO - 03/11/25 10:16:12 - 0:42:28 - Epoch 177: train_loss=1.1166120767593384
INFO - 03/11/25 10:16:26 - 0:42:42 - Epoch 178: train_loss=1.116599440574646
INFO - 03/11/25 10:16:41 - 0:42:57 - Epoch 179: train_loss=1.1163864135742188
INFO - 03/11/25 10:16:55 - 0:43:11 - Epoch 180: train_loss=1.1167758703231812
INFO - 03/11/25 10:17:10 - 0:43:26 - Epoch 181: train_loss=1.116599440574646
INFO - 03/11/25 10:17:24 - 0:43:40 - Epoch 182: train_loss=1.1165881156921387
INFO - 03/11/25 10:17:38 - 0:43:54 - Epoch 183: train_loss=1.1163817644119263
INFO - 03/11/25 10:17:53 - 0:44:09 - Epoch 184: train_loss=1.116895079612732
INFO - 03/11/25 10:18:07 - 0:44:23 - Epoch 185: train_loss=1.11658775806427
INFO - 03/11/25 10:18:21 - 0:44:37 - Epoch 186: train_loss=1.1163760423660278
INFO - 03/11/25 10:18:36 - 0:44:52 - Epoch 187: train_loss=1.1168593168258667
INFO - 03/11/25 10:18:50 - 0:45:06 - Epoch 188: train_loss=1.1165746450424194
INFO - 03/11/25 10:19:05 - 0:45:21 - Epoch 189: train_loss=1.1163676977157593
INFO - 03/11/25 10:19:19 - 0:45:35 - Epoch 190: train_loss=1.1167380809783936
INFO - 03/11/25 10:19:33 - 0:45:49 - Epoch 191: train_loss=1.116557002067566
INFO - 03/11/25 10:19:48 - 0:46:04 - Epoch 192: train_loss=1.1167254447937012
INFO - 03/11/25 10:20:02 - 0:46:18 - Epoch 193: train_loss=1.1163499355316162
INFO - 03/11/25 10:20:16 - 0:46:32 - Epoch 194: train_loss=1.1163445711135864
INFO - 03/11/25 10:20:31 - 0:46:47 - Epoch 195: train_loss=1.1165542602539062
INFO - 03/11/25 10:20:45 - 0:47:01 - Epoch 196: train_loss=1.1165428161621094
INFO - 03/11/25 10:20:59 - 0:47:15 - Epoch 197: train_loss=1.116544485092163
INFO - 03/11/25 10:21:14 - 0:47:30 - Epoch 198: train_loss=1.1163272857666016
INFO - 03/11/25 10:21:28 - 0:47:44 - Epoch 199: train_loss=1.116701364517212
INFO - 03/11/25 10:21:28 - 0:47:44 - --------------------------Training Start-------------------------
WARNING - 03/11/25 10:21:36 - 0:47:52 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 10:21:36 - 0:47:52 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 94, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 88, in loss
                                            ass_j = ass_mat[k][edge_index[1]]
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.69 GiB. GPU 0 has a total capacity of 79.25 GiB of which 4.89 GiB is free. Including non-PyTorch memory, this process has 74.35 GiB memory in use. Of the allocated memory 73.25 GiB is allocated by PyTorch, and 615.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/11/25 10:21:36 - 0:47:52 - Added config 072866 as new incumbent because there are no incumbents yet.
INFO - 03/11/25 10:21:38 - 0:00:02 - 
                                     train iters 0
INFO - 03/11/25 10:21:52 - 0:00:16 - Epoch 1: train_loss=6.019809722900391
INFO - 03/11/25 10:22:06 - 0:00:31 - Epoch 2: train_loss=1.3100296258926392
INFO - 03/11/25 10:22:21 - 0:00:45 - Epoch 3: train_loss=1.1417672634124756
INFO - 03/11/25 10:22:35 - 0:00:59 - Epoch 4: train_loss=1.1382273435592651
INFO - 03/11/25 10:22:50 - 0:01:14 - Epoch 5: train_loss=1.1350669860839844
INFO - 03/11/25 10:23:04 - 0:01:28 - Epoch 6: train_loss=1.1318097114562988
INFO - 03/11/25 10:23:18 - 0:01:43 - Epoch 7: train_loss=1.1323622465133667
INFO - 03/11/25 10:23:33 - 0:01:57 - Epoch 8: train_loss=1.1299811601638794
INFO - 03/11/25 10:23:47 - 0:02:11 - Epoch 9: train_loss=1.1273751258850098
INFO - 03/11/25 10:24:02 - 0:02:26 - Epoch 10: train_loss=1.1258817911148071
INFO - 03/11/25 10:24:16 - 0:02:40 - Epoch 11: train_loss=1.1243010759353638
INFO - 03/11/25 10:24:30 - 0:02:55 - Epoch 12: train_loss=1.126457691192627
INFO - 03/11/25 10:24:45 - 0:03:09 - Epoch 13: train_loss=1.1301558017730713
INFO - 03/11/25 10:24:59 - 0:03:23 - Epoch 14: train_loss=1.131255865097046
INFO - 03/11/25 10:25:13 - 0:03:38 - Epoch 15: train_loss=1.1292535066604614
INFO - 03/11/25 10:25:28 - 0:03:52 - Epoch 16: train_loss=1.1292518377304077
INFO - 03/11/25 10:25:42 - 0:04:06 - Epoch 17: train_loss=1.1276713609695435
INFO - 03/11/25 10:25:57 - 0:04:21 - Epoch 18: train_loss=1.1262727975845337
INFO - 03/11/25 10:26:11 - 0:04:35 - Epoch 19: train_loss=1.1227329969406128
INFO - 03/11/25 10:26:25 - 0:04:49 - Epoch 20: train_loss=1.1234211921691895
INFO - 03/11/25 10:26:40 - 0:05:04 - Epoch 21: train_loss=1.1216291189193726
INFO - 03/11/25 10:26:54 - 0:05:18 - Epoch 22: train_loss=1.1248453855514526
INFO - 03/11/25 10:27:08 - 0:05:33 - Epoch 23: train_loss=1.121183156967163
INFO - 03/11/25 10:27:23 - 0:05:47 - Epoch 24: train_loss=1.1211193799972534
INFO - 03/11/25 10:27:37 - 0:06:01 - Epoch 25: train_loss=1.1196171045303345
INFO - 03/11/25 10:27:52 - 0:06:16 - Epoch 26: train_loss=1.120172381401062
INFO - 03/11/25 10:28:06 - 0:06:30 - Epoch 27: train_loss=1.1195882558822632
INFO - 03/11/25 10:28:20 - 0:06:44 - Epoch 28: train_loss=1.1192705631256104
INFO - 03/11/25 10:28:35 - 0:06:59 - Epoch 29: train_loss=1.1192234754562378
INFO - 03/11/25 10:28:49 - 0:07:13 - Epoch 30: train_loss=1.118638038635254
INFO - 03/11/25 10:29:03 - 0:07:28 - Epoch 31: train_loss=1.1182500123977661
INFO - 03/11/25 10:29:18 - 0:07:42 - Epoch 32: train_loss=1.1182851791381836
INFO - 03/11/25 10:29:32 - 0:07:56 - Epoch 33: train_loss=1.1180400848388672
INFO - 03/11/25 10:29:47 - 0:08:11 - Epoch 34: train_loss=1.1178617477416992
INFO - 03/11/25 10:30:01 - 0:08:25 - Epoch 35: train_loss=1.1176851987838745
INFO - 03/11/25 10:30:15 - 0:08:40 - Epoch 36: train_loss=1.117509365081787
INFO - 03/11/25 10:30:30 - 0:08:54 - Epoch 37: train_loss=1.117342472076416
INFO - 03/11/25 10:30:44 - 0:09:08 - Epoch 38: train_loss=1.1179853677749634
INFO - 03/11/25 10:30:59 - 0:09:23 - Epoch 39: train_loss=1.1177046298980713
INFO - 03/11/25 10:31:13 - 0:09:37 - Epoch 40: train_loss=1.1172056198120117
INFO - 03/11/25 10:31:27 - 0:09:51 - Epoch 41: train_loss=1.1175429821014404
INFO - 03/11/25 10:31:42 - 0:10:06 - Epoch 42: train_loss=1.116856575012207
INFO - 03/11/25 10:31:56 - 0:10:20 - Epoch 43: train_loss=1.1169124841690063
INFO - 03/11/25 10:32:10 - 0:10:35 - Epoch 44: train_loss=1.1171218156814575
INFO - 03/11/25 10:32:25 - 0:10:49 - Epoch 45: train_loss=1.1165916919708252
INFO - 03/11/25 10:32:39 - 0:11:03 - Epoch 46: train_loss=1.117028832435608
INFO - 03/11/25 10:32:54 - 0:11:18 - Epoch 47: train_loss=1.1165450811386108
INFO - 03/11/25 10:33:08 - 0:11:32 - Epoch 48: train_loss=1.1166841983795166
INFO - 03/11/25 10:33:22 - 0:11:46 - Epoch 49: train_loss=1.1165363788604736
INFO - 03/11/25 10:33:37 - 0:12:01 - Epoch 50: train_loss=1.1165013313293457
INFO - 03/11/25 10:33:51 - 0:12:15 - Epoch 51: train_loss=1.116653561592102
INFO - 03/11/25 10:34:05 - 0:12:30 - Epoch 52: train_loss=1.1162232160568237
INFO - 03/11/25 10:34:20 - 0:12:44 - Epoch 53: train_loss=1.1165094375610352
INFO - 03/11/25 10:34:34 - 0:12:58 - Epoch 54: train_loss=1.1163039207458496
INFO - 03/11/25 10:34:49 - 0:13:13 - Epoch 55: train_loss=1.1164517402648926
INFO - 03/11/25 10:35:03 - 0:13:27 - Epoch 56: train_loss=1.116428256034851
INFO - 03/11/25 10:35:17 - 0:13:42 - Epoch 57: train_loss=1.1162173748016357
INFO - 03/11/25 10:35:32 - 0:13:56 - Epoch 58: train_loss=1.116472601890564
INFO - 03/11/25 10:35:46 - 0:14:10 - Epoch 59: train_loss=1.116234540939331
INFO - 03/11/25 10:36:01 - 0:14:25 - Epoch 60: train_loss=1.116206169128418
INFO - 03/11/25 10:36:15 - 0:14:39 - Epoch 61: train_loss=1.1163948774337769
INFO - 03/11/25 10:36:29 - 0:14:53 - Epoch 62: train_loss=1.116200566291809
INFO - 03/11/25 10:36:44 - 0:15:08 - Epoch 63: train_loss=1.1164088249206543
INFO - 03/11/25 10:36:58 - 0:15:22 - Epoch 64: train_loss=1.1161882877349854
INFO - 03/11/25 10:37:12 - 0:15:37 - Epoch 65: train_loss=1.1161762475967407
INFO - 03/11/25 10:37:27 - 0:15:51 - Epoch 66: train_loss=1.1162112951278687
INFO - 03/11/25 10:37:41 - 0:16:05 - Epoch 67: train_loss=1.1161869764328003
INFO - 03/11/25 10:37:56 - 0:16:20 - Epoch 68: train_loss=1.116174578666687
INFO - 03/11/25 10:38:10 - 0:16:34 - Epoch 69: train_loss=1.1161646842956543
INFO - 03/11/25 10:38:24 - 0:16:48 - Epoch 70: train_loss=1.1161553859710693
INFO - 03/11/25 10:38:39 - 0:17:03 - Epoch 71: train_loss=1.11615788936615
INFO - 03/11/25 10:38:53 - 0:17:17 - Epoch 72: train_loss=1.1156623363494873
INFO - 03/11/25 10:39:07 - 0:17:32 - Epoch 73: train_loss=1.1161764860153198
INFO - 03/11/25 10:39:22 - 0:17:46 - Epoch 74: train_loss=1.1161397695541382
INFO - 03/11/25 10:39:36 - 0:18:00 - Epoch 75: train_loss=1.1161645650863647
INFO - 03/11/25 10:39:51 - 0:18:15 - Epoch 76: train_loss=1.1161394119262695
INFO - 03/11/25 10:40:05 - 0:18:29 - Epoch 77: train_loss=1.1161381006240845
INFO - 03/11/25 10:40:19 - 0:18:43 - Epoch 78: train_loss=1.1161361932754517
INFO - 03/11/25 10:40:34 - 0:18:58 - Epoch 79: train_loss=1.11564302444458
INFO - 03/11/25 10:40:48 - 0:19:12 - Epoch 80: train_loss=1.116144061088562
INFO - 03/11/25 10:41:02 - 0:19:27 - Epoch 81: train_loss=1.1161175966262817
INFO - 03/11/25 10:41:17 - 0:19:41 - Epoch 82: train_loss=1.1161394119262695
INFO - 03/11/25 10:41:31 - 0:19:55 - Epoch 83: train_loss=1.116120457649231
INFO - 03/11/25 10:41:46 - 0:20:10 - Epoch 84: train_loss=1.1161177158355713
INFO - 03/11/25 10:42:00 - 0:20:24 - Epoch 85: train_loss=1.116105079650879
INFO - 03/11/25 10:42:14 - 0:20:39 - Epoch 86: train_loss=1.1161125898361206
INFO - 03/11/25 10:42:29 - 0:20:53 - Epoch 87: train_loss=1.115615725517273
INFO - 03/11/25 10:42:43 - 0:21:07 - Epoch 88: train_loss=1.1161123514175415
INFO - 03/11/25 10:42:58 - 0:21:22 - Epoch 89: train_loss=1.1156036853790283
INFO - 03/11/25 10:43:12 - 0:21:36 - Epoch 90: train_loss=1.1161315441131592
INFO - 03/11/25 10:43:26 - 0:21:50 - Epoch 91: train_loss=1.115598201751709
INFO - 03/11/25 10:43:41 - 0:22:05 - Epoch 92: train_loss=1.1161537170410156
INFO - 03/11/25 10:43:55 - 0:22:19 - Epoch 93: train_loss=1.1155974864959717
INFO - 03/11/25 10:44:09 - 0:22:34 - Epoch 94: train_loss=1.116389274597168
INFO - 03/11/25 10:44:24 - 0:22:48 - Epoch 95: train_loss=1.1155897378921509
INFO - 03/11/25 10:44:38 - 0:23:02 - Epoch 96: train_loss=1.1163508892059326
INFO - 03/11/25 10:44:53 - 0:23:17 - Epoch 97: train_loss=1.1161082983016968
INFO - 03/11/25 10:45:07 - 0:23:31 - Epoch 98: train_loss=1.1161386966705322
INFO - 03/11/25 10:45:21 - 0:23:45 - Epoch 99: train_loss=1.1160941123962402
INFO - 03/11/25 10:45:36 - 0:24:00 - Epoch 100: train_loss=1.1161431074142456
INFO - 03/11/25 10:45:50 - 0:24:14 - Epoch 101: train_loss=1.1160732507705688
INFO - 03/11/25 10:46:04 - 0:24:29 - Epoch 102: train_loss=1.1160852909088135
INFO - 03/11/25 10:46:19 - 0:24:43 - Epoch 103: train_loss=1.1160542964935303
INFO - 03/11/25 10:46:33 - 0:24:57 - Epoch 104: train_loss=1.1160516738891602
INFO - 03/11/25 10:46:48 - 0:25:12 - Epoch 105: train_loss=1.116062879562378
INFO - 03/11/25 10:47:02 - 0:25:26 - Epoch 106: train_loss=1.1160578727722168
INFO - 03/11/25 10:47:16 - 0:25:40 - Epoch 107: train_loss=1.1160701513290405
INFO - 03/11/25 10:47:31 - 0:25:55 - Epoch 108: train_loss=1.1155507564544678
INFO - 03/11/25 10:47:45 - 0:26:09 - Epoch 109: train_loss=1.11604905128479
INFO - 03/11/25 10:57:49 - 0:26:24 - Epoch 110: train_loss=1.116248369216919
INFO - 03/11/25 10:58:03 - 0:36:27 - Epoch 111: train_loss=1.1160469055175781
INFO - 03/11/25 10:58:17 - 0:36:42 - Epoch 112: train_loss=1.116286277770996
INFO - 03/11/25 10:59:33 - 0:36:56 - Epoch 113: train_loss=1.1160391569137573
INFO - 03/11/25 10:59:48 - 0:38:12 - Epoch 114: train_loss=1.1160602569580078
INFO - 03/11/25 11:00:02 - 0:38:26 - Epoch 115: train_loss=1.1160732507705688
INFO - 03/11/25 11:02:49 - 0:38:40 - Epoch 116: train_loss=1.1160409450531006
INFO - 03/11/25 11:03:04 - 0:41:28 - Epoch 117: train_loss=1.1160387992858887
INFO - 03/11/25 11:03:18 - 0:41:42 - Epoch 118: train_loss=1.1160115003585815
INFO - 03/11/25 11:03:32 - 0:41:56 - Epoch 119: train_loss=1.1160242557525635
INFO - 03/11/25 11:03:47 - 0:42:11 - Epoch 120: train_loss=1.1160058975219727
INFO - 03/11/25 11:04:01 - 0:42:25 - Epoch 121: train_loss=1.1160093545913696
INFO - 03/11/25 11:04:15 - 0:42:39 - Epoch 122: train_loss=1.1160012483596802
INFO - 03/11/25 11:04:30 - 0:42:54 - Epoch 123: train_loss=1.1160000562667847
INFO - 03/11/25 11:04:44 - 0:43:08 - Epoch 124: train_loss=1.1159917116165161
INFO - 03/11/25 11:06:31 - 0:43:23 - Epoch 125: train_loss=1.1159961223602295
INFO - 03/11/25 11:06:46 - 0:45:10 - Epoch 126: train_loss=1.1154935359954834
INFO - 03/11/25 11:07:00 - 0:45:24 - Epoch 127: train_loss=1.1159895658493042
INFO - 03/11/25 11:07:15 - 0:45:39 - Epoch 128: train_loss=1.1154841184616089
INFO - 03/11/25 11:07:29 - 0:45:53 - Epoch 129: train_loss=1.1160184144973755
INFO - 03/11/25 11:07:43 - 0:46:07 - Epoch 130: train_loss=1.1154814958572388
INFO - 03/11/25 11:07:58 - 0:46:22 - Epoch 131: train_loss=1.1162713766098022
INFO - 03/11/25 11:08:29 - 0:46:36 - Epoch 132: train_loss=1.115477442741394
INFO - 03/11/25 11:08:43 - 0:47:08 - Epoch 133: train_loss=1.1162457466125488
INFO - 03/11/25 11:08:58 - 0:47:22 - Epoch 134: train_loss=1.1159775257110596
INFO - 03/11/25 11:09:12 - 0:47:36 - Epoch 135: train_loss=1.1160049438476562
INFO - 03/11/25 11:09:27 - 0:47:51 - Epoch 136: train_loss=1.1159720420837402
INFO - 03/11/25 11:09:41 - 0:48:05 - Epoch 137: train_loss=1.1159875392913818
INFO - 03/11/25 11:09:55 - 0:48:20 - Epoch 138: train_loss=1.1159597635269165
INFO - 03/11/25 11:10:10 - 0:48:34 - Epoch 139: train_loss=1.1159731149673462
INFO - 03/11/25 11:10:24 - 0:48:48 - Epoch 140: train_loss=1.1159331798553467
INFO - 03/11/25 11:10:39 - 0:49:03 - Epoch 141: train_loss=1.1159584522247314
INFO - 03/11/25 11:10:53 - 0:49:17 - Epoch 142: train_loss=1.1159226894378662
INFO - 03/11/25 11:12:07 - 0:49:31 - Epoch 143: train_loss=1.1159350872039795
INFO - 03/11/25 11:14:02 - 0:50:46 - Epoch 144: train_loss=1.1159172058105469
INFO - 03/11/25 11:14:16 - 0:52:40 - Epoch 145: train_loss=1.1159203052520752
INFO - 03/11/25 11:14:31 - 0:52:55 - Epoch 146: train_loss=1.1159065961837769
INFO - 03/11/25 11:14:45 - 0:53:09 - Epoch 147: train_loss=1.115421175956726
INFO - 03/11/25 11:14:59 - 0:53:23 - Epoch 148: train_loss=1.11592698097229
INFO - 03/11/25 11:15:14 - 0:53:38 - Epoch 149: train_loss=1.1154067516326904
INFO - 03/11/25 11:15:28 - 0:53:52 - Epoch 150: train_loss=1.115952730178833
INFO - 03/11/25 11:15:42 - 0:54:07 - Epoch 151: train_loss=1.1153956651687622
INFO - 03/11/25 11:15:57 - 0:54:21 - Epoch 152: train_loss=1.1159603595733643
INFO - 03/11/25 11:16:11 - 0:54:35 - Epoch 153: train_loss=1.115387201309204
INFO - 03/11/25 11:16:26 - 0:54:50 - Epoch 154: train_loss=1.1159342527389526
INFO - 03/11/25 11:16:40 - 0:55:04 - Epoch 155: train_loss=1.1153837442398071
INFO - 03/11/25 11:16:54 - 0:55:18 - Epoch 156: train_loss=1.1158925294876099
INFO - 03/11/25 11:17:09 - 0:55:33 - Epoch 157: train_loss=1.1153815984725952
INFO - 03/11/25 11:17:23 - 0:55:47 - Epoch 158: train_loss=1.1158709526062012
INFO - 03/11/25 11:17:37 - 0:56:02 - Epoch 159: train_loss=1.1158733367919922
INFO - 03/11/25 11:17:52 - 0:56:16 - Epoch 160: train_loss=1.11586332321167
INFO - 03/11/25 11:18:06 - 0:56:30 - Epoch 161: train_loss=1.1158649921417236
INFO - 03/11/25 11:18:21 - 0:56:45 - Epoch 162: train_loss=1.115852952003479
INFO - 03/11/25 11:18:35 - 0:56:59 - Epoch 163: train_loss=1.1158475875854492
INFO - 03/11/25 11:18:49 - 0:57:14 - Epoch 164: train_loss=1.1158459186553955
INFO - 03/11/25 11:19:04 - 0:57:28 - Epoch 165: train_loss=1.1153440475463867
INFO - 03/11/25 11:19:42 - 0:57:42 - Epoch 166: train_loss=1.1158349514007568
INFO - 03/11/25 11:19:57 - 0:58:21 - Epoch 167: train_loss=1.1153395175933838
INFO - 03/11/25 11:20:11 - 0:58:35 - Epoch 168: train_loss=1.1158370971679688
INFO - 03/11/25 11:20:26 - 0:58:50 - Epoch 169: train_loss=1.1158503293991089
INFO - 03/11/25 11:21:12 - 0:59:04 - Epoch 170: train_loss=1.1158268451690674
INFO - 03/11/25 11:21:39 - 1:00:03 - Epoch 171: train_loss=1.1158510446548462
INFO - 03/11/25 11:21:53 - 1:00:18 - Epoch 172: train_loss=1.1153279542922974
INFO - 03/11/25 11:24:09 - 1:00:32 - Epoch 173: train_loss=1.1158480644226074
INFO - 03/11/25 11:24:23 - 1:02:48 - Epoch 174: train_loss=1.1153231859207153
INFO - 03/11/25 11:24:38 - 1:03:02 - Epoch 175: train_loss=1.115867018699646
INFO - 03/11/25 11:24:58 - 1:03:16 - Epoch 176: train_loss=1.1158020496368408
INFO - 03/11/25 11:25:12 - 1:03:36 - Epoch 177: train_loss=1.1158357858657837
INFO - 03/11/25 11:25:26 - 1:03:50 - Epoch 178: train_loss=1.1157991886138916
INFO - 03/11/25 11:25:41 - 1:04:05 - Epoch 179: train_loss=1.1158114671707153
INFO - 03/11/25 11:27:21 - 1:04:19 - Epoch 180: train_loss=1.1157903671264648
INFO - 03/11/25 11:27:36 - 1:06:01 - Epoch 181: train_loss=1.1158018112182617
INFO - 03/11/25 11:27:51 - 1:06:15 - Epoch 182: train_loss=1.1152923107147217
INFO - 03/11/25 11:28:05 - 1:06:29 - Epoch 183: train_loss=1.1157852411270142
INFO - 03/11/25 11:28:20 - 1:06:44 - Epoch 184: train_loss=1.11528742313385
INFO - 03/11/25 11:28:34 - 1:06:58 - Epoch 185: train_loss=1.115789532661438
INFO - 03/11/25 11:28:48 - 1:07:13 - Epoch 186: train_loss=1.1152793169021606
INFO - 03/11/25 11:29:03 - 1:07:27 - Epoch 187: train_loss=1.1158071756362915
INFO - 03/11/25 11:29:25 - 1:07:41 - Epoch 188: train_loss=1.1157716512680054
INFO - 03/11/25 11:29:39 - 1:08:03 - Epoch 189: train_loss=1.11579430103302
INFO - 03/11/25 11:29:53 - 1:08:18 - Epoch 190: train_loss=1.1157610416412354
INFO - 03/11/25 11:30:08 - 1:08:32 - Epoch 191: train_loss=1.1157751083374023
INFO - 03/11/25 11:30:22 - 1:08:46 - Epoch 192: train_loss=1.1157560348510742
INFO - 03/11/25 11:30:37 - 1:09:01 - Epoch 193: train_loss=1.1152820587158203
INFO - 03/11/25 11:30:51 - 1:09:15 - Epoch 194: train_loss=1.1157840490341187
INFO - 03/11/25 11:31:07 - 1:09:30 - Epoch 195: train_loss=1.1157617568969727
INFO - 03/11/25 11:31:25 - 1:09:45 - Epoch 196: train_loss=1.1157684326171875
INFO - 03/11/25 11:31:39 - 1:10:03 - Epoch 197: train_loss=1.115745186805725
INFO - 03/11/25 11:31:53 - 1:10:17 - Epoch 198: train_loss=1.115256428718567
INFO - 03/11/25 11:32:37 - 1:10:32 - Epoch 199: train_loss=1.1152533292770386
INFO - 03/11/25 11:32:37 - 1:11:01 - --------------------------Training Start-------------------------
WARNING - 03/11/25 11:32:44 - 1:11:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 11:32:44 - 1:11:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 94, in train_clu
                                            loss = model.loss(data, data['edge_index'], data['neg_edge_index'], device)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/models/hyperSE.py", line 88, in loss
                                            ass_j = ass_mat[k][edge_index[1]]
                                        torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.69 GiB. GPU 0 has a total capacity of 79.25 GiB of which 16.75 MiB is free. Including non-PyTorch memory, this process has 79.23 GiB memory in use. Of the allocated memory 73.25 GiB is allocated by PyTorch, and 5.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
                                        
                                        
INFO - 03/11/25 11:33:46 - 0:00:02 - 
                                     train iters 0
INFO - 03/11/25 11:33:47 - 0:00:03 - Epoch 1: train_loss=6.013554573059082
INFO - 03/11/25 11:33:49 - 0:00:04 - Epoch 2: train_loss=2.6235709190368652
INFO - 03/11/25 11:33:50 - 0:00:05 - Epoch 3: train_loss=3.004467487335205
INFO - 03/11/25 11:33:51 - 0:00:06 - Epoch 4: train_loss=2.8651881217956543
INFO - 03/11/25 11:33:52 - 0:00:07 - Epoch 5: train_loss=2.7046279907226562
INFO - 03/11/25 11:33:53 - 0:00:08 - Epoch 6: train_loss=2.570068597793579
INFO - 03/11/25 11:33:54 - 0:00:09 - Epoch 7: train_loss=2.456939220428467
INFO - 03/11/25 11:33:55 - 0:00:10 - Epoch 8: train_loss=2.360424518585205
INFO - 03/11/25 11:33:56 - 0:00:11 - Epoch 9: train_loss=2.276444911956787
INFO - 03/11/25 11:33:57 - 0:00:13 - Epoch 10: train_loss=2.202026844024658
INFO - 03/11/25 11:33:58 - 0:00:14 - Epoch 11: train_loss=2.134951591491699
INFO - 03/11/25 11:34:00 - 0:00:15 - Epoch 12: train_loss=2.073483943939209
INFO - 03/11/25 11:34:02 - 0:00:16 - Epoch 13: train_loss=2.0162343978881836
INFO - 03/11/25 11:34:03 - 0:00:18 - Epoch 14: train_loss=1.962032437324524
INFO - 03/11/25 11:34:04 - 0:00:19 - Epoch 15: train_loss=1.90985107421875
INFO - 03/11/25 11:34:05 - 0:00:20 - Epoch 16: train_loss=1.8587394952774048
INFO - 03/11/25 11:34:06 - 0:00:21 - Epoch 17: train_loss=1.8077855110168457
INFO - 03/11/25 11:34:07 - 0:00:23 - Epoch 18: train_loss=1.7560946941375732
INFO - 03/11/25 11:34:08 - 0:00:24 - Epoch 19: train_loss=1.7027631998062134
INFO - 03/11/25 11:34:10 - 0:00:25 - Epoch 20: train_loss=1.6468806266784668
INFO - 03/11/25 11:34:11 - 0:00:26 - Epoch 21: train_loss=1.5875537395477295
INFO - 03/11/25 11:34:12 - 0:00:27 - Epoch 22: train_loss=1.5239168405532837
INFO - 03/11/25 11:34:13 - 0:00:28 - Epoch 23: train_loss=1.4551604986190796
INFO - 03/11/25 11:34:14 - 0:00:29 - Epoch 24: train_loss=1.38069486618042
INFO - 03/11/25 11:34:15 - 0:00:30 - Epoch 25: train_loss=1.3006246089935303
INFO - 03/11/25 11:34:38 - 0:00:31 - Epoch 26: train_loss=1.2162282466888428
INFO - 03/11/25 11:34:39 - 0:00:55 - Epoch 27: train_loss=1.1304371356964111
INFO - 03/11/25 11:34:40 - 0:00:56 - Epoch 28: train_loss=1.0477265119552612
INFO - 03/11/25 11:34:42 - 0:00:57 - Epoch 29: train_loss=0.9743770360946655
INFO - 03/11/25 11:34:43 - 0:00:58 - Epoch 30: train_loss=0.918068528175354
INFO - 03/11/25 11:34:44 - 0:00:59 - Epoch 31: train_loss=0.8871082663536072
INFO - 03/11/25 11:34:45 - 0:01:00 - Epoch 32: train_loss=0.872908890247345
INFO - 03/11/25 11:34:46 - 0:01:01 - Epoch 33: train_loss=0.8819270133972168
INFO - 03/11/25 11:34:47 - 0:01:02 - Epoch 34: train_loss=0.8982250690460205
INFO - 03/11/25 11:34:48 - 0:01:03 - Epoch 35: train_loss=0.9120671153068542
INFO - 03/11/25 11:34:49 - 0:01:04 - Epoch 36: train_loss=0.9181699752807617
INFO - 03/11/25 11:34:50 - 0:01:06 - Epoch 37: train_loss=0.9158353209495544
INFO - 03/11/25 11:34:51 - 0:01:07 - Epoch 38: train_loss=0.9060214161872864
INFO - 03/11/25 11:35:01 - 0:01:08 - Epoch 39: train_loss=0.89020836353302
INFO - 03/11/25 11:35:02 - 0:01:17 - Epoch 40: train_loss=0.870050311088562
INFO - 03/11/25 11:35:03 - 0:01:18 - Epoch 41: train_loss=0.8477804660797119
INFO - 03/11/25 11:35:04 - 0:01:19 - Epoch 42: train_loss=0.8267472386360168
INFO - 03/11/25 11:35:05 - 0:01:20 - Epoch 43: train_loss=0.8083967566490173
INFO - 03/11/25 11:35:06 - 0:01:21 - Epoch 44: train_loss=0.7984055876731873
INFO - 03/11/25 11:35:07 - 0:01:22 - Epoch 45: train_loss=0.7961763143539429
INFO - 03/11/25 11:35:08 - 0:01:24 - Epoch 46: train_loss=0.7993307113647461
INFO - 03/11/25 11:35:09 - 0:01:25 - Epoch 47: train_loss=0.8029423952102661
INFO - 03/11/25 11:35:11 - 0:01:26 - Epoch 48: train_loss=0.8049137592315674
INFO - 03/11/25 11:35:12 - 0:01:27 - Epoch 49: train_loss=0.8052999973297119
INFO - 03/11/25 11:35:13 - 0:01:28 - Epoch 50: train_loss=0.8033487200737
INFO - 03/11/25 11:35:14 - 0:01:29 - Epoch 51: train_loss=0.7992939949035645
INFO - 03/11/25 11:35:15 - 0:01:30 - Epoch 52: train_loss=0.7932578921318054
INFO - 03/11/25 11:35:16 - 0:01:31 - Epoch 53: train_loss=0.7853664755821228
INFO - 03/11/25 11:35:17 - 0:01:32 - Epoch 54: train_loss=0.7768049240112305
INFO - 03/11/25 11:35:18 - 0:01:33 - Epoch 55: train_loss=0.7694030404090881
INFO - 03/11/25 11:35:19 - 0:01:35 - Epoch 56: train_loss=0.766140878200531
INFO - 03/11/25 11:35:20 - 0:01:36 - Epoch 57: train_loss=0.7672828435897827
INFO - 03/11/25 11:35:22 - 0:01:37 - Epoch 58: train_loss=0.7689242959022522
INFO - 03/11/25 11:35:23 - 0:01:38 - Epoch 59: train_loss=0.7688033580780029
INFO - 03/11/25 11:35:24 - 0:01:39 - Epoch 60: train_loss=0.7670091390609741
INFO - 03/11/25 11:35:25 - 0:01:40 - Epoch 61: train_loss=0.7640950083732605
INFO - 03/11/25 11:35:26 - 0:01:41 - Epoch 62: train_loss=0.7608407735824585
INFO - 03/11/25 11:35:27 - 0:01:42 - Epoch 63: train_loss=0.7599651217460632
INFO - 03/11/25 11:35:28 - 0:01:43 - Epoch 64: train_loss=0.754835844039917
INFO - 03/11/25 11:35:29 - 0:01:44 - Epoch 65: train_loss=0.749614953994751
INFO - 03/11/25 11:35:30 - 0:01:46 - Epoch 66: train_loss=0.7489503622055054
INFO - 03/11/25 11:35:31 - 0:01:47 - Epoch 67: train_loss=0.7491923570632935
INFO - 03/11/25 11:35:33 - 0:01:48 - Epoch 68: train_loss=0.7482491731643677
INFO - 03/11/25 11:35:34 - 0:01:49 - Epoch 69: train_loss=0.7458698153495789
INFO - 03/11/25 11:35:35 - 0:01:50 - Epoch 70: train_loss=0.7410200834274292
INFO - 03/11/25 11:35:36 - 0:01:51 - Epoch 71: train_loss=0.7381202578544617
INFO - 03/11/25 11:35:37 - 0:01:52 - Epoch 72: train_loss=0.7390941381454468
INFO - 03/11/25 11:35:38 - 0:01:53 - Epoch 73: train_loss=0.7383482456207275
INFO - 03/11/25 11:35:39 - 0:01:54 - Epoch 74: train_loss=0.7367937564849854
INFO - 03/11/25 11:35:40 - 0:01:55 - Epoch 75: train_loss=0.7332941889762878
INFO - 03/11/25 11:35:41 - 0:01:56 - Epoch 76: train_loss=0.731214165687561
INFO - 03/11/25 11:35:42 - 0:01:58 - Epoch 77: train_loss=0.7308548092842102
INFO - 03/11/25 11:35:44 - 0:01:59 - Epoch 78: train_loss=0.7291301488876343
INFO - 03/11/25 11:35:45 - 0:02:00 - Epoch 79: train_loss=0.7272275686264038
INFO - 03/11/25 11:35:46 - 0:02:01 - Epoch 80: train_loss=0.7257733941078186
INFO - 03/11/25 11:35:47 - 0:02:02 - Epoch 81: train_loss=0.7226079106330872
INFO - 03/11/25 11:35:48 - 0:02:03 - Epoch 82: train_loss=0.721416711807251
INFO - 03/11/25 11:35:49 - 0:02:04 - Epoch 83: train_loss=0.7221595048904419
INFO - 03/11/25 11:35:50 - 0:02:05 - Epoch 84: train_loss=0.7201848030090332
INFO - 03/11/25 11:35:51 - 0:02:07 - Epoch 85: train_loss=0.7172176241874695
INFO - 03/11/25 11:35:52 - 0:02:08 - Epoch 86: train_loss=0.7157878279685974
INFO - 03/11/25 11:35:54 - 0:02:09 - Epoch 87: train_loss=0.7157797813415527
INFO - 03/11/25 11:35:55 - 0:02:10 - Epoch 88: train_loss=0.7113534212112427
INFO - 03/11/25 11:35:56 - 0:02:11 - Epoch 89: train_loss=0.710838258266449
INFO - 03/11/25 11:35:57 - 0:02:12 - Epoch 90: train_loss=0.7100362777709961
INFO - 03/11/25 11:35:58 - 0:02:13 - Epoch 91: train_loss=0.7060246467590332
INFO - 03/11/25 11:35:59 - 0:02:14 - Epoch 92: train_loss=0.7079437375068665
INFO - 03/11/25 11:36:00 - 0:02:15 - Epoch 93: train_loss=0.7034527659416199
INFO - 03/11/25 11:36:01 - 0:02:16 - Epoch 94: train_loss=0.7019469141960144
INFO - 03/11/25 11:36:02 - 0:02:18 - Epoch 95: train_loss=0.7055009603500366
INFO - 03/11/25 11:36:03 - 0:02:19 - Epoch 96: train_loss=0.698566734790802
INFO - 03/11/25 11:36:05 - 0:02:20 - Epoch 97: train_loss=0.7045686841011047
INFO - 03/11/25 11:36:06 - 0:02:21 - Epoch 98: train_loss=0.6984826326370239
INFO - 03/11/25 11:36:07 - 0:02:22 - Epoch 99: train_loss=0.702914834022522
INFO - 03/11/25 11:36:08 - 0:02:23 - Epoch 100: train_loss=0.700143575668335
INFO - 03/11/25 11:36:09 - 0:02:24 - Epoch 101: train_loss=0.6946402192115784
INFO - 03/11/25 11:36:10 - 0:02:25 - Epoch 102: train_loss=0.6940244436264038
INFO - 03/11/25 11:36:11 - 0:02:26 - Epoch 103: train_loss=0.6911617517471313
INFO - 03/11/25 11:36:12 - 0:02:27 - Epoch 104: train_loss=0.6887199878692627
INFO - 03/11/25 11:36:13 - 0:02:29 - Epoch 105: train_loss=0.6881765723228455
INFO - 03/11/25 11:36:16 - 0:02:30 - Epoch 106: train_loss=0.6844055652618408
INFO - 03/11/25 11:36:17 - 0:02:32 - Epoch 107: train_loss=0.6854329705238342
INFO - 03/11/25 11:36:18 - 0:02:33 - Epoch 108: train_loss=0.6805317997932434
INFO - 03/11/25 11:36:19 - 0:02:34 - Epoch 109: train_loss=0.6842347979545593
INFO - 03/11/25 11:36:20 - 0:02:35 - Epoch 110: train_loss=0.6801521182060242
INFO - 03/11/25 11:36:21 - 0:02:36 - Epoch 111: train_loss=0.6804913282394409
INFO - 03/11/25 11:36:22 - 0:02:38 - Epoch 112: train_loss=0.678256094455719
INFO - 03/11/25 11:36:23 - 0:02:39 - Epoch 113: train_loss=0.6746890544891357
INFO - 03/11/25 11:36:25 - 0:02:40 - Epoch 114: train_loss=0.6726399660110474
INFO - 03/11/25 11:36:26 - 0:02:41 - Epoch 115: train_loss=0.6722743511199951
INFO - 03/11/25 11:36:27 - 0:02:42 - Epoch 116: train_loss=0.6686722040176392
INFO - 03/11/25 11:36:28 - 0:02:43 - Epoch 117: train_loss=0.6703256964683533
INFO - 03/11/25 11:36:29 - 0:02:44 - Epoch 118: train_loss=0.6668748259544373
INFO - 03/11/25 11:36:30 - 0:02:45 - Epoch 119: train_loss=0.6663222908973694
INFO - 03/11/25 11:36:32 - 0:02:46 - Epoch 120: train_loss=0.6641702651977539
INFO - 03/11/25 11:36:33 - 0:02:48 - Epoch 121: train_loss=0.6612460017204285
INFO - 03/11/25 11:36:34 - 0:02:50 - Epoch 122: train_loss=0.6586332321166992
INFO - 03/11/25 11:36:35 - 0:02:51 - Epoch 123: train_loss=0.6595739722251892
INFO - 03/11/25 11:36:37 - 0:02:52 - Epoch 124: train_loss=0.6562252640724182
INFO - 03/11/25 11:36:38 - 0:02:53 - Epoch 125: train_loss=0.6562326550483704
INFO - 03/11/25 11:36:39 - 0:02:54 - Epoch 126: train_loss=0.6538471579551697
INFO - 03/11/25 11:36:40 - 0:02:55 - Epoch 127: train_loss=0.6510404944419861
INFO - 03/11/25 11:36:41 - 0:02:56 - Epoch 128: train_loss=0.648472249507904
INFO - 03/11/25 11:36:42 - 0:02:57 - Epoch 129: train_loss=0.6492453813552856
INFO - 03/11/25 11:36:43 - 0:02:58 - Epoch 130: train_loss=0.6461982727050781
INFO - 03/11/25 11:36:44 - 0:02:59 - Epoch 131: train_loss=0.6452044248580933
INFO - 03/11/25 11:36:45 - 0:03:01 - Epoch 132: train_loss=0.6426860690116882
INFO - 03/11/25 11:36:46 - 0:03:02 - Epoch 133: train_loss=0.6412684321403503
INFO - 03/11/25 11:36:48 - 0:03:03 - Epoch 134: train_loss=0.6383323073387146
INFO - 03/11/25 11:36:49 - 0:03:04 - Epoch 135: train_loss=0.6388446688652039
INFO - 03/11/25 11:36:50 - 0:03:05 - Epoch 136: train_loss=0.6360015273094177
INFO - 03/11/25 11:36:51 - 0:03:06 - Epoch 137: train_loss=0.6343472599983215
INFO - 03/11/25 11:36:52 - 0:03:07 - Epoch 138: train_loss=0.6317616105079651
INFO - 03/11/25 11:36:53 - 0:03:08 - Epoch 139: train_loss=0.631161093711853
INFO - 03/11/25 11:36:54 - 0:03:09 - Epoch 140: train_loss=0.6281712055206299
INFO - 03/11/25 11:36:55 - 0:03:10 - Epoch 141: train_loss=0.6280798316001892
INFO - 03/11/25 11:36:56 - 0:03:12 - Epoch 142: train_loss=0.6253724694252014
INFO - 03/11/25 11:36:57 - 0:03:13 - Epoch 143: train_loss=0.6237218976020813
INFO - 03/11/25 11:36:59 - 0:03:14 - Epoch 144: train_loss=0.6212111115455627
INFO - 03/11/25 11:37:00 - 0:03:15 - Epoch 145: train_loss=0.6207869648933411
INFO - 03/11/25 11:37:01 - 0:03:16 - Epoch 146: train_loss=0.618577241897583
INFO - 03/11/25 11:37:02 - 0:03:17 - Epoch 147: train_loss=0.6166379451751709
INFO - 03/11/25 11:37:03 - 0:03:18 - Epoch 148: train_loss=0.6151545643806458
INFO - 03/11/25 11:37:04 - 0:03:19 - Epoch 149: train_loss=0.6124444007873535
INFO - 03/11/25 11:37:05 - 0:03:20 - Epoch 150: train_loss=0.6112719178199768
INFO - 03/11/25 11:37:06 - 0:03:21 - Epoch 151: train_loss=0.6091427803039551
INFO - 03/11/25 11:37:07 - 0:03:22 - Epoch 152: train_loss=0.6066977977752686
INFO - 03/11/25 11:37:08 - 0:03:24 - Epoch 153: train_loss=0.6053915619850159
INFO - 03/11/25 11:37:10 - 0:03:25 - Epoch 154: train_loss=0.6019589304924011
INFO - 03/11/25 11:37:11 - 0:03:26 - Epoch 155: train_loss=0.6036912798881531
INFO - 03/11/25 11:37:12 - 0:03:27 - Epoch 156: train_loss=0.5978381037712097
INFO - 03/11/25 11:37:13 - 0:03:28 - Epoch 157: train_loss=0.5999739170074463
INFO - 03/11/25 11:37:14 - 0:03:29 - Epoch 158: train_loss=0.5961008667945862
INFO - 03/11/25 11:37:15 - 0:03:30 - Epoch 159: train_loss=0.5967251658439636
INFO - 03/11/25 11:37:16 - 0:03:31 - Epoch 160: train_loss=0.5948770046234131
INFO - 03/11/25 11:37:17 - 0:03:32 - Epoch 161: train_loss=0.5881030559539795
INFO - 03/11/25 11:37:18 - 0:03:33 - Epoch 162: train_loss=0.5893828272819519
INFO - 03/11/25 11:37:19 - 0:03:35 - Epoch 163: train_loss=0.5883877277374268
INFO - 03/11/25 11:37:20 - 0:03:36 - Epoch 164: train_loss=0.5822643637657166
INFO - 03/11/25 11:37:22 - 0:03:37 - Epoch 165: train_loss=0.5842128396034241
INFO - 03/11/25 11:37:23 - 0:03:38 - Epoch 166: train_loss=0.5788577795028687
INFO - 03/11/25 11:37:24 - 0:03:39 - Epoch 167: train_loss=0.5858438611030579
INFO - 03/11/25 11:37:25 - 0:03:40 - Epoch 168: train_loss=0.584282636642456
INFO - 03/11/25 11:37:26 - 0:03:41 - Epoch 169: train_loss=0.573022186756134
INFO - 03/11/25 11:37:27 - 0:03:42 - Epoch 170: train_loss=0.5868842601776123
INFO - 03/11/25 11:37:28 - 0:03:43 - Epoch 171: train_loss=0.5830140113830566
INFO - 03/11/25 11:37:29 - 0:03:44 - Epoch 172: train_loss=0.5723601579666138
INFO - 03/11/25 11:37:30 - 0:03:46 - Epoch 173: train_loss=0.5720070004463196
INFO - 03/11/25 11:37:31 - 0:03:47 - Epoch 174: train_loss=0.5718532204627991
INFO - 03/11/25 11:37:33 - 0:03:48 - Epoch 175: train_loss=0.5613329410552979
INFO - 03/11/25 11:37:34 - 0:03:49 - Epoch 176: train_loss=0.5689952969551086
INFO - 03/11/25 11:37:35 - 0:03:50 - Epoch 177: train_loss=0.5599258542060852
INFO - 03/11/25 11:37:36 - 0:03:51 - Epoch 178: train_loss=0.564912736415863
INFO - 03/11/25 11:37:37 - 0:03:52 - Epoch 179: train_loss=0.5600242614746094
INFO - 03/11/25 11:37:38 - 0:03:53 - Epoch 180: train_loss=0.5586531162261963
INFO - 03/11/25 11:37:39 - 0:03:54 - Epoch 181: train_loss=0.5555704236030579
INFO - 03/11/25 11:37:40 - 0:03:55 - Epoch 182: train_loss=0.5521493554115295
INFO - 03/11/25 11:37:41 - 0:03:57 - Epoch 183: train_loss=0.5549490451812744
INFO - 03/11/25 11:37:42 - 0:03:58 - Epoch 184: train_loss=0.5500409007072449
INFO - 03/11/25 11:37:44 - 0:03:59 - Epoch 185: train_loss=0.5518313646316528
INFO - 03/11/25 11:37:45 - 0:04:00 - Epoch 186: train_loss=0.5463149547576904
INFO - 03/11/25 11:37:46 - 0:04:01 - Epoch 187: train_loss=0.5496225953102112
INFO - 03/11/25 11:37:47 - 0:04:02 - Epoch 188: train_loss=0.5436515212059021
INFO - 03/11/25 11:37:48 - 0:04:03 - Epoch 189: train_loss=0.5463614463806152
INFO - 03/11/25 11:37:49 - 0:04:04 - Epoch 190: train_loss=0.5425947904586792
INFO - 03/11/25 11:37:50 - 0:04:05 - Epoch 191: train_loss=0.5396406650543213
INFO - 03/11/25 11:37:51 - 0:04:06 - Epoch 192: train_loss=0.536274254322052
INFO - 03/11/25 11:37:52 - 0:04:07 - Epoch 193: train_loss=0.5368326306343079
INFO - 03/11/25 11:37:53 - 0:04:09 - Epoch 194: train_loss=0.5327221751213074
INFO - 03/11/25 11:37:55 - 0:04:10 - Epoch 195: train_loss=0.532355546951294
INFO - 03/11/25 11:37:56 - 0:04:11 - Epoch 196: train_loss=0.5295878648757935
INFO - 03/11/25 11:37:57 - 0:04:12 - Epoch 197: train_loss=0.5261948108673096
INFO - 03/11/25 11:37:58 - 0:04:13 - Epoch 198: train_loss=0.5226999521255493
INFO - 03/11/25 11:37:59 - 0:04:14 - Epoch 199: train_loss=0.5249223113059998
INFO - 03/11/25 11:37:59 - 0:04:14 - --------------------------Training Start-------------------------
INFO - 03/11/25 11:38:01 - 0:04:16 - Epoch 1: train_loss=15.310712814331055
INFO - 03/11/25 11:38:03 - 0:04:18 - Epoch 2: train_loss=15.471606254577637
INFO - 03/11/25 11:38:04 - 0:04:19 - Epoch 3: train_loss=15.133831024169922
INFO - 03/11/25 11:38:05 - 0:04:21 - Epoch 4: train_loss=14.83789348602295
INFO - 03/11/25 11:38:07 - 0:04:22 - Epoch 5: train_loss=14.689549446105957
INFO - 03/11/25 11:38:08 - 0:04:23 - Epoch 6: train_loss=14.700129508972168
INFO - 03/11/25 11:38:09 - 0:04:25 - Epoch 7: train_loss=14.695001602172852
INFO - 03/11/25 11:38:11 - 0:04:26 - Epoch 8: train_loss=14.732969284057617
INFO - 03/11/25 11:38:12 - 0:04:27 - Epoch 9: train_loss=14.661897659301758
INFO - 03/11/25 11:38:14 - 0:04:29 - Epoch 10: train_loss=14.62073802947998
INFO - 03/11/25 11:38:15 - 0:04:30 - Epoch 11: train_loss=14.658218383789062
INFO - 03/11/25 11:38:16 - 0:04:31 - Epoch 12: train_loss=14.650664329528809
INFO - 03/11/25 11:38:18 - 0:04:33 - Epoch 13: train_loss=14.608443260192871
INFO - 03/11/25 11:38:19 - 0:04:34 - Epoch 14: train_loss=14.584120750427246
INFO - 03/11/25 11:38:20 - 0:04:35 - Epoch 15: train_loss=14.568203926086426
INFO - 03/11/25 11:38:22 - 0:04:37 - Epoch 16: train_loss=14.559067726135254
INFO - 03/11/25 11:38:23 - 0:04:38 - Epoch 17: train_loss=14.556922912597656
INFO - 03/11/25 11:38:24 - 0:04:40 - Epoch 18: train_loss=14.552873611450195
INFO - 03/11/25 11:38:26 - 0:04:41 - Epoch 19: train_loss=14.553834915161133
INFO - 03/11/25 11:38:27 - 0:04:42 - Epoch 20: train_loss=14.547962188720703
INFO - 03/11/25 11:38:28 - 0:04:44 - Epoch 21: train_loss=14.536093711853027
INFO - 03/11/25 11:38:30 - 0:04:45 - Epoch 22: train_loss=14.526471138000488
INFO - 03/11/25 11:38:31 - 0:04:46 - Epoch 23: train_loss=14.526243209838867
INFO - 03/11/25 11:38:33 - 0:04:48 - Epoch 24: train_loss=14.525101661682129
INFO - 03/11/25 11:38:34 - 0:04:49 - Epoch 25: train_loss=14.523868560791016
INFO - 03/11/25 11:38:35 - 0:04:50 - Epoch 26: train_loss=14.522236824035645
INFO - 03/11/25 11:38:37 - 0:04:52 - Epoch 27: train_loss=14.514726638793945
INFO - 03/11/25 11:38:38 - 0:04:53 - Epoch 28: train_loss=14.509225845336914
INFO - 03/11/25 11:38:39 - 0:04:54 - Epoch 29: train_loss=14.506911277770996
INFO - 03/11/25 11:38:41 - 0:04:56 - Epoch 30: train_loss=14.504619598388672
INFO - 03/11/25 11:38:42 - 0:04:57 - Epoch 31: train_loss=14.506878852844238
INFO - 03/11/25 11:38:43 - 0:04:59 - Epoch 32: train_loss=14.50252914428711
INFO - 03/11/25 11:38:45 - 0:05:00 - Epoch 33: train_loss=14.500052452087402
INFO - 03/11/25 11:38:46 - 0:05:01 - Epoch 34: train_loss=14.498417854309082
INFO - 03/11/25 11:38:47 - 0:05:03 - Epoch 35: train_loss=14.496271133422852
INFO - 03/11/25 11:38:49 - 0:05:04 - Epoch 36: train_loss=14.494129180908203
INFO - 03/11/25 11:38:50 - 0:05:05 - Epoch 37: train_loss=14.492362976074219
INFO - 03/11/25 11:39:19 - 0:05:07 - Epoch 38: train_loss=14.492690086364746
INFO - 03/11/25 11:39:20 - 0:05:36 - Epoch 39: train_loss=14.489313125610352
INFO - 03/11/25 11:39:22 - 0:05:37 - Epoch 40: train_loss=14.488204002380371
INFO - 03/11/25 11:39:23 - 0:05:38 - Epoch 41: train_loss=14.485748291015625
INFO - 03/11/25 11:39:24 - 0:05:40 - Epoch 42: train_loss=14.484633445739746
INFO - 03/11/25 11:39:26 - 0:05:41 - Epoch 43: train_loss=14.483589172363281
INFO - 03/11/25 11:39:27 - 0:05:42 - Epoch 44: train_loss=14.482544898986816
INFO - 03/11/25 11:39:28 - 0:05:44 - Epoch 45: train_loss=14.481476783752441
INFO - 03/11/25 11:39:30 - 0:05:45 - Epoch 46: train_loss=14.48122787475586
INFO - 03/11/25 11:39:31 - 0:05:46 - Epoch 47: train_loss=14.479959487915039
INFO - 03/11/25 11:39:33 - 0:05:48 - Epoch 48: train_loss=14.47677993774414
INFO - 03/11/25 11:39:34 - 0:05:49 - Epoch 49: train_loss=14.480231285095215
INFO - 03/11/25 11:39:35 - 0:05:50 - Epoch 50: train_loss=14.473483085632324
INFO - 03/11/25 11:39:37 - 0:05:52 - Epoch 51: train_loss=14.475262641906738
INFO - 03/11/25 11:39:38 - 0:05:53 - Epoch 52: train_loss=14.472026824951172
INFO - 03/11/25 11:39:39 - 0:05:55 - Epoch 53: train_loss=14.475075721740723
INFO - 03/11/25 11:39:41 - 0:05:56 - Epoch 54: train_loss=14.470426559448242
INFO - 03/11/25 11:39:42 - 0:05:57 - Epoch 55: train_loss=14.476530075073242
INFO - 03/11/25 11:39:43 - 0:05:59 - Epoch 56: train_loss=14.468337059020996
INFO - 03/11/25 11:39:45 - 0:06:00 - Epoch 57: train_loss=14.480402946472168
INFO - 03/11/25 11:39:46 - 0:06:01 - Epoch 58: train_loss=14.478569030761719
INFO - 03/11/25 11:39:48 - 0:06:03 - Epoch 59: train_loss=14.469979286193848
INFO - 03/11/25 11:39:49 - 0:06:04 - Epoch 60: train_loss=14.47294807434082
INFO - 03/11/25 11:39:50 - 0:06:05 - Epoch 61: train_loss=14.471866607666016
INFO - 03/11/25 11:39:52 - 0:06:07 - Epoch 62: train_loss=14.464932441711426
INFO - 03/11/25 11:39:53 - 0:06:08 - Epoch 63: train_loss=14.471870422363281
INFO - 03/11/25 11:39:54 - 0:06:09 - Epoch 64: train_loss=14.466582298278809
INFO - 03/11/25 11:39:56 - 0:06:11 - Epoch 65: train_loss=14.470078468322754
INFO - 03/11/25 11:39:57 - 0:06:12 - Epoch 66: train_loss=14.47044563293457
INFO - 03/11/25 11:39:58 - 0:06:14 - Epoch 67: train_loss=14.460387229919434
INFO - 03/11/25 11:40:00 - 0:06:15 - Epoch 68: train_loss=14.467116355895996
INFO - 03/11/25 11:40:01 - 0:06:16 - Epoch 69: train_loss=14.462900161743164
INFO - 03/11/25 11:40:02 - 0:06:18 - Epoch 70: train_loss=14.46364974975586
INFO - 03/11/25 11:40:04 - 0:06:19 - Epoch 71: train_loss=14.46308422088623
INFO - 03/11/25 11:40:05 - 0:06:20 - Epoch 72: train_loss=14.459790229797363
INFO - 03/11/25 11:40:07 - 0:06:22 - Epoch 73: train_loss=14.461570739746094
INFO - 03/11/25 11:40:08 - 0:06:23 - Epoch 74: train_loss=14.459701538085938
INFO - 03/11/25 11:40:09 - 0:06:24 - Epoch 75: train_loss=14.457612037658691
INFO - 03/11/25 11:40:11 - 0:06:26 - Epoch 76: train_loss=14.458115577697754
INFO - 03/11/25 11:40:12 - 0:06:27 - Epoch 77: train_loss=14.452362060546875
INFO - 03/11/25 11:40:13 - 0:06:28 - Epoch 78: train_loss=14.461666107177734
INFO - 03/11/25 11:40:15 - 0:06:30 - Epoch 79: train_loss=14.45469856262207
INFO - 03/11/25 11:40:16 - 0:06:31 - Epoch 80: train_loss=14.463515281677246
INFO - 03/11/25 11:40:17 - 0:06:33 - Epoch 81: train_loss=14.464005470275879
INFO - 03/11/25 11:40:19 - 0:06:34 - Epoch 82: train_loss=14.45235824584961
INFO - 03/11/25 11:40:21 - 0:06:35 - Epoch 83: train_loss=14.458927154541016
INFO - 03/11/25 11:40:23 - 0:06:38 - Epoch 84: train_loss=14.45772933959961
INFO - 03/11/25 11:40:24 - 0:06:39 - Epoch 85: train_loss=14.452826499938965
INFO - 03/11/25 11:40:25 - 0:06:40 - Epoch 86: train_loss=14.452738761901855
INFO - 03/11/25 11:40:27 - 0:06:42 - Epoch 87: train_loss=14.453925132751465
INFO - 03/11/25 11:40:28 - 0:06:43 - Epoch 88: train_loss=14.447690963745117
INFO - 03/11/25 11:40:29 - 0:06:45 - Epoch 89: train_loss=14.457186698913574
INFO - 03/11/25 11:40:31 - 0:06:46 - Epoch 90: train_loss=14.456985473632812
INFO - 03/11/25 11:40:32 - 0:06:47 - Epoch 91: train_loss=14.443107604980469
INFO - 03/11/25 11:40:33 - 0:06:49 - Epoch 92: train_loss=14.454401016235352
INFO - 03/11/25 11:40:35 - 0:06:50 - Epoch 93: train_loss=14.451784133911133
INFO - 03/11/25 11:40:36 - 0:06:51 - Epoch 94: train_loss=14.446267127990723
INFO - 03/11/25 11:40:37 - 0:06:53 - Epoch 95: train_loss=14.447396278381348
INFO - 03/11/25 11:40:39 - 0:06:54 - Epoch 96: train_loss=14.44642162322998
INFO - 03/11/25 11:40:40 - 0:06:55 - Epoch 97: train_loss=14.44295597076416
INFO - 03/11/25 11:40:42 - 0:06:57 - Epoch 98: train_loss=14.444634437561035
INFO - 03/11/25 11:40:43 - 0:06:58 - Epoch 99: train_loss=14.439139366149902
INFO - 03/11/25 11:40:44 - 0:06:59 - Epoch 100: train_loss=14.450850486755371
INFO - 03/11/25 11:40:44 - 0:06:59 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:40:46 - 0:07:01 - Decoding cost time:  1.997 s
INFO - 03/11/25 11:40:49 - 0:07:04 - ------------------Saving best model-------------------
INFO - 03/11/25 11:40:49 - 0:07:04 - Epoch 100: ACC: 0.0, NMI: 0.03513110948833969, F1: 0.0, ARI: 0.00783604320733863
INFO - 03/11/25 11:40:49 - 0:07:04 - -------------------------------------------------------------------------
INFO - 03/11/25 11:40:51 - 0:07:06 - Epoch 101: train_loss=14.44836139678955
INFO - 03/11/25 11:40:52 - 0:07:07 - Epoch 102: train_loss=14.440677642822266
INFO - 03/11/25 11:40:53 - 0:07:09 - Epoch 103: train_loss=14.442560195922852
INFO - 03/11/25 11:40:55 - 0:07:10 - Epoch 104: train_loss=14.437806129455566
INFO - 03/11/25 11:40:56 - 0:07:11 - Epoch 105: train_loss=14.445196151733398
INFO - 03/11/25 11:40:57 - 0:07:13 - Epoch 106: train_loss=14.437967300415039
INFO - 03/11/25 11:40:59 - 0:07:14 - Epoch 107: train_loss=14.450634956359863
INFO - 03/11/25 11:41:00 - 0:07:15 - Epoch 108: train_loss=14.45127010345459
INFO - 03/11/25 11:41:02 - 0:07:17 - Epoch 109: train_loss=14.436766624450684
INFO - 03/11/25 11:41:03 - 0:07:18 - Epoch 110: train_loss=14.446712493896484
INFO - 03/11/25 11:41:04 - 0:07:19 - Epoch 111: train_loss=14.445030212402344
INFO - 03/11/25 11:41:06 - 0:07:21 - Epoch 112: train_loss=14.441805839538574
INFO - 03/11/25 11:41:07 - 0:07:22 - Epoch 113: train_loss=14.438529014587402
INFO - 03/11/25 11:41:08 - 0:07:24 - Epoch 114: train_loss=14.442865371704102
INFO - 03/11/25 11:41:10 - 0:07:25 - Epoch 115: train_loss=14.441912651062012
INFO - 03/11/25 11:41:11 - 0:07:26 - Epoch 116: train_loss=14.431731224060059
INFO - 03/11/25 11:41:12 - 0:07:28 - Epoch 117: train_loss=14.432781219482422
INFO - 03/11/25 11:41:14 - 0:07:29 - Epoch 118: train_loss=14.433128356933594
INFO - 03/11/25 11:41:15 - 0:07:30 - Epoch 119: train_loss=14.432157516479492
INFO - 03/11/25 11:41:16 - 0:07:32 - Epoch 120: train_loss=14.429595947265625
INFO - 03/11/25 11:41:18 - 0:07:33 - Epoch 121: train_loss=14.432604789733887
INFO - 03/11/25 11:41:19 - 0:07:34 - Epoch 122: train_loss=14.427966117858887
INFO - 03/11/25 11:41:21 - 0:07:36 - Epoch 123: train_loss=14.435124397277832
INFO - 03/11/25 11:41:22 - 0:07:37 - Epoch 124: train_loss=14.429123878479004
INFO - 03/11/25 11:41:23 - 0:07:38 - Epoch 125: train_loss=14.43997573852539
INFO - 03/11/25 11:41:25 - 0:07:40 - Epoch 126: train_loss=14.441917419433594
INFO - 03/11/25 11:41:26 - 0:07:41 - Epoch 127: train_loss=14.426712036132812
INFO - 03/11/25 11:41:27 - 0:07:43 - Epoch 128: train_loss=14.445819854736328
INFO - 03/11/25 11:41:29 - 0:07:44 - Epoch 129: train_loss=14.453452110290527
INFO - 03/11/25 11:41:30 - 0:07:45 - Epoch 130: train_loss=14.442949295043945
INFO - 03/11/25 11:41:31 - 0:07:47 - Epoch 131: train_loss=14.425618171691895
INFO - 03/11/25 11:41:33 - 0:07:48 - Epoch 132: train_loss=14.43759536743164
INFO - 03/11/25 11:41:34 - 0:07:49 - Epoch 133: train_loss=14.435282707214355
INFO - 03/11/25 11:41:35 - 0:07:51 - Epoch 134: train_loss=14.424877166748047
INFO - 03/11/25 11:41:37 - 0:07:52 - Epoch 135: train_loss=14.429346084594727
INFO - 03/11/25 11:41:38 - 0:07:53 - Epoch 136: train_loss=14.42643928527832
INFO - 03/11/25 11:41:40 - 0:07:55 - Epoch 137: train_loss=14.426155090332031
INFO - 03/11/25 11:41:41 - 0:07:56 - Epoch 138: train_loss=14.42612361907959
INFO - 03/11/25 11:41:42 - 0:07:57 - Epoch 139: train_loss=14.423334121704102
INFO - 03/11/25 11:41:44 - 0:07:59 - Epoch 140: train_loss=14.425585746765137
INFO - 03/11/25 11:41:45 - 0:08:00 - Epoch 141: train_loss=14.422222137451172
INFO - 03/11/25 11:41:46 - 0:08:02 - Epoch 142: train_loss=14.42725944519043
INFO - 03/11/25 11:41:48 - 0:08:03 - Epoch 143: train_loss=14.426719665527344
INFO - 03/11/25 11:41:49 - 0:08:04 - Epoch 144: train_loss=14.419658660888672
INFO - 03/11/25 11:41:50 - 0:08:06 - Epoch 145: train_loss=14.422260284423828
INFO - 03/11/25 11:41:52 - 0:08:07 - Epoch 146: train_loss=14.420202255249023
INFO - 03/11/25 11:41:53 - 0:08:08 - Epoch 147: train_loss=14.417935371398926
INFO - 03/11/25 11:41:54 - 0:08:10 - Epoch 148: train_loss=14.42296028137207
INFO - 03/11/25 11:41:56 - 0:08:11 - Epoch 149: train_loss=14.419010162353516
INFO - 03/11/25 11:41:57 - 0:08:12 - Epoch 150: train_loss=14.422247886657715
INFO - 03/11/25 11:41:59 - 0:08:14 - Epoch 151: train_loss=14.421536445617676
INFO - 03/11/25 11:42:00 - 0:08:15 - Epoch 152: train_loss=14.41744613647461
INFO - 03/11/25 11:42:01 - 0:08:16 - Epoch 153: train_loss=14.421807289123535
INFO - 03/11/25 11:42:03 - 0:08:18 - Epoch 154: train_loss=14.414557456970215
INFO - 03/11/25 11:42:04 - 0:08:19 - Epoch 155: train_loss=14.419126510620117
INFO - 03/11/25 11:42:05 - 0:08:20 - Epoch 156: train_loss=14.415855407714844
INFO - 03/11/25 11:42:07 - 0:08:22 - Epoch 157: train_loss=14.41920280456543
INFO - 03/11/25 11:42:08 - 0:08:23 - Epoch 158: train_loss=14.413002014160156
INFO - 03/11/25 11:42:09 - 0:08:25 - Epoch 159: train_loss=14.423603057861328
INFO - 03/11/25 11:42:11 - 0:08:26 - Epoch 160: train_loss=14.420732498168945
INFO - 03/11/25 11:42:12 - 0:08:27 - Epoch 161: train_loss=14.418559074401855
INFO - 03/11/25 11:42:13 - 0:08:29 - Epoch 162: train_loss=14.418657302856445
INFO - 03/11/25 11:42:15 - 0:08:30 - Epoch 163: train_loss=14.416045188903809
INFO - 03/11/25 11:42:16 - 0:08:31 - Epoch 164: train_loss=14.417797088623047
INFO - 03/11/25 11:42:18 - 0:08:33 - Epoch 165: train_loss=14.413981437683105
INFO - 03/11/25 11:42:19 - 0:08:34 - Epoch 166: train_loss=14.421119689941406
INFO - 03/11/25 11:42:20 - 0:08:35 - Epoch 167: train_loss=14.420238494873047
INFO - 03/11/25 11:42:22 - 0:08:37 - Epoch 168: train_loss=14.413708686828613
INFO - 03/11/25 11:42:23 - 0:08:38 - Epoch 169: train_loss=14.414615631103516
INFO - 03/11/25 11:42:24 - 0:08:39 - Epoch 170: train_loss=14.41351318359375
INFO - 03/11/25 11:42:26 - 0:08:41 - Epoch 171: train_loss=14.413301467895508
INFO - 03/11/25 11:42:27 - 0:08:42 - Epoch 172: train_loss=14.41122055053711
INFO - 03/11/25 11:42:28 - 0:08:44 - Epoch 173: train_loss=14.41486930847168
INFO - 03/11/25 11:42:30 - 0:08:45 - Epoch 174: train_loss=14.410768508911133
INFO - 03/11/25 11:42:31 - 0:08:46 - Epoch 175: train_loss=14.417279243469238
INFO - 03/11/25 11:42:32 - 0:08:48 - Epoch 176: train_loss=14.414032936096191
INFO - 03/11/25 11:42:34 - 0:08:49 - Epoch 177: train_loss=14.41537094116211
INFO - 03/11/25 11:42:35 - 0:08:50 - Epoch 178: train_loss=14.415070533752441
INFO - 03/11/25 11:42:36 - 0:08:52 - Epoch 179: train_loss=14.412151336669922
INFO - 03/11/25 11:42:38 - 0:08:53 - Epoch 180: train_loss=14.411102294921875
INFO - 03/11/25 11:42:39 - 0:08:54 - Epoch 181: train_loss=14.413361549377441
INFO - 03/11/25 11:42:41 - 0:08:56 - Epoch 182: train_loss=14.410409927368164
INFO - 03/11/25 11:42:42 - 0:08:57 - Epoch 183: train_loss=14.414453506469727
INFO - 03/11/25 11:42:43 - 0:08:58 - Epoch 184: train_loss=14.413688659667969
INFO - 03/11/25 11:42:45 - 0:09:00 - Epoch 185: train_loss=14.408336639404297
INFO - 03/11/25 11:42:46 - 0:09:01 - Epoch 186: train_loss=14.407429695129395
INFO - 03/11/25 11:42:47 - 0:09:03 - Epoch 187: train_loss=14.412757873535156
INFO - 03/11/25 11:42:49 - 0:09:04 - Epoch 188: train_loss=14.41107177734375
INFO - 03/11/25 11:42:50 - 0:09:05 - Epoch 189: train_loss=14.40919303894043
INFO - 03/11/25 11:42:51 - 0:09:07 - Epoch 190: train_loss=14.408571243286133
INFO - 03/11/25 11:42:53 - 0:09:08 - Epoch 191: train_loss=14.40911865234375
INFO - 03/11/25 11:42:54 - 0:09:09 - Epoch 192: train_loss=14.40733528137207
INFO - 03/11/25 11:42:55 - 0:09:11 - Epoch 193: train_loss=14.410148620605469
INFO - 03/11/25 11:42:57 - 0:09:12 - Epoch 194: train_loss=14.408882141113281
INFO - 03/11/25 11:42:58 - 0:09:13 - Epoch 195: train_loss=14.407355308532715
INFO - 03/11/25 11:43:00 - 0:09:15 - Epoch 196: train_loss=14.406460762023926
INFO - 03/11/25 11:43:01 - 0:09:16 - Epoch 197: train_loss=14.407949447631836
INFO - 03/11/25 11:43:02 - 0:09:17 - Epoch 198: train_loss=14.406521797180176
INFO - 03/11/25 11:43:04 - 0:09:19 - Epoch 199: train_loss=14.407282829284668
INFO - 03/11/25 11:43:05 - 0:09:20 - Epoch 200: train_loss=14.406326293945312
INFO - 03/11/25 11:43:05 - 0:09:20 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:43:07 - 0:09:22 - Decoding cost time:  1.860 s
INFO - 03/11/25 11:43:09 - 0:09:25 - Epoch 200: ACC: 0.0, NMI: 0.012870109215791866, F1: 0.0, ARI: -9.118730653302475e-06
INFO - 03/11/25 11:43:09 - 0:09:25 - -------------------------------------------------------------------------
INFO - 03/11/25 11:43:11 - 0:09:26 - Epoch 201: train_loss=14.40600299835205
INFO - 03/11/25 11:43:12 - 0:09:27 - Epoch 202: train_loss=14.404985427856445
INFO - 03/11/25 11:43:14 - 0:09:29 - Epoch 203: train_loss=14.406015396118164
INFO - 03/11/25 11:43:15 - 0:09:30 - Epoch 204: train_loss=14.40478229522705
INFO - 03/11/25 11:43:16 - 0:09:31 - Epoch 205: train_loss=14.40540599822998
INFO - 03/11/25 11:43:18 - 0:09:33 - Epoch 206: train_loss=14.404452323913574
INFO - 03/11/25 11:43:19 - 0:09:34 - Epoch 207: train_loss=14.404300689697266
INFO - 03/11/25 11:43:20 - 0:09:36 - Epoch 208: train_loss=14.403257369995117
INFO - 03/11/25 11:43:22 - 0:09:37 - Epoch 209: train_loss=14.404312133789062
INFO - 03/11/25 11:43:23 - 0:09:38 - Epoch 210: train_loss=14.403100967407227
INFO - 03/11/25 11:43:24 - 0:09:40 - Epoch 211: train_loss=14.403586387634277
INFO - 03/11/25 11:43:26 - 0:09:41 - Epoch 212: train_loss=14.40261173248291
INFO - 03/11/25 11:43:27 - 0:09:42 - Epoch 213: train_loss=14.402886390686035
INFO - 03/11/25 11:43:28 - 0:09:44 - Epoch 214: train_loss=14.40180778503418
INFO - 03/11/25 11:43:30 - 0:09:45 - Epoch 215: train_loss=14.402690887451172
INFO - 03/11/25 11:43:31 - 0:09:46 - Epoch 216: train_loss=14.401525497436523
INFO - 03/11/25 11:43:33 - 0:09:48 - Epoch 217: train_loss=14.402255058288574
INFO - 03/11/25 11:43:34 - 0:09:49 - Epoch 218: train_loss=14.401309967041016
INFO - 03/11/25 11:43:35 - 0:09:50 - Epoch 219: train_loss=14.401341438293457
INFO - 03/11/25 11:43:37 - 0:09:52 - Epoch 220: train_loss=14.400310516357422
INFO - 03/11/25 11:43:38 - 0:09:53 - Epoch 221: train_loss=14.401354789733887
INFO - 03/11/25 11:43:39 - 0:09:55 - Epoch 222: train_loss=14.400221824645996
INFO - 03/11/25 11:43:41 - 0:09:56 - Epoch 223: train_loss=14.400609016418457
INFO - 03/11/25 11:43:42 - 0:09:57 - Epoch 224: train_loss=14.39965534210205
INFO - 03/11/25 11:43:43 - 0:09:59 - Epoch 225: train_loss=14.400042533874512
INFO - 03/11/25 11:43:45 - 0:10:00 - Epoch 226: train_loss=14.39902114868164
INFO - 03/11/25 11:43:46 - 0:10:01 - Epoch 227: train_loss=14.39962387084961
INFO - 03/11/25 11:43:47 - 0:10:03 - Epoch 228: train_loss=14.398468017578125
INFO - 03/11/25 11:43:49 - 0:10:04 - Epoch 229: train_loss=14.399375915527344
INFO - 03/11/25 11:43:50 - 0:10:05 - Epoch 230: train_loss=14.398249626159668
INFO - 03/11/25 11:43:52 - 0:10:07 - Epoch 231: train_loss=14.39873218536377
INFO - 03/11/25 11:43:53 - 0:10:08 - Epoch 232: train_loss=14.397756576538086
INFO - 03/11/25 11:43:54 - 0:10:09 - Epoch 233: train_loss=14.3981351852417
INFO - 03/11/25 11:43:56 - 0:10:11 - Epoch 234: train_loss=14.397164344787598
INFO - 03/11/25 11:43:57 - 0:10:12 - Epoch 235: train_loss=14.397527694702148
INFO - 03/11/25 11:43:58 - 0:10:14 - Epoch 236: train_loss=14.396535873413086
INFO - 03/11/25 11:44:00 - 0:10:15 - Epoch 237: train_loss=14.396987915039062
INFO - 03/11/25 11:44:01 - 0:10:16 - Epoch 238: train_loss=14.396023750305176
INFO - 03/11/25 11:44:02 - 0:10:18 - Epoch 239: train_loss=14.396336555480957
INFO - 03/11/25 11:44:04 - 0:10:19 - Epoch 240: train_loss=14.395431518554688
INFO - 03/11/25 11:44:05 - 0:10:20 - Epoch 241: train_loss=14.395986557006836
INFO - 03/11/25 11:44:06 - 0:10:22 - Epoch 242: train_loss=14.394838333129883
INFO - 03/11/25 11:44:08 - 0:10:23 - Epoch 243: train_loss=14.396491050720215
INFO - 03/11/25 11:44:09 - 0:10:24 - Epoch 244: train_loss=14.395425796508789
INFO - 03/11/25 11:44:11 - 0:10:26 - Epoch 245: train_loss=14.395522117614746
INFO - 03/11/25 11:44:12 - 0:10:27 - Epoch 246: train_loss=14.395004272460938
INFO - 03/11/25 11:44:13 - 0:10:28 - Epoch 247: train_loss=14.39454460144043
INFO - 03/11/25 11:44:15 - 0:10:30 - Epoch 248: train_loss=14.393465042114258
INFO - 03/11/25 11:44:16 - 0:10:31 - Epoch 249: train_loss=14.395748138427734
INFO - 03/11/25 11:44:17 - 0:10:33 - Epoch 250: train_loss=14.395236015319824
INFO - 03/11/25 11:44:19 - 0:10:34 - Epoch 251: train_loss=14.39268684387207
INFO - 03/11/25 11:44:20 - 0:10:35 - Epoch 252: train_loss=14.392457962036133
INFO - 03/11/25 11:44:21 - 0:10:37 - Epoch 253: train_loss=14.394058227539062
INFO - 03/11/25 11:44:23 - 0:10:38 - Epoch 254: train_loss=14.392584800720215
INFO - 03/11/25 11:44:24 - 0:10:39 - Epoch 255: train_loss=14.394731521606445
INFO - 03/11/25 11:44:25 - 0:10:41 - Epoch 256: train_loss=14.394508361816406
INFO - 03/11/25 11:44:27 - 0:10:42 - Epoch 257: train_loss=14.39100456237793
INFO - 03/11/25 11:44:28 - 0:10:43 - Epoch 258: train_loss=14.391678810119629
INFO - 03/11/25 11:44:30 - 0:10:45 - Epoch 259: train_loss=14.391806602478027
INFO - 03/11/25 11:44:31 - 0:10:46 - Epoch 260: train_loss=14.391351699829102
INFO - 03/11/25 11:44:32 - 0:10:47 - Epoch 261: train_loss=14.394102096557617
INFO - 03/11/25 11:44:34 - 0:10:49 - Epoch 262: train_loss=14.391556739807129
INFO - 03/11/25 11:44:35 - 0:10:50 - Epoch 263: train_loss=14.394463539123535
INFO - 03/11/25 11:44:36 - 0:10:51 - Epoch 264: train_loss=14.393019676208496
INFO - 03/11/25 11:44:38 - 0:10:53 - Epoch 265: train_loss=14.391801834106445
INFO - 03/11/25 11:44:39 - 0:10:54 - Epoch 266: train_loss=14.395188331604004
INFO - 03/11/25 11:44:40 - 0:10:56 - Epoch 267: train_loss=14.392561912536621
INFO - 03/11/25 11:44:42 - 0:10:57 - Epoch 268: train_loss=14.394716262817383
INFO - 03/11/25 11:44:43 - 0:10:58 - Epoch 269: train_loss=14.393662452697754
INFO - 03/11/25 11:44:44 - 0:11:00 - Epoch 270: train_loss=14.393691062927246
INFO - 03/11/25 11:44:46 - 0:11:01 - Epoch 271: train_loss=14.390934944152832
INFO - 03/11/25 11:44:47 - 0:11:02 - Epoch 272: train_loss=14.395682334899902
INFO - 03/11/25 11:44:49 - 0:11:04 - Epoch 273: train_loss=14.391014099121094
INFO - 03/11/25 11:44:50 - 0:11:05 - Epoch 274: train_loss=14.39913558959961
INFO - 03/11/25 11:44:51 - 0:11:06 - Epoch 275: train_loss=14.400533676147461
INFO - 03/11/25 11:44:53 - 0:11:08 - Epoch 276: train_loss=14.388790130615234
INFO - 03/11/25 11:44:54 - 0:11:09 - Epoch 277: train_loss=14.401453971862793
INFO - 03/11/25 11:44:55 - 0:11:10 - Epoch 278: train_loss=14.403891563415527
INFO - 03/11/25 11:44:57 - 0:11:12 - Epoch 279: train_loss=14.393986701965332
INFO - 03/11/25 11:44:58 - 0:11:13 - Epoch 280: train_loss=14.398211479187012
INFO - 03/11/25 11:44:59 - 0:11:15 - Epoch 281: train_loss=14.400238037109375
INFO - 03/11/25 11:45:01 - 0:11:16 - Epoch 282: train_loss=14.394342422485352
INFO - 03/11/25 11:45:02 - 0:11:17 - Epoch 283: train_loss=14.39653491973877
INFO - 03/11/25 11:45:03 - 0:11:19 - Epoch 284: train_loss=14.394906997680664
INFO - 03/11/25 11:45:05 - 0:11:20 - Epoch 285: train_loss=14.392608642578125
INFO - 03/11/25 11:45:06 - 0:11:21 - Epoch 286: train_loss=14.395002365112305
INFO - 03/11/25 11:45:08 - 0:11:23 - Epoch 287: train_loss=14.389226913452148
INFO - 03/11/25 11:45:09 - 0:11:24 - Epoch 288: train_loss=14.395393371582031
INFO - 03/11/25 11:45:10 - 0:11:25 - Epoch 289: train_loss=14.396756172180176
INFO - 03/11/25 11:45:12 - 0:11:27 - Epoch 290: train_loss=14.388655662536621
INFO - 03/11/25 11:45:13 - 0:11:28 - Epoch 291: train_loss=14.394397735595703
INFO - 03/11/25 11:45:14 - 0:11:29 - Epoch 292: train_loss=14.396402359008789
INFO - 03/11/25 11:45:16 - 0:11:31 - Epoch 293: train_loss=14.38928508758545
INFO - 03/11/25 11:45:17 - 0:11:32 - Epoch 294: train_loss=14.392826080322266
INFO - 03/11/25 11:45:18 - 0:11:34 - Epoch 295: train_loss=14.394131660461426
INFO - 03/11/25 11:45:20 - 0:11:35 - Epoch 296: train_loss=14.389631271362305
INFO - 03/11/25 11:45:21 - 0:11:36 - Epoch 297: train_loss=14.391029357910156
INFO - 03/11/25 11:45:22 - 0:11:38 - Epoch 298: train_loss=14.389771461486816
INFO - 03/11/25 11:45:24 - 0:11:39 - Epoch 299: train_loss=14.390127182006836
INFO - 03/11/25 11:45:25 - 0:11:40 - Epoch 300: train_loss=14.389641761779785
INFO - 03/11/25 11:45:25 - 0:11:40 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:45:27 - 0:11:42 - Decoding cost time:  1.874 s
INFO - 03/11/25 11:45:30 - 0:11:45 - ------------------Saving best model-------------------
INFO - 03/11/25 11:45:30 - 0:11:45 - Epoch 300: ACC: 0.0, NMI: 0.32671084876622253, F1: 0.0, ARI: 0.16718497081169964
INFO - 03/11/25 11:45:30 - 0:11:45 - -------------------------------------------------------------------------
INFO - 03/11/25 11:45:31 - 0:11:46 - Epoch 301: train_loss=14.385773658752441
INFO - 03/11/25 11:45:32 - 0:11:48 - Epoch 302: train_loss=14.388711929321289
INFO - 03/11/25 11:45:34 - 0:11:49 - Epoch 303: train_loss=14.382083892822266
INFO - 03/11/25 11:45:35 - 0:11:50 - Epoch 304: train_loss=14.39230728149414
INFO - 03/11/25 11:45:37 - 0:11:52 - Epoch 305: train_loss=14.389033317565918
INFO - 03/11/25 11:45:38 - 0:11:53 - Epoch 306: train_loss=14.389105796813965
INFO - 03/11/25 11:45:39 - 0:11:54 - Epoch 307: train_loss=14.389815330505371
INFO - 03/11/25 11:45:41 - 0:11:56 - Epoch 308: train_loss=14.385842323303223
INFO - 03/11/25 11:45:42 - 0:11:57 - Epoch 309: train_loss=14.391129493713379
INFO - 03/11/25 11:45:43 - 0:11:59 - Epoch 310: train_loss=14.387948989868164
INFO - 03/11/25 11:45:45 - 0:12:00 - Epoch 311: train_loss=14.39091968536377
INFO - 03/11/25 11:45:46 - 0:12:01 - Epoch 312: train_loss=14.390048027038574
INFO - 03/11/25 11:45:47 - 0:12:03 - Epoch 313: train_loss=14.389019966125488
INFO - 03/11/25 11:45:49 - 0:12:04 - Epoch 314: train_loss=14.38690185546875
INFO - 03/11/25 11:45:50 - 0:12:05 - Epoch 315: train_loss=14.39087200164795
INFO - 03/11/25 11:45:51 - 0:12:07 - Epoch 316: train_loss=14.386672973632812
INFO - 03/11/25 11:45:53 - 0:12:08 - Epoch 317: train_loss=14.393046379089355
INFO - 03/11/25 11:45:54 - 0:12:09 - Epoch 318: train_loss=14.393041610717773
INFO - 03/11/25 11:45:56 - 0:12:11 - Epoch 319: train_loss=14.384818077087402
INFO - 03/11/25 11:45:57 - 0:12:12 - Epoch 320: train_loss=14.387964248657227
INFO - 03/11/25 11:45:58 - 0:12:13 - Epoch 321: train_loss=14.386306762695312
INFO - 03/11/25 11:46:00 - 0:12:15 - Epoch 322: train_loss=14.385438919067383
INFO - 03/11/25 11:46:01 - 0:12:16 - Epoch 323: train_loss=14.385725021362305
INFO - 03/11/25 11:46:02 - 0:12:18 - Epoch 324: train_loss=14.384806632995605
INFO - 03/11/25 11:46:04 - 0:12:19 - Epoch 325: train_loss=14.382536888122559
INFO - 03/11/25 11:46:05 - 0:12:20 - Epoch 326: train_loss=14.390742301940918
INFO - 03/11/25 11:46:06 - 0:12:22 - Epoch 327: train_loss=14.389445304870605
INFO - 03/11/25 11:46:08 - 0:12:23 - Epoch 328: train_loss=14.38486099243164
INFO - 03/11/25 11:46:09 - 0:12:24 - Epoch 329: train_loss=14.3856201171875
INFO - 03/11/25 11:46:10 - 0:12:26 - Epoch 330: train_loss=14.385008811950684
INFO - 03/11/25 11:46:12 - 0:12:27 - Epoch 331: train_loss=14.38438606262207
INFO - 03/11/25 11:46:13 - 0:12:28 - Epoch 332: train_loss=14.383350372314453
INFO - 03/11/25 11:46:15 - 0:12:30 - Epoch 333: train_loss=14.385790824890137
INFO - 03/11/25 11:46:16 - 0:12:31 - Epoch 334: train_loss=14.382536888122559
INFO - 03/11/25 11:46:17 - 0:12:32 - Epoch 335: train_loss=14.387847900390625
INFO - 03/11/25 11:46:19 - 0:12:34 - Epoch 336: train_loss=14.38588809967041
INFO - 03/11/25 11:46:20 - 0:12:35 - Epoch 337: train_loss=14.386137008666992
INFO - 03/11/25 11:46:21 - 0:12:36 - Epoch 338: train_loss=14.384663581848145
INFO - 03/11/25 11:46:23 - 0:12:38 - Epoch 339: train_loss=14.386456489562988
INFO - 03/11/25 11:46:24 - 0:12:39 - Epoch 340: train_loss=14.38306999206543
INFO - 03/11/25 11:46:25 - 0:12:41 - Epoch 341: train_loss=14.38846206665039
INFO - 03/11/25 11:46:27 - 0:12:42 - Epoch 342: train_loss=14.38557243347168
INFO - 03/11/25 11:46:28 - 0:12:43 - Epoch 343: train_loss=14.387449264526367
INFO - 03/11/25 11:46:29 - 0:12:45 - Epoch 344: train_loss=14.387674331665039
INFO - 03/11/25 11:46:31 - 0:12:46 - Epoch 345: train_loss=14.381890296936035
INFO - 03/11/25 11:46:32 - 0:12:47 - Epoch 346: train_loss=14.381290435791016
INFO - 03/11/25 11:46:34 - 0:12:49 - Epoch 347: train_loss=14.386940956115723
INFO - 03/11/25 11:46:35 - 0:12:50 - Epoch 348: train_loss=14.384588241577148
INFO - 03/11/25 11:46:36 - 0:12:51 - Epoch 349: train_loss=14.38570499420166
INFO - 03/11/25 11:46:38 - 0:12:53 - Epoch 350: train_loss=14.385576248168945
INFO - 03/11/25 11:46:39 - 0:12:54 - Epoch 351: train_loss=14.382659912109375
INFO - 03/11/25 11:46:40 - 0:12:55 - Epoch 352: train_loss=14.381857872009277
INFO - 03/11/25 11:46:42 - 0:12:57 - Epoch 353: train_loss=14.385109901428223
INFO - 03/11/25 11:46:43 - 0:12:58 - Epoch 354: train_loss=14.382742881774902
INFO - 03/11/25 11:46:44 - 0:13:00 - Epoch 355: train_loss=14.385927200317383
INFO - 03/11/25 11:46:46 - 0:13:01 - Epoch 356: train_loss=14.385504722595215
INFO - 03/11/25 11:46:47 - 0:13:02 - Epoch 357: train_loss=14.381697654724121
INFO - 03/11/25 11:46:48 - 0:13:04 - Epoch 358: train_loss=14.381614685058594
INFO - 03/11/25 11:46:50 - 0:13:05 - Epoch 359: train_loss=14.38332748413086
INFO - 03/11/25 11:46:51 - 0:13:06 - Epoch 360: train_loss=14.38092041015625
INFO - 03/11/25 11:46:53 - 0:13:08 - Epoch 361: train_loss=14.385758399963379
INFO - 03/11/25 11:46:54 - 0:13:09 - Epoch 362: train_loss=14.385029792785645
INFO - 03/11/25 11:46:55 - 0:13:10 - Epoch 363: train_loss=14.38082504272461
INFO - 03/11/25 11:46:57 - 0:13:12 - Epoch 364: train_loss=14.381107330322266
INFO - 03/11/25 11:46:58 - 0:13:13 - Epoch 365: train_loss=14.382133483886719
INFO - 03/11/25 11:46:59 - 0:13:14 - Epoch 366: train_loss=14.379878044128418
INFO - 03/11/25 11:47:01 - 0:13:16 - Epoch 367: train_loss=14.385208129882812
INFO - 03/11/25 11:47:02 - 0:13:17 - Epoch 368: train_loss=14.384431838989258
INFO - 03/11/25 11:47:03 - 0:13:19 - Epoch 369: train_loss=14.380027770996094
INFO - 03/11/25 11:47:05 - 0:13:20 - Epoch 370: train_loss=14.379796981811523
INFO - 03/11/25 11:47:06 - 0:13:21 - Epoch 371: train_loss=14.38308048248291
INFO - 03/11/25 11:47:07 - 0:13:23 - Epoch 372: train_loss=14.38113784790039
INFO - 03/11/25 11:47:09 - 0:13:24 - Epoch 373: train_loss=14.383434295654297
INFO - 03/11/25 11:47:10 - 0:13:25 - Epoch 374: train_loss=14.383401870727539
INFO - 03/11/25 11:47:11 - 0:13:27 - Epoch 375: train_loss=14.379149436950684
INFO - 03/11/25 11:47:13 - 0:13:28 - Epoch 376: train_loss=14.377906799316406
INFO - 03/11/25 11:47:14 - 0:13:29 - Epoch 377: train_loss=14.384960174560547
INFO - 03/11/25 11:47:16 - 0:13:31 - Epoch 378: train_loss=14.38410758972168
INFO - 03/11/25 11:47:17 - 0:13:32 - Epoch 379: train_loss=14.378451347351074
INFO - 03/11/25 11:47:18 - 0:13:33 - Epoch 380: train_loss=14.378273010253906
INFO - 03/11/25 11:47:20 - 0:13:35 - Epoch 381: train_loss=14.382411003112793
INFO - 03/11/25 11:47:21 - 0:13:36 - Epoch 382: train_loss=14.380419731140137
INFO - 03/11/25 11:47:22 - 0:13:38 - Epoch 383: train_loss=14.382118225097656
INFO - 03/11/25 11:47:24 - 0:13:39 - Epoch 384: train_loss=14.382041931152344
INFO - 03/11/25 11:47:25 - 0:13:40 - Epoch 385: train_loss=14.378792762756348
INFO - 03/11/25 11:47:26 - 0:13:42 - Epoch 386: train_loss=14.378302574157715
INFO - 03/11/25 11:47:28 - 0:13:43 - Epoch 387: train_loss=14.381226539611816
INFO - 03/11/25 11:47:29 - 0:13:44 - Epoch 388: train_loss=14.379194259643555
INFO - 03/11/25 11:47:30 - 0:13:46 - Epoch 389: train_loss=14.382094383239746
INFO - 03/11/25 11:47:32 - 0:13:47 - Epoch 390: train_loss=14.381864547729492
INFO - 03/11/25 11:47:33 - 0:13:48 - Epoch 391: train_loss=14.378222465515137
INFO - 03/11/25 11:47:35 - 0:13:50 - Epoch 392: train_loss=14.378620147705078
INFO - 03/11/25 11:47:36 - 0:13:51 - Epoch 393: train_loss=14.378944396972656
INFO - 03/11/25 11:47:37 - 0:13:52 - Epoch 394: train_loss=14.377335548400879
INFO - 03/11/25 11:47:39 - 0:13:54 - Epoch 395: train_loss=14.380287170410156
INFO - 03/11/25 11:47:40 - 0:13:55 - Epoch 396: train_loss=14.378194808959961
INFO - 03/11/25 11:47:41 - 0:13:57 - Epoch 397: train_loss=14.381064414978027
INFO - 03/11/25 11:47:43 - 0:13:58 - Epoch 398: train_loss=14.380746841430664
INFO - 03/11/25 11:47:44 - 0:13:59 - Epoch 399: train_loss=14.37748908996582
INFO - 03/11/25 11:47:45 - 0:14:01 - Epoch 400: train_loss=14.378421783447266
INFO - 03/11/25 11:47:45 - 0:14:01 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:47:47 - 0:14:02 - Decoding cost time:  1.891 s
INFO - 03/11/25 11:47:50 - 0:14:05 - Epoch 400: ACC: 0.0, NMI: 0.056202886918891935, F1: 0.0, ARI: 0.002443820870918119
INFO - 03/11/25 11:47:50 - 0:14:05 - -------------------------------------------------------------------------
INFO - 03/11/25 11:47:51 - 0:14:06 - Epoch 401: train_loss=14.376993179321289
INFO - 03/11/25 11:47:53 - 0:14:08 - Epoch 402: train_loss=14.377517700195312
INFO - 03/11/25 11:47:54 - 0:14:09 - Epoch 403: train_loss=14.376787185668945
INFO - 03/11/25 11:47:55 - 0:14:10 - Epoch 404: train_loss=14.376208305358887
INFO - 03/11/25 11:47:57 - 0:14:12 - Epoch 405: train_loss=14.378776550292969
INFO - 03/11/25 11:47:58 - 0:14:13 - Epoch 406: train_loss=14.374855995178223
INFO - 03/11/25 11:47:59 - 0:14:15 - Epoch 407: train_loss=14.384164810180664
INFO - 03/11/25 11:48:01 - 0:14:16 - Epoch 408: train_loss=14.38404655456543
INFO - 03/11/25 11:48:02 - 0:14:17 - Epoch 409: train_loss=14.374009132385254
INFO - 03/11/25 11:48:03 - 0:14:19 - Epoch 410: train_loss=14.382833480834961
INFO - 03/11/25 11:48:05 - 0:14:20 - Epoch 411: train_loss=14.38046932220459
INFO - 03/11/25 11:48:06 - 0:14:21 - Epoch 412: train_loss=14.37866497039795
INFO - 03/11/25 11:48:07 - 0:14:23 - Epoch 413: train_loss=14.379619598388672
INFO - 03/11/25 11:48:09 - 0:14:24 - Epoch 414: train_loss=14.376876831054688
INFO - 03/11/25 11:48:10 - 0:14:25 - Epoch 415: train_loss=14.380716323852539
INFO - 03/11/25 11:48:12 - 0:14:27 - Epoch 416: train_loss=14.378144264221191
INFO - 03/11/25 11:48:13 - 0:14:28 - Epoch 417: train_loss=14.381336212158203
INFO - 03/11/25 11:48:14 - 0:14:29 - Epoch 418: train_loss=14.381299018859863
INFO - 03/11/25 11:48:16 - 0:14:31 - Epoch 419: train_loss=14.37761116027832
INFO - 03/11/25 11:48:17 - 0:14:32 - Epoch 420: train_loss=14.378766059875488
INFO - 03/11/25 11:48:18 - 0:14:34 - Epoch 421: train_loss=14.377787590026855
INFO - 03/11/25 11:48:20 - 0:14:35 - Epoch 422: train_loss=14.378270149230957
INFO - 03/11/25 11:48:21 - 0:14:36 - Epoch 423: train_loss=14.377172470092773
INFO - 03/11/25 11:48:22 - 0:14:38 - Epoch 424: train_loss=14.379151344299316
INFO - 03/11/25 11:48:24 - 0:14:39 - Epoch 425: train_loss=14.378501892089844
INFO - 03/11/25 11:48:25 - 0:14:40 - Epoch 426: train_loss=14.377294540405273
INFO - 03/11/25 11:48:26 - 0:14:42 - Epoch 427: train_loss=14.377689361572266
INFO - 03/11/25 11:48:28 - 0:14:43 - Epoch 428: train_loss=14.375421524047852
INFO - 03/11/25 11:48:29 - 0:14:44 - Epoch 429: train_loss=14.381012916564941
INFO - 03/11/25 11:48:31 - 0:14:46 - Epoch 430: train_loss=14.37873649597168
INFO - 03/11/25 11:48:32 - 0:14:47 - Epoch 431: train_loss=14.379476547241211
INFO - 03/11/25 11:48:33 - 0:14:48 - Epoch 432: train_loss=14.37939167022705
INFO - 03/11/25 11:48:35 - 0:14:50 - Epoch 433: train_loss=14.377363204956055
INFO - 03/11/25 11:48:36 - 0:14:51 - Epoch 434: train_loss=14.37635326385498
INFO - 03/11/25 11:48:37 - 0:14:53 - Epoch 435: train_loss=14.379159927368164
INFO - 03/11/25 11:48:39 - 0:14:54 - Epoch 436: train_loss=14.375913619995117
INFO - 03/11/25 11:48:40 - 0:14:55 - Epoch 437: train_loss=14.381699562072754
INFO - 03/11/25 11:48:41 - 0:14:57 - Epoch 438: train_loss=14.380633354187012
INFO - 03/11/25 11:48:43 - 0:14:58 - Epoch 439: train_loss=14.376669883728027
INFO - 03/11/25 11:48:44 - 0:14:59 - Epoch 440: train_loss=14.376794815063477
INFO - 03/11/25 11:48:45 - 0:15:01 - Epoch 441: train_loss=14.378143310546875
INFO - 03/11/25 11:48:47 - 0:15:02 - Epoch 442: train_loss=14.376221656799316
INFO - 03/11/25 11:48:48 - 0:15:03 - Epoch 443: train_loss=14.379950523376465
INFO - 03/11/25 11:48:50 - 0:15:05 - Epoch 444: train_loss=14.37960433959961
INFO - 03/11/25 11:48:51 - 0:15:06 - Epoch 445: train_loss=14.375019073486328
INFO - 03/11/25 11:48:52 - 0:15:07 - Epoch 446: train_loss=14.374384880065918
INFO - 03/11/25 11:48:54 - 0:15:09 - Epoch 447: train_loss=14.378883361816406
INFO - 03/11/25 11:48:55 - 0:15:10 - Epoch 448: train_loss=14.376726150512695
INFO - 03/11/25 11:48:56 - 0:15:12 - Epoch 449: train_loss=14.378458976745605
INFO - 03/11/25 11:48:58 - 0:15:13 - Epoch 450: train_loss=14.378250122070312
INFO - 03/11/25 11:48:59 - 0:15:14 - Epoch 451: train_loss=14.37533950805664
INFO - 03/11/25 11:49:00 - 0:15:16 - Epoch 452: train_loss=14.374678611755371
INFO - 03/11/25 11:49:02 - 0:15:17 - Epoch 453: train_loss=14.377728462219238
INFO - 03/11/25 11:49:03 - 0:15:18 - Epoch 454: train_loss=14.375739097595215
INFO - 03/11/25 11:49:04 - 0:15:20 - Epoch 455: train_loss=14.378000259399414
INFO - 03/11/25 11:49:06 - 0:15:21 - Epoch 456: train_loss=14.377685546875
INFO - 03/11/25 11:49:07 - 0:15:22 - Epoch 457: train_loss=14.374457359313965
INFO - 03/11/25 11:49:09 - 0:15:24 - Epoch 458: train_loss=14.373836517333984
INFO - 03/11/25 11:49:10 - 0:15:25 - Epoch 459: train_loss=14.377284049987793
INFO - 03/11/25 11:49:11 - 0:15:26 - Epoch 460: train_loss=14.375632286071777
INFO - 03/11/25 11:49:13 - 0:15:28 - Epoch 461: train_loss=14.37649917602539
INFO - 03/11/25 11:49:14 - 0:15:29 - Epoch 462: train_loss=14.376252174377441
INFO - 03/11/25 11:49:15 - 0:15:31 - Epoch 463: train_loss=14.374252319335938
INFO - 03/11/25 11:49:17 - 0:15:32 - Epoch 464: train_loss=14.373255729675293
INFO - 03/11/25 11:49:18 - 0:15:33 - Epoch 465: train_loss=14.376791954040527
INFO - 03/11/25 11:49:19 - 0:15:35 - Epoch 466: train_loss=14.375439643859863
INFO - 03/11/25 11:49:21 - 0:15:36 - Epoch 467: train_loss=14.375088691711426
INFO - 03/11/25 11:49:22 - 0:15:37 - Epoch 468: train_loss=14.374591827392578
INFO - 03/11/25 11:49:23 - 0:15:39 - Epoch 469: train_loss=14.374780654907227
INFO - 03/11/25 11:49:25 - 0:15:40 - Epoch 470: train_loss=14.373764991760254
INFO - 03/11/25 11:49:26 - 0:15:41 - Epoch 471: train_loss=14.375521659851074
INFO - 03/11/25 11:49:28 - 0:15:43 - Epoch 472: train_loss=14.374649047851562
INFO - 03/11/25 11:49:29 - 0:15:44 - Epoch 473: train_loss=14.374327659606934
INFO - 03/11/25 11:49:30 - 0:15:45 - Epoch 474: train_loss=14.373685836791992
INFO - 03/11/25 11:49:32 - 0:15:47 - Epoch 475: train_loss=14.37449836730957
INFO - 03/11/25 11:49:33 - 0:15:48 - Epoch 476: train_loss=14.37339973449707
INFO - 03/11/25 11:49:34 - 0:15:49 - Epoch 477: train_loss=14.374909400939941
INFO - 03/11/25 11:49:36 - 0:15:51 - Epoch 478: train_loss=14.37438678741455
INFO - 03/11/25 11:49:37 - 0:15:52 - Epoch 479: train_loss=14.373017311096191
INFO - 03/11/25 11:49:38 - 0:15:54 - Epoch 480: train_loss=14.372002601623535
INFO - 03/11/25 11:49:40 - 0:15:55 - Epoch 481: train_loss=14.375513076782227
INFO - 03/11/25 11:49:41 - 0:15:56 - Epoch 482: train_loss=14.374917030334473
INFO - 03/11/25 11:49:42 - 0:15:58 - Epoch 483: train_loss=14.37188720703125
INFO - 03/11/25 11:49:44 - 0:15:59 - Epoch 484: train_loss=14.370896339416504
INFO - 03/11/25 11:49:45 - 0:16:00 - Epoch 485: train_loss=14.375967025756836
INFO - 03/11/25 11:49:47 - 0:16:02 - Epoch 486: train_loss=14.37543773651123
INFO - 03/11/25 11:49:48 - 0:16:03 - Epoch 487: train_loss=14.370676040649414
INFO - 03/11/25 11:49:49 - 0:16:04 - Epoch 488: train_loss=14.369892120361328
INFO - 03/11/25 11:49:51 - 0:16:06 - Epoch 489: train_loss=14.375798225402832
INFO - 03/11/25 11:49:52 - 0:16:07 - Epoch 490: train_loss=14.374772071838379
INFO - 03/11/25 11:49:53 - 0:16:08 - Epoch 491: train_loss=14.371068000793457
INFO - 03/11/25 11:49:55 - 0:16:10 - Epoch 492: train_loss=14.370668411254883
INFO - 03/11/25 11:49:56 - 0:16:11 - Epoch 493: train_loss=14.374092102050781
INFO - 03/11/25 11:49:57 - 0:16:13 - Epoch 494: train_loss=14.372889518737793
INFO - 03/11/25 11:49:59 - 0:16:14 - Epoch 495: train_loss=14.37254810333252
INFO - 03/11/25 11:50:00 - 0:16:15 - Epoch 496: train_loss=14.372095108032227
INFO - 03/11/25 11:50:01 - 0:16:17 - Epoch 497: train_loss=14.372440338134766
INFO - 03/11/25 11:50:03 - 0:16:18 - Epoch 498: train_loss=14.371585845947266
INFO - 03/11/25 11:50:04 - 0:16:19 - Epoch 499: train_loss=14.372968673706055
INFO - 03/11/25 11:50:05 - 0:16:21 - Epoch 500: train_loss=14.37215518951416
INFO - 03/11/25 11:50:05 - 0:16:21 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:50:07 - 0:16:23 - Decoding cost time:  1.877 s
INFO - 03/11/25 11:50:10 - 0:16:25 - Epoch 500: ACC: 0.0, NMI: 0.11022363603933612, F1: 0.0, ARI: 0.009571663368454898
INFO - 03/11/25 11:50:10 - 0:16:25 - -------------------------------------------------------------------------
INFO - 03/11/25 11:50:11 - 0:16:27 - Epoch 501: train_loss=14.372289657592773
INFO - 03/11/25 11:50:13 - 0:16:28 - Epoch 502: train_loss=14.371760368347168
INFO - 03/11/25 11:50:14 - 0:16:29 - Epoch 503: train_loss=14.372079849243164
INFO - 03/11/25 11:50:15 - 0:16:31 - Epoch 504: train_loss=14.371095657348633
INFO - 03/11/25 11:50:17 - 0:16:32 - Epoch 505: train_loss=14.372941970825195
INFO - 03/11/25 11:50:18 - 0:16:33 - Epoch 506: train_loss=14.372397422790527
INFO - 03/11/25 11:50:20 - 0:16:35 - Epoch 507: train_loss=14.371088027954102
INFO - 03/11/25 11:50:21 - 0:16:36 - Epoch 508: train_loss=14.370348930358887
INFO - 03/11/25 11:50:22 - 0:16:37 - Epoch 509: train_loss=14.3729248046875
INFO - 03/11/25 11:50:24 - 0:16:39 - Epoch 510: train_loss=14.372121810913086
INFO - 03/11/25 11:50:25 - 0:16:40 - Epoch 511: train_loss=14.371129035949707
INFO - 03/11/25 11:50:26 - 0:16:42 - Epoch 512: train_loss=14.370542526245117
INFO - 03/11/25 11:50:28 - 0:16:43 - Epoch 513: train_loss=14.372281074523926
INFO - 03/11/25 11:50:29 - 0:16:44 - Epoch 514: train_loss=14.371479988098145
INFO - 03/11/25 11:50:30 - 0:16:46 - Epoch 515: train_loss=14.371306419372559
INFO - 03/11/25 11:50:32 - 0:16:47 - Epoch 516: train_loss=14.370631217956543
INFO - 03/11/25 11:50:33 - 0:16:48 - Epoch 517: train_loss=14.371927261352539
INFO - 03/11/25 11:50:34 - 0:16:50 - Epoch 518: train_loss=14.371275901794434
INFO - 03/11/25 11:50:36 - 0:16:51 - Epoch 519: train_loss=14.371021270751953
INFO - 03/11/25 11:50:37 - 0:16:52 - Epoch 520: train_loss=14.370261192321777
INFO - 03/11/25 11:50:39 - 0:16:54 - Epoch 521: train_loss=14.3719482421875
INFO - 03/11/25 11:50:40 - 0:16:55 - Epoch 522: train_loss=14.371308326721191
INFO - 03/11/25 11:50:41 - 0:16:56 - Epoch 523: train_loss=14.370597839355469
INFO - 03/11/25 11:50:43 - 0:16:58 - Epoch 524: train_loss=14.369912147521973
INFO - 03/11/25 11:50:44 - 0:16:59 - Epoch 525: train_loss=14.37183666229248
INFO - 03/11/25 11:50:45 - 0:17:00 - Epoch 526: train_loss=14.37119197845459
INFO - 03/11/25 11:50:47 - 0:17:02 - Epoch 527: train_loss=14.370416641235352
INFO - 03/11/25 11:50:48 - 0:17:03 - Epoch 528: train_loss=14.369773864746094
INFO - 03/11/25 11:50:49 - 0:17:05 - Epoch 529: train_loss=14.371665000915527
INFO - 03/11/25 11:50:51 - 0:17:06 - Epoch 530: train_loss=14.371024131774902
INFO - 03/11/25 11:50:52 - 0:17:07 - Epoch 531: train_loss=14.370139122009277
INFO - 03/11/25 11:50:53 - 0:17:09 - Epoch 532: train_loss=14.369455337524414
INFO - 03/11/25 11:50:55 - 0:17:10 - Epoch 533: train_loss=14.371603965759277
INFO - 03/11/25 11:50:56 - 0:17:11 - Epoch 534: train_loss=14.371031761169434
INFO - 03/11/25 11:50:57 - 0:17:13 - Epoch 535: train_loss=14.369757652282715
INFO - 03/11/25 11:50:59 - 0:17:14 - Epoch 536: train_loss=14.369026184082031
INFO - 03/11/25 11:51:00 - 0:17:15 - Epoch 537: train_loss=14.371781349182129
INFO - 03/11/25 11:51:02 - 0:17:17 - Epoch 538: train_loss=14.371299743652344
INFO - 03/11/25 11:51:03 - 0:17:18 - Epoch 539: train_loss=14.36910343170166
INFO - 03/11/25 11:51:04 - 0:17:19 - Epoch 540: train_loss=14.368302345275879
INFO - 03/11/25 11:51:06 - 0:17:21 - Epoch 541: train_loss=14.372208595275879
INFO - 03/11/25 11:51:07 - 0:17:22 - Epoch 542: train_loss=14.3717679977417
INFO - 03/11/25 11:51:08 - 0:17:24 - Epoch 543: train_loss=14.368289947509766
INFO - 03/11/25 11:51:10 - 0:17:25 - Epoch 544: train_loss=14.367627143859863
INFO - 03/11/25 11:51:11 - 0:17:26 - Epoch 545: train_loss=14.37266731262207
INFO - 03/11/25 11:51:12 - 0:17:28 - Epoch 546: train_loss=14.371996879577637
INFO - 03/11/25 11:51:14 - 0:17:29 - Epoch 547: train_loss=14.36841106414795
INFO - 03/11/25 11:51:15 - 0:17:30 - Epoch 548: train_loss=14.370772361755371
INFO - 03/11/25 11:51:16 - 0:17:32 - Epoch 549: train_loss=14.366445541381836
INFO - 03/11/25 11:51:18 - 0:17:33 - Epoch 550: train_loss=14.367061614990234
INFO - 03/11/25 11:51:19 - 0:17:34 - Epoch 551: train_loss=14.369006156921387
INFO - 03/11/25 11:51:21 - 0:17:36 - Epoch 552: train_loss=14.366118431091309
INFO - 03/11/25 11:51:22 - 0:17:37 - Epoch 553: train_loss=14.374629020690918
INFO - 03/11/25 11:51:23 - 0:17:38 - Epoch 554: train_loss=14.372245788574219
INFO - 03/11/25 11:51:25 - 0:17:40 - Epoch 555: train_loss=14.371837615966797
INFO - 03/11/25 11:51:26 - 0:17:41 - Epoch 556: train_loss=14.371785163879395
INFO - 03/11/25 11:51:27 - 0:17:43 - Epoch 557: train_loss=14.37151050567627
INFO - 03/11/25 11:51:29 - 0:17:44 - Epoch 558: train_loss=14.369484901428223
INFO - 03/11/25 11:51:30 - 0:17:45 - Epoch 559: train_loss=14.371500015258789
INFO - 03/11/25 11:51:31 - 0:17:47 - Epoch 560: train_loss=14.368014335632324
INFO - 03/11/25 11:51:33 - 0:17:48 - Epoch 561: train_loss=14.369298934936523
INFO - 03/11/25 11:51:34 - 0:17:49 - Epoch 562: train_loss=14.370793342590332
INFO - 03/11/25 11:51:35 - 0:17:51 - Epoch 563: train_loss=14.366479873657227
INFO - 03/11/25 11:51:37 - 0:17:52 - Epoch 564: train_loss=14.379907608032227
INFO - 03/11/25 11:51:38 - 0:17:53 - Epoch 565: train_loss=14.381532669067383
INFO - 03/11/25 11:51:40 - 0:17:55 - Epoch 566: train_loss=14.369346618652344
INFO - 03/11/25 11:51:41 - 0:17:56 - Epoch 567: train_loss=14.38338565826416
INFO - 03/11/25 11:51:42 - 0:17:57 - Epoch 568: train_loss=14.3902006149292
INFO - 03/11/25 11:51:44 - 0:17:59 - Epoch 569: train_loss=14.382586479187012
INFO - 03/11/25 11:51:45 - 0:18:00 - Epoch 570: train_loss=14.370265007019043
INFO - 03/11/25 11:51:46 - 0:18:02 - Epoch 571: train_loss=14.378491401672363
INFO - 03/11/25 11:51:48 - 0:18:03 - Epoch 572: train_loss=14.378847122192383
INFO - 03/11/25 11:51:49 - 0:18:04 - Epoch 573: train_loss=14.373831748962402
INFO - 03/11/25 11:51:50 - 0:18:06 - Epoch 574: train_loss=14.373445510864258
INFO - 03/11/25 11:51:52 - 0:18:07 - Epoch 575: train_loss=14.374754905700684
INFO - 03/11/25 11:51:53 - 0:18:08 - Epoch 576: train_loss=14.374696731567383
INFO - 03/11/25 11:51:54 - 0:18:10 - Epoch 577: train_loss=14.36954116821289
INFO - 03/11/25 11:51:56 - 0:18:11 - Epoch 578: train_loss=14.375557899475098
INFO - 03/11/25 11:51:57 - 0:18:12 - Epoch 579: train_loss=14.376439094543457
INFO - 03/11/25 11:51:59 - 0:18:14 - Epoch 580: train_loss=14.365744590759277
INFO - 03/11/25 11:52:00 - 0:18:15 - Epoch 581: train_loss=14.380549430847168
INFO - 03/11/25 11:52:01 - 0:18:16 - Epoch 582: train_loss=14.384049415588379
INFO - 03/11/25 11:52:03 - 0:18:18 - Epoch 583: train_loss=14.375462532043457
INFO - 03/11/25 11:52:04 - 0:18:19 - Epoch 584: train_loss=14.373538970947266
INFO - 03/11/25 11:52:05 - 0:18:21 - Epoch 585: train_loss=14.378037452697754
INFO - 03/11/25 11:52:07 - 0:18:22 - Epoch 586: train_loss=14.372878074645996
INFO - 03/11/25 11:52:08 - 0:18:23 - Epoch 587: train_loss=14.373370170593262
INFO - 03/11/25 11:52:09 - 0:18:25 - Epoch 588: train_loss=14.3750638961792
INFO - 03/11/25 11:52:11 - 0:18:26 - Epoch 589: train_loss=14.370403289794922
INFO - 03/11/25 11:52:12 - 0:18:27 - Epoch 590: train_loss=14.372469902038574
INFO - 03/11/25 11:52:13 - 0:18:29 - Epoch 591: train_loss=14.373209953308105
INFO - 03/11/25 11:52:15 - 0:18:30 - Epoch 592: train_loss=14.369940757751465
INFO - 03/11/25 11:52:16 - 0:18:31 - Epoch 593: train_loss=14.370597839355469
INFO - 03/11/25 11:52:18 - 0:18:33 - Epoch 594: train_loss=14.371715545654297
INFO - 03/11/25 11:52:19 - 0:18:34 - Epoch 595: train_loss=14.367443084716797
INFO - 03/11/25 11:52:20 - 0:18:35 - Epoch 596: train_loss=14.372404098510742
INFO - 03/11/25 11:52:22 - 0:18:37 - Epoch 597: train_loss=14.372172355651855
INFO - 03/11/25 11:52:23 - 0:18:38 - Epoch 598: train_loss=14.36723804473877
INFO - 03/11/25 11:52:24 - 0:18:39 - Epoch 599: train_loss=14.369224548339844
INFO - 03/11/25 11:52:26 - 0:18:41 - Epoch 600: train_loss=14.367969512939453
INFO - 03/11/25 11:52:26 - 0:18:41 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:52:28 - 0:18:43 - Decoding cost time:  1.902 s
INFO - 03/11/25 11:52:30 - 0:18:45 - Epoch 600: ACC: 0.0, NMI: 0.09884581675153159, F1: 0.0, ARI: 0.00784218360210137
INFO - 03/11/25 11:52:30 - 0:18:45 - -------------------------------------------------------------------------
INFO - 03/11/25 11:52:32 - 0:18:47 - Epoch 601: train_loss=14.368156433105469
INFO - 03/11/25 11:52:33 - 0:18:48 - Epoch 602: train_loss=14.367071151733398
INFO - 03/11/25 11:52:34 - 0:18:50 - Epoch 603: train_loss=14.36900806427002
INFO - 03/11/25 11:52:36 - 0:18:51 - Epoch 604: train_loss=14.366841316223145
INFO - 03/11/25 11:52:37 - 0:18:52 - Epoch 605: train_loss=14.371087074279785
INFO - 03/11/25 11:52:38 - 0:18:54 - Epoch 606: train_loss=14.371585845947266
INFO - 03/11/25 11:52:40 - 0:18:55 - Epoch 607: train_loss=14.36512565612793
INFO - 03/11/25 11:52:41 - 0:18:56 - Epoch 608: train_loss=14.369660377502441
INFO - 03/11/25 11:52:42 - 0:18:58 - Epoch 609: train_loss=14.3681001663208
INFO - 03/11/25 11:52:44 - 0:18:59 - Epoch 610: train_loss=14.368294715881348
INFO - 03/11/25 11:52:45 - 0:19:00 - Epoch 611: train_loss=14.36742877960205
INFO - 03/11/25 11:52:47 - 0:19:02 - Epoch 612: train_loss=14.368754386901855
INFO - 03/11/25 11:52:48 - 0:19:03 - Epoch 613: train_loss=14.367650985717773
INFO - 03/11/25 11:52:49 - 0:19:04 - Epoch 614: train_loss=14.36783504486084
INFO - 03/11/25 11:52:51 - 0:19:06 - Epoch 615: train_loss=14.368045806884766
INFO - 03/11/25 11:52:52 - 0:19:07 - Epoch 616: train_loss=14.365924835205078
INFO - 03/11/25 11:52:53 - 0:19:09 - Epoch 617: train_loss=14.36801528930664
INFO - 03/11/25 11:52:55 - 0:19:10 - Epoch 618: train_loss=14.365315437316895
INFO - 03/11/25 11:52:56 - 0:19:11 - Epoch 619: train_loss=14.369872093200684
INFO - 03/11/25 11:52:57 - 0:19:13 - Epoch 620: train_loss=14.36800479888916
INFO - 03/11/25 11:52:59 - 0:19:14 - Epoch 621: train_loss=14.369552612304688
INFO - 03/11/25 11:53:00 - 0:19:15 - Epoch 622: train_loss=14.368324279785156
INFO - 03/11/25 11:53:01 - 0:19:17 - Epoch 623: train_loss=14.368946075439453
INFO - 03/11/25 11:53:03 - 0:19:18 - Epoch 624: train_loss=14.368969917297363
INFO - 03/11/25 11:53:04 - 0:19:19 - Epoch 625: train_loss=14.366633415222168
INFO - 03/11/25 11:53:06 - 0:19:21 - Epoch 626: train_loss=14.366896629333496
INFO - 03/11/25 11:53:07 - 0:19:22 - Epoch 627: train_loss=14.36671257019043
INFO - 03/11/25 11:53:08 - 0:19:23 - Epoch 628: train_loss=14.366381645202637
INFO - 03/11/25 11:53:10 - 0:19:25 - Epoch 629: train_loss=14.36555004119873
INFO - 03/11/25 11:53:11 - 0:19:26 - Epoch 630: train_loss=14.36709976196289
INFO - 03/11/25 11:53:12 - 0:19:28 - Epoch 631: train_loss=14.364629745483398
INFO - 03/11/25 11:53:14 - 0:19:29 - Epoch 632: train_loss=14.367021560668945
INFO - 03/11/25 11:53:15 - 0:19:30 - Epoch 633: train_loss=14.364678382873535
INFO - 03/11/25 11:53:16 - 0:19:32 - Epoch 634: train_loss=14.3674898147583
INFO - 03/11/25 11:53:18 - 0:19:33 - Epoch 635: train_loss=14.36456298828125
INFO - 03/11/25 11:53:19 - 0:19:34 - Epoch 636: train_loss=14.371088981628418
INFO - 03/11/25 11:53:21 - 0:19:36 - Epoch 637: train_loss=14.370887756347656
INFO - 03/11/25 11:53:22 - 0:19:37 - Epoch 638: train_loss=14.364166259765625
INFO - 03/11/25 11:53:23 - 0:19:38 - Epoch 639: train_loss=14.366626739501953
INFO - 03/11/25 11:53:25 - 0:19:40 - Epoch 640: train_loss=14.365181922912598
INFO - 03/11/25 11:53:26 - 0:19:41 - Epoch 641: train_loss=14.364480972290039
INFO - 03/11/25 11:53:27 - 0:19:42 - Epoch 642: train_loss=14.368362426757812
INFO - 03/11/25 11:53:29 - 0:19:44 - Epoch 643: train_loss=14.366640090942383
INFO - 03/11/25 11:53:30 - 0:19:45 - Epoch 644: train_loss=14.368097305297852
INFO - 03/11/25 11:53:31 - 0:19:47 - Epoch 645: train_loss=14.367701530456543
INFO - 03/11/25 11:53:33 - 0:19:48 - Epoch 646: train_loss=14.367003440856934
INFO - 03/11/25 11:53:34 - 0:19:49 - Epoch 647: train_loss=14.3663969039917
INFO - 03/11/25 11:53:35 - 0:19:51 - Epoch 648: train_loss=14.367308616638184
INFO - 03/11/25 11:53:37 - 0:19:52 - Epoch 649: train_loss=14.36672306060791
INFO - 03/11/25 11:53:38 - 0:19:53 - Epoch 650: train_loss=14.366445541381836
INFO - 03/11/25 11:53:40 - 0:19:55 - Epoch 651: train_loss=14.366092681884766
INFO - 03/11/25 11:53:41 - 0:19:56 - Epoch 652: train_loss=14.366347312927246
INFO - 03/11/25 11:53:42 - 0:19:57 - Epoch 653: train_loss=14.365320205688477
INFO - 03/11/25 11:53:44 - 0:19:59 - Epoch 654: train_loss=14.36746597290039
INFO - 03/11/25 11:53:45 - 0:20:00 - Epoch 655: train_loss=14.365938186645508
INFO - 03/11/25 11:53:46 - 0:20:01 - Epoch 656: train_loss=14.368183135986328
INFO - 03/11/25 11:53:48 - 0:20:03 - Epoch 657: train_loss=14.368058204650879
INFO - 03/11/25 11:53:49 - 0:20:04 - Epoch 658: train_loss=14.365250587463379
INFO - 03/11/25 11:53:50 - 0:20:06 - Epoch 659: train_loss=14.365229606628418
INFO - 03/11/25 11:53:52 - 0:20:07 - Epoch 660: train_loss=14.366467475891113
INFO - 03/11/25 11:53:53 - 0:20:08 - Epoch 661: train_loss=14.364834785461426
INFO - 03/11/25 11:53:54 - 0:20:10 - Epoch 662: train_loss=14.368412971496582
INFO - 03/11/25 11:53:56 - 0:20:11 - Epoch 663: train_loss=14.368173599243164
INFO - 03/11/25 11:53:57 - 0:20:12 - Epoch 664: train_loss=14.364532470703125
INFO - 03/11/25 11:53:59 - 0:20:14 - Epoch 665: train_loss=14.364938735961914
INFO - 03/11/25 11:54:00 - 0:20:15 - Epoch 666: train_loss=14.366081237792969
INFO - 03/11/25 11:54:01 - 0:20:16 - Epoch 667: train_loss=14.364131927490234
INFO - 03/11/25 11:54:03 - 0:20:18 - Epoch 668: train_loss=14.369155883789062
INFO - 03/11/25 11:54:04 - 0:20:19 - Epoch 669: train_loss=14.369102478027344
INFO - 03/11/25 11:54:05 - 0:20:20 - Epoch 670: train_loss=14.363091468811035
INFO - 03/11/25 11:54:07 - 0:20:22 - Epoch 671: train_loss=14.363698959350586
INFO - 03/11/25 11:54:08 - 0:20:23 - Epoch 672: train_loss=14.367483139038086
INFO - 03/11/25 11:54:09 - 0:20:25 - Epoch 673: train_loss=14.364535331726074
INFO - 03/11/25 11:54:11 - 0:20:26 - Epoch 674: train_loss=14.370392799377441
INFO - 03/11/25 11:54:12 - 0:20:27 - Epoch 675: train_loss=14.3709135055542
INFO - 03/11/25 11:54:13 - 0:20:29 - Epoch 676: train_loss=14.364400863647461
INFO - 03/11/25 11:54:15 - 0:20:30 - Epoch 677: train_loss=14.370686531066895
INFO - 03/11/25 11:54:16 - 0:20:31 - Epoch 678: train_loss=14.371506690979004
INFO - 03/11/25 11:54:18 - 0:20:33 - Epoch 679: train_loss=14.364108085632324
INFO - 03/11/25 11:54:19 - 0:20:34 - Epoch 680: train_loss=14.373417854309082
INFO - 03/11/25 11:54:20 - 0:20:35 - Epoch 681: train_loss=14.376716613769531
INFO - 03/11/25 11:54:22 - 0:20:37 - Epoch 682: train_loss=14.370680809020996
INFO - 03/11/25 11:54:23 - 0:20:38 - Epoch 683: train_loss=14.367993354797363
INFO - 03/11/25 11:54:24 - 0:20:40 - Epoch 684: train_loss=14.371437072753906
INFO - 03/11/25 11:54:26 - 0:20:41 - Epoch 685: train_loss=14.368158340454102
INFO - 03/11/25 11:54:27 - 0:20:42 - Epoch 686: train_loss=14.368091583251953
INFO - 03/11/25 11:54:28 - 0:20:44 - Epoch 687: train_loss=14.369499206542969
INFO - 03/11/25 11:54:30 - 0:20:45 - Epoch 688: train_loss=14.365486145019531
INFO - 03/11/25 11:54:31 - 0:20:46 - Epoch 689: train_loss=14.369956970214844
INFO - 03/11/25 11:54:32 - 0:20:48 - Epoch 690: train_loss=14.370530128479004
INFO - 03/11/25 11:54:34 - 0:20:49 - Epoch 691: train_loss=14.363125801086426
INFO - 03/11/25 11:54:35 - 0:20:50 - Epoch 692: train_loss=14.370948791503906
INFO - 03/11/25 11:54:37 - 0:20:52 - Epoch 693: train_loss=14.371819496154785
INFO - 03/11/25 11:54:38 - 0:20:53 - Epoch 694: train_loss=14.365476608276367
INFO - 03/11/25 11:54:39 - 0:20:54 - Epoch 695: train_loss=14.370574951171875
INFO - 03/11/25 11:54:41 - 0:20:56 - Epoch 696: train_loss=14.372882843017578
INFO - 03/11/25 11:54:42 - 0:20:57 - Epoch 697: train_loss=14.36789608001709
INFO - 03/11/25 11:54:43 - 0:20:59 - Epoch 698: train_loss=14.368809700012207
INFO - 03/11/25 11:54:45 - 0:21:00 - Epoch 699: train_loss=14.370656967163086
INFO - 03/11/25 11:54:46 - 0:21:01 - Epoch 700: train_loss=14.366658210754395
INFO - 03/11/25 11:54:46 - 0:21:01 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:54:48 - 0:21:03 - Decoding cost time:  1.938 s
INFO - 03/11/25 11:54:50 - 0:21:06 - Epoch 700: ACC: 0.0, NMI: 0.07473276744859073, F1: 0.0, ARI: 0.003959444499262695
INFO - 03/11/25 11:54:50 - 0:21:06 - -------------------------------------------------------------------------
INFO - 03/11/25 11:54:52 - 0:21:07 - Epoch 701: train_loss=14.368969917297363
INFO - 03/11/25 11:54:53 - 0:21:08 - Epoch 702: train_loss=14.369503021240234
INFO - 03/11/25 11:54:55 - 0:21:10 - Epoch 703: train_loss=14.36491584777832
INFO - 03/11/25 11:54:56 - 0:21:11 - Epoch 704: train_loss=14.368904113769531
INFO - 03/11/25 11:54:57 - 0:21:12 - Epoch 705: train_loss=14.36833667755127
INFO - 03/11/25 11:54:59 - 0:21:14 - Epoch 706: train_loss=14.364988327026367
INFO - 03/11/25 11:55:00 - 0:21:15 - Epoch 707: train_loss=14.36670970916748
INFO - 03/11/25 11:55:01 - 0:21:16 - Epoch 708: train_loss=14.364099502563477
INFO - 03/11/25 11:55:03 - 0:21:18 - Epoch 709: train_loss=14.366619110107422
INFO - 03/11/25 11:55:04 - 0:21:19 - Epoch 710: train_loss=14.364325523376465
INFO - 03/11/25 11:55:05 - 0:21:21 - Epoch 711: train_loss=14.367835998535156
INFO - 03/11/25 11:55:07 - 0:21:22 - Epoch 712: train_loss=14.367766380310059
INFO - 03/11/25 11:55:08 - 0:21:23 - Epoch 713: train_loss=14.363374710083008
INFO - 03/11/25 11:55:09 - 0:21:25 - Epoch 714: train_loss=14.365986824035645
INFO - 03/11/25 11:55:11 - 0:21:26 - Epoch 715: train_loss=14.363018989562988
INFO - 03/11/25 11:55:12 - 0:21:27 - Epoch 716: train_loss=14.364174842834473
INFO - 03/11/25 11:55:14 - 0:21:29 - Epoch 717: train_loss=14.364307403564453
INFO - 03/11/25 11:55:15 - 0:21:30 - Epoch 718: train_loss=14.362899780273438
INFO - 03/11/25 11:55:16 - 0:21:31 - Epoch 719: train_loss=14.367984771728516
INFO - 03/11/25 11:55:18 - 0:21:33 - Epoch 720: train_loss=14.365945816040039
INFO - 03/11/25 11:55:19 - 0:21:34 - Epoch 721: train_loss=14.367355346679688
INFO - 03/11/25 11:55:20 - 0:21:36 - Epoch 722: train_loss=14.367203712463379
INFO - 03/11/25 11:55:22 - 0:21:37 - Epoch 723: train_loss=14.36622428894043
INFO - 03/11/25 11:55:23 - 0:21:38 - Epoch 724: train_loss=14.365945816040039
INFO - 03/11/25 11:55:24 - 0:21:40 - Epoch 725: train_loss=14.365894317626953
INFO - 03/11/25 11:55:26 - 0:21:41 - Epoch 726: train_loss=14.365861892700195
INFO - 03/11/25 11:55:27 - 0:21:42 - Epoch 727: train_loss=14.364679336547852
INFO - 03/11/25 11:55:28 - 0:21:44 - Epoch 728: train_loss=14.368075370788574
INFO - 03/11/25 11:55:30 - 0:21:45 - Epoch 729: train_loss=14.367547988891602
INFO - 03/11/25 11:55:31 - 0:21:46 - Epoch 730: train_loss=14.365723609924316
INFO - 03/11/25 11:55:33 - 0:21:48 - Epoch 731: train_loss=14.366077423095703
INFO - 03/11/25 11:55:34 - 0:21:49 - Epoch 732: train_loss=14.3656005859375
INFO - 03/11/25 11:55:35 - 0:21:50 - Epoch 733: train_loss=14.3660306930542
INFO - 03/11/25 11:55:37 - 0:21:52 - Epoch 734: train_loss=14.364761352539062
INFO - 03/11/25 11:55:38 - 0:21:53 - Epoch 735: train_loss=14.368021011352539
INFO - 03/11/25 11:55:39 - 0:21:55 - Epoch 736: train_loss=14.367659568786621
INFO - 03/11/25 11:55:41 - 0:21:56 - Epoch 737: train_loss=14.365426063537598
INFO - 03/11/25 11:55:42 - 0:21:57 - Epoch 738: train_loss=14.366006851196289
INFO - 03/11/25 11:55:43 - 0:21:59 - Epoch 739: train_loss=14.365340232849121
INFO - 03/11/25 11:55:45 - 0:22:00 - Epoch 740: train_loss=14.366262435913086
INFO - 03/11/25 11:55:46 - 0:22:01 - Epoch 741: train_loss=14.365342140197754
INFO - 03/11/25 11:55:47 - 0:22:03 - Epoch 742: train_loss=14.366778373718262
INFO - 03/11/25 11:55:49 - 0:22:04 - Epoch 743: train_loss=14.366231918334961
INFO - 03/11/25 11:55:50 - 0:22:05 - Epoch 744: train_loss=14.365823745727539
INFO - 03/11/25 11:55:52 - 0:22:07 - Epoch 745: train_loss=14.365734100341797
INFO - 03/11/25 11:55:53 - 0:22:08 - Epoch 746: train_loss=14.364911079406738
INFO - 03/11/25 11:55:54 - 0:22:09 - Epoch 747: train_loss=14.367097854614258
INFO - 03/11/25 11:55:56 - 0:22:11 - Epoch 748: train_loss=14.365601539611816
INFO - 03/11/25 11:55:57 - 0:22:12 - Epoch 749: train_loss=14.367770195007324
INFO - 03/11/25 11:55:58 - 0:22:14 - Epoch 750: train_loss=14.367696762084961
INFO - 03/11/25 11:56:00 - 0:22:15 - Epoch 751: train_loss=14.365457534790039
INFO - 03/11/25 11:56:01 - 0:22:16 - Epoch 752: train_loss=14.365445137023926
INFO - 03/11/25 11:56:02 - 0:22:18 - Epoch 753: train_loss=14.366005897521973
INFO - 03/11/25 11:56:04 - 0:22:19 - Epoch 754: train_loss=14.364324569702148
INFO - 03/11/25 11:56:05 - 0:22:20 - Epoch 755: train_loss=14.364465713500977
INFO - 03/11/25 11:56:06 - 0:22:22 - Epoch 756: train_loss=14.366768836975098
INFO - 03/11/25 11:56:08 - 0:22:23 - Epoch 757: train_loss=14.364314079284668
INFO - 03/11/25 11:56:09 - 0:22:24 - Epoch 758: train_loss=14.370172500610352
INFO - 03/11/25 11:56:11 - 0:22:26 - Epoch 759: train_loss=14.370797157287598
INFO - 03/11/25 11:56:12 - 0:22:27 - Epoch 760: train_loss=14.363937377929688
INFO - 03/11/25 11:56:13 - 0:22:28 - Epoch 761: train_loss=14.369280815124512
INFO - 03/11/25 11:56:15 - 0:22:30 - Epoch 762: train_loss=14.369656562805176
INFO - 03/11/25 11:56:16 - 0:22:31 - Epoch 763: train_loss=14.365652084350586
INFO - 03/11/25 11:56:17 - 0:22:33 - Epoch 764: train_loss=14.36789321899414
INFO - 03/11/25 11:56:19 - 0:22:34 - Epoch 765: train_loss=14.369685173034668
INFO - 03/11/25 11:56:20 - 0:22:35 - Epoch 766: train_loss=14.363401412963867
INFO - 03/11/25 11:56:21 - 0:22:37 - Epoch 767: train_loss=14.372852325439453
INFO - 03/11/25 11:56:23 - 0:22:38 - Epoch 768: train_loss=14.376741409301758
INFO - 03/11/25 11:56:24 - 0:22:39 - Epoch 769: train_loss=14.371699333190918
INFO - 03/11/25 11:56:26 - 0:22:41 - Epoch 770: train_loss=14.365612983703613
INFO - 03/11/25 11:56:27 - 0:22:42 - Epoch 771: train_loss=14.370087623596191
INFO - 03/11/25 11:56:28 - 0:22:43 - Epoch 772: train_loss=14.371461868286133
INFO - 03/11/25 11:56:30 - 0:22:45 - Epoch 773: train_loss=14.36601448059082
INFO - 03/11/25 11:56:31 - 0:22:46 - Epoch 774: train_loss=14.369568824768066
INFO - 03/11/25 11:56:32 - 0:22:48 - Epoch 775: train_loss=14.37307357788086
INFO - 03/11/25 11:56:34 - 0:22:49 - Epoch 776: train_loss=14.367755889892578
INFO - 03/11/25 11:56:35 - 0:22:50 - Epoch 777: train_loss=14.367307662963867
INFO - 03/11/25 11:56:36 - 0:22:52 - Epoch 778: train_loss=14.370129585266113
INFO - 03/11/25 11:56:38 - 0:22:53 - Epoch 779: train_loss=14.36756706237793
INFO - 03/11/25 11:56:39 - 0:22:54 - Epoch 780: train_loss=14.365105628967285
INFO - 03/11/25 11:56:40 - 0:22:56 - Epoch 781: train_loss=14.368303298950195
INFO - 03/11/25 11:56:42 - 0:22:57 - Epoch 782: train_loss=14.365606307983398
INFO - 03/11/25 11:56:43 - 0:22:58 - Epoch 783: train_loss=14.368231773376465
INFO - 03/11/25 11:56:45 - 0:23:00 - Epoch 784: train_loss=14.369382858276367
INFO - 03/11/25 11:56:46 - 0:23:01 - Epoch 785: train_loss=14.363725662231445
INFO - 03/11/25 11:56:47 - 0:23:02 - Epoch 786: train_loss=14.368001937866211
INFO - 03/11/25 11:56:49 - 0:23:04 - Epoch 787: train_loss=14.368965148925781
INFO - 03/11/25 11:56:50 - 0:23:05 - Epoch 788: train_loss=14.363757133483887
INFO - 03/11/25 11:56:51 - 0:23:07 - Epoch 789: train_loss=14.370471954345703
INFO - 03/11/25 11:56:53 - 0:23:08 - Epoch 790: train_loss=14.373345375061035
INFO - 03/11/25 11:56:54 - 0:23:09 - Epoch 791: train_loss=14.368171691894531
INFO - 03/11/25 11:56:55 - 0:23:11 - Epoch 792: train_loss=14.366506576538086
INFO - 03/11/25 11:56:57 - 0:23:12 - Epoch 793: train_loss=14.36942195892334
INFO - 03/11/25 11:56:58 - 0:23:13 - Epoch 794: train_loss=14.367786407470703
INFO - 03/11/25 11:56:59 - 0:23:15 - Epoch 795: train_loss=14.363897323608398
INFO - 03/11/25 11:57:01 - 0:23:16 - Epoch 796: train_loss=14.368477821350098
INFO - 03/11/25 11:57:02 - 0:23:17 - Epoch 797: train_loss=14.365653038024902
INFO - 03/11/25 11:57:04 - 0:23:19 - Epoch 798: train_loss=14.36874771118164
INFO - 03/11/25 11:57:05 - 0:23:20 - Epoch 799: train_loss=14.369756698608398
INFO - 03/11/25 11:57:06 - 0:23:21 - Epoch 800: train_loss=14.363828659057617
INFO - 03/11/25 11:57:06 - 0:23:21 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:57:08 - 0:23:23 - Decoding cost time:  1.901 s
INFO - 03/11/25 11:57:11 - 0:23:26 - Epoch 800: ACC: 0.0, NMI: 0.21118919131192138, F1: 0.0, ARI: 0.032989243001147646
INFO - 03/11/25 11:57:11 - 0:23:26 - -------------------------------------------------------------------------
INFO - 03/11/25 11:57:12 - 0:23:27 - Epoch 801: train_loss=14.367517471313477
INFO - 03/11/25 11:57:14 - 0:23:29 - Epoch 802: train_loss=14.367629051208496
INFO - 03/11/25 11:57:15 - 0:23:30 - Epoch 803: train_loss=14.364712715148926
INFO - 03/11/25 11:57:16 - 0:23:31 - Epoch 804: train_loss=14.367758750915527
INFO - 03/11/25 11:57:18 - 0:23:33 - Epoch 805: train_loss=14.368439674377441
INFO - 03/11/25 11:57:19 - 0:23:34 - Epoch 806: train_loss=14.362526893615723
INFO - 03/11/25 11:57:20 - 0:23:36 - Epoch 807: train_loss=14.36838436126709
INFO - 03/11/25 11:57:22 - 0:23:37 - Epoch 808: train_loss=14.368876457214355
INFO - 03/11/25 11:57:23 - 0:23:38 - Epoch 809: train_loss=14.363265991210938
INFO - 03/11/25 11:57:24 - 0:23:40 - Epoch 810: train_loss=14.368834495544434
INFO - 03/11/25 11:57:26 - 0:23:41 - Epoch 811: train_loss=14.37101936340332
INFO - 03/11/25 11:57:27 - 0:23:42 - Epoch 812: train_loss=14.366790771484375
INFO - 03/11/25 11:57:29 - 0:23:44 - Epoch 813: train_loss=14.365876197814941
INFO - 03/11/25 11:57:30 - 0:23:45 - Epoch 814: train_loss=14.36809253692627
INFO - 03/11/25 11:57:31 - 0:23:46 - Epoch 815: train_loss=14.365682601928711
INFO - 03/11/25 11:57:33 - 0:23:48 - Epoch 816: train_loss=14.365715980529785
INFO - 03/11/25 11:57:34 - 0:23:49 - Epoch 817: train_loss=14.365972518920898
INFO - 03/11/25 11:57:35 - 0:23:51 - Epoch 818: train_loss=14.365262031555176
INFO - 03/11/25 11:57:37 - 0:23:52 - Epoch 819: train_loss=14.36501407623291
INFO - 03/11/25 11:57:38 - 0:23:53 - Epoch 820: train_loss=14.364400863647461
INFO - 03/11/25 11:57:39 - 0:23:55 - Epoch 821: train_loss=14.365036964416504
INFO - 03/11/25 11:57:41 - 0:23:56 - Epoch 822: train_loss=14.362757682800293
INFO - 03/11/25 11:57:42 - 0:23:57 - Epoch 823: train_loss=14.364891052246094
INFO - 03/11/25 11:57:43 - 0:23:59 - Epoch 824: train_loss=14.362872123718262
INFO - 03/11/25 11:57:45 - 0:24:00 - Epoch 825: train_loss=14.366332054138184
INFO - 03/11/25 11:57:46 - 0:24:01 - Epoch 826: train_loss=14.366225242614746
INFO - 03/11/25 11:57:48 - 0:24:03 - Epoch 827: train_loss=14.362833023071289
INFO - 03/11/25 11:57:49 - 0:24:04 - Epoch 828: train_loss=14.364274024963379
INFO - 03/11/25 11:57:50 - 0:24:05 - Epoch 829: train_loss=14.363510131835938
INFO - 03/11/25 11:57:52 - 0:24:07 - Epoch 830: train_loss=14.36302375793457
INFO - 03/11/25 11:57:53 - 0:24:08 - Epoch 831: train_loss=14.36462688446045
INFO - 03/11/25 11:57:54 - 0:24:10 - Epoch 832: train_loss=14.363265037536621
INFO - 03/11/25 11:57:56 - 0:24:11 - Epoch 833: train_loss=14.36523151397705
INFO - 03/11/25 11:57:57 - 0:24:12 - Epoch 834: train_loss=14.365120887756348
INFO - 03/11/25 11:57:58 - 0:24:14 - Epoch 835: train_loss=14.363126754760742
INFO - 03/11/25 11:58:00 - 0:24:15 - Epoch 836: train_loss=14.36306095123291
INFO - 03/11/25 11:58:01 - 0:24:16 - Epoch 837: train_loss=14.36440658569336
INFO - 03/11/25 11:58:03 - 0:24:18 - Epoch 838: train_loss=14.363015174865723
INFO - 03/11/25 11:58:04 - 0:24:19 - Epoch 839: train_loss=14.365684509277344
INFO - 03/11/25 11:58:05 - 0:24:20 - Epoch 840: train_loss=14.365437507629395
INFO - 03/11/25 11:58:07 - 0:24:22 - Epoch 841: train_loss=14.363121032714844
INFO - 03/11/25 11:58:08 - 0:24:23 - Epoch 842: train_loss=14.364059448242188
INFO - 03/11/25 11:58:09 - 0:24:24 - Epoch 843: train_loss=14.363832473754883
INFO - 03/11/25 11:58:11 - 0:24:26 - Epoch 844: train_loss=14.362898826599121
INFO - 03/11/25 11:58:12 - 0:24:27 - Epoch 845: train_loss=14.364895820617676
INFO - 03/11/25 11:58:13 - 0:24:29 - Epoch 846: train_loss=14.363759994506836
INFO - 03/11/25 11:58:15 - 0:24:30 - Epoch 847: train_loss=14.364819526672363
INFO - 03/11/25 11:58:16 - 0:24:31 - Epoch 848: train_loss=14.364797592163086
INFO - 03/11/25 11:58:17 - 0:24:33 - Epoch 849: train_loss=14.363088607788086
INFO - 03/11/25 11:58:19 - 0:24:34 - Epoch 850: train_loss=14.363227844238281
INFO - 03/11/25 11:58:20 - 0:24:35 - Epoch 851: train_loss=14.365575790405273
INFO - 03/11/25 11:58:22 - 0:24:37 - Epoch 852: train_loss=14.364238739013672
INFO - 03/11/25 11:58:23 - 0:24:38 - Epoch 853: train_loss=14.365010261535645
INFO - 03/11/25 11:58:24 - 0:24:39 - Epoch 854: train_loss=14.365242004394531
INFO - 03/11/25 11:58:26 - 0:24:41 - Epoch 855: train_loss=14.363064765930176
INFO - 03/11/25 11:58:27 - 0:24:42 - Epoch 856: train_loss=14.368650436401367
INFO - 03/11/25 11:58:28 - 0:24:44 - Epoch 857: train_loss=14.367120742797852
INFO - 03/11/25 11:58:30 - 0:24:45 - Epoch 858: train_loss=14.36649227142334
INFO - 03/11/25 11:58:31 - 0:24:46 - Epoch 859: train_loss=14.366328239440918
INFO - 03/11/25 11:58:32 - 0:24:48 - Epoch 860: train_loss=14.366345405578613
INFO - 03/11/25 11:58:34 - 0:24:49 - Epoch 861: train_loss=14.365001678466797
INFO - 03/11/25 11:58:35 - 0:24:50 - Epoch 862: train_loss=14.368250846862793
INFO - 03/11/25 11:58:36 - 0:24:52 - Epoch 863: train_loss=14.367558479309082
INFO - 03/11/25 11:58:38 - 0:24:53 - Epoch 864: train_loss=14.365235328674316
INFO - 03/11/25 11:58:39 - 0:24:54 - Epoch 865: train_loss=14.364750862121582
INFO - 03/11/25 11:58:41 - 0:24:56 - Epoch 866: train_loss=14.367534637451172
INFO - 03/11/25 11:58:42 - 0:24:57 - Epoch 867: train_loss=14.36652946472168
INFO - 03/11/25 11:58:43 - 0:24:58 - Epoch 868: train_loss=14.36652660369873
INFO - 03/11/25 11:58:45 - 0:25:00 - Epoch 869: train_loss=14.365900993347168
INFO - 03/11/25 11:58:46 - 0:25:01 - Epoch 870: train_loss=14.366862297058105
INFO - 03/11/25 11:58:47 - 0:25:03 - Epoch 871: train_loss=14.366259574890137
INFO - 03/11/25 11:58:49 - 0:25:04 - Epoch 872: train_loss=14.366104125976562
INFO - 03/11/25 11:58:50 - 0:25:05 - Epoch 873: train_loss=14.365411758422852
INFO - 03/11/25 11:58:51 - 0:25:07 - Epoch 874: train_loss=14.366988182067871
INFO - 03/11/25 11:58:53 - 0:25:08 - Epoch 875: train_loss=14.366299629211426
INFO - 03/11/25 11:58:54 - 0:25:09 - Epoch 876: train_loss=14.366189002990723
INFO - 03/11/25 11:58:55 - 0:25:11 - Epoch 877: train_loss=14.365530967712402
INFO - 03/11/25 11:58:57 - 0:25:12 - Epoch 878: train_loss=14.366813659667969
INFO - 03/11/25 11:58:58 - 0:25:13 - Epoch 879: train_loss=14.366214752197266
INFO - 03/11/25 11:59:00 - 0:25:15 - Epoch 880: train_loss=14.365971565246582
INFO - 03/11/25 11:59:01 - 0:25:16 - Epoch 881: train_loss=14.365256309509277
INFO - 03/11/25 11:59:02 - 0:25:17 - Epoch 882: train_loss=14.366974830627441
INFO - 03/11/25 11:59:04 - 0:25:19 - Epoch 883: train_loss=14.36644172668457
INFO - 03/11/25 11:59:05 - 0:25:20 - Epoch 884: train_loss=14.365547180175781
INFO - 03/11/25 11:59:06 - 0:25:22 - Epoch 885: train_loss=14.364805221557617
INFO - 03/11/25 11:59:08 - 0:25:23 - Epoch 886: train_loss=14.36728286743164
INFO - 03/11/25 11:59:09 - 0:25:24 - Epoch 887: train_loss=14.366732597351074
INFO - 03/11/25 11:59:10 - 0:25:26 - Epoch 888: train_loss=14.365145683288574
INFO - 03/11/25 11:59:12 - 0:25:27 - Epoch 889: train_loss=14.364493370056152
INFO - 03/11/25 11:59:13 - 0:25:28 - Epoch 890: train_loss=14.367379188537598
INFO - 03/11/25 11:59:15 - 0:25:30 - Epoch 891: train_loss=14.366806983947754
INFO - 03/11/25 11:59:16 - 0:25:31 - Epoch 892: train_loss=14.364873886108398
INFO - 03/11/25 11:59:17 - 0:25:32 - Epoch 893: train_loss=14.364173889160156
INFO - 03/11/25 11:59:19 - 0:25:34 - Epoch 894: train_loss=14.36760425567627
INFO - 03/11/25 11:59:20 - 0:25:35 - Epoch 895: train_loss=14.367085456848145
INFO - 03/11/25 11:59:21 - 0:25:36 - Epoch 896: train_loss=14.364459991455078
INFO - 03/11/25 11:59:23 - 0:25:38 - Epoch 897: train_loss=14.363781929016113
INFO - 03/11/25 11:59:24 - 0:25:39 - Epoch 898: train_loss=14.367801666259766
INFO - 03/11/25 11:59:25 - 0:25:41 - Epoch 899: train_loss=14.367216110229492
INFO - 03/11/25 11:59:27 - 0:25:42 - Epoch 900: train_loss=14.36423110961914
INFO - 03/11/25 11:59:27 - 0:25:42 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 11:59:29 - 0:25:44 - Decoding cost time:  1.916 s
INFO - 03/11/25 11:59:31 - 0:25:47 - Epoch 900: ACC: 0.0, NMI: 0.20465692801038793, F1: 0.0, ARI: 0.03350099276459878
INFO - 03/11/25 11:59:31 - 0:25:47 - -------------------------------------------------------------------------
INFO - 03/11/25 11:59:33 - 0:25:48 - Epoch 901: train_loss=14.36359691619873
INFO - 03/11/25 11:59:34 - 0:25:49 - Epoch 902: train_loss=14.367859840393066
INFO - 03/11/25 11:59:35 - 0:25:51 - Epoch 903: train_loss=14.36733627319336
INFO - 03/11/25 11:59:37 - 0:25:52 - Epoch 904: train_loss=14.363954544067383
INFO - 03/11/25 11:59:38 - 0:25:53 - Epoch 905: train_loss=14.36331558227539
INFO - 03/11/25 11:59:40 - 0:25:55 - Epoch 906: train_loss=14.368037223815918
INFO - 03/11/25 11:59:41 - 0:25:56 - Epoch 907: train_loss=14.367501258850098
INFO - 03/11/25 11:59:42 - 0:25:57 - Epoch 908: train_loss=14.363710403442383
INFO - 03/11/25 11:59:44 - 0:25:59 - Epoch 909: train_loss=14.36308479309082
INFO - 03/11/25 11:59:45 - 0:26:00 - Epoch 910: train_loss=14.36811637878418
INFO - 03/11/25 11:59:46 - 0:26:01 - Epoch 911: train_loss=14.367592811584473
INFO - 03/11/25 11:59:48 - 0:26:03 - Epoch 912: train_loss=14.363460540771484
INFO - 03/11/25 11:59:49 - 0:26:04 - Epoch 913: train_loss=14.362871170043945
INFO - 03/11/25 11:59:50 - 0:26:06 - Epoch 914: train_loss=14.368234634399414
INFO - 03/11/25 11:59:52 - 0:26:07 - Epoch 915: train_loss=14.36770248413086
INFO - 03/11/25 11:59:53 - 0:26:08 - Epoch 916: train_loss=14.363292694091797
INFO - 03/11/25 11:59:54 - 0:26:10 - Epoch 917: train_loss=14.362788200378418
INFO - 03/11/25 11:59:56 - 0:26:11 - Epoch 918: train_loss=14.367980003356934
INFO - 03/11/25 11:59:57 - 0:26:12 - Epoch 919: train_loss=14.367257118225098
INFO - 03/11/25 11:59:59 - 0:26:14 - Epoch 920: train_loss=14.363798141479492
INFO - 03/11/25 12:00:00 - 0:26:15 - Epoch 921: train_loss=14.3634614944458
INFO - 03/11/25 12:00:01 - 0:26:16 - Epoch 922: train_loss=14.367053985595703
INFO - 03/11/25 12:00:03 - 0:26:18 - Epoch 923: train_loss=14.366158485412598
INFO - 03/11/25 12:00:04 - 0:26:19 - Epoch 924: train_loss=14.364959716796875
INFO - 03/11/25 12:00:05 - 0:26:21 - Epoch 925: train_loss=14.364648818969727
INFO - 03/11/25 12:00:07 - 0:26:22 - Epoch 926: train_loss=14.365917205810547
INFO - 03/11/25 12:00:08 - 0:26:23 - Epoch 927: train_loss=14.365165710449219
INFO - 03/11/25 12:00:09 - 0:26:25 - Epoch 928: train_loss=14.365704536437988
INFO - 03/11/25 12:00:11 - 0:26:26 - Epoch 929: train_loss=14.365278244018555
INFO - 03/11/25 12:00:12 - 0:26:27 - Epoch 930: train_loss=14.365273475646973
INFO - 03/11/25 12:00:13 - 0:26:29 - Epoch 931: train_loss=14.36467170715332
INFO - 03/11/25 12:00:15 - 0:26:30 - Epoch 932: train_loss=14.366002082824707
INFO - 03/11/25 12:00:16 - 0:26:31 - Epoch 933: train_loss=14.365493774414062
INFO - 03/11/25 12:00:18 - 0:26:33 - Epoch 934: train_loss=14.365089416503906
INFO - 03/11/25 12:00:19 - 0:26:34 - Epoch 935: train_loss=14.364591598510742
INFO - 03/11/25 12:00:20 - 0:26:35 - Epoch 936: train_loss=14.365883827209473
INFO - 03/11/25 12:00:22 - 0:26:37 - Epoch 937: train_loss=14.365246772766113
INFO - 03/11/25 12:00:23 - 0:26:38 - Epoch 938: train_loss=14.365318298339844
INFO - 03/11/25 12:00:24 - 0:26:40 - Epoch 939: train_loss=14.3648681640625
INFO - 03/11/25 12:00:26 - 0:26:41 - Epoch 940: train_loss=14.365429878234863
INFO - 03/11/25 12:00:27 - 0:26:42 - Epoch 941: train_loss=14.364741325378418
INFO - 03/11/25 12:00:28 - 0:26:44 - Epoch 942: train_loss=14.365800857543945
INFO - 03/11/25 12:00:30 - 0:26:45 - Epoch 943: train_loss=14.36541748046875
INFO - 03/11/25 12:00:31 - 0:26:46 - Epoch 944: train_loss=14.364737510681152
INFO - 03/11/25 12:00:32 - 0:26:48 - Epoch 945: train_loss=14.364007949829102
INFO - 03/11/25 12:00:34 - 0:26:49 - Epoch 946: train_loss=14.366484642028809
INFO - 03/11/25 12:00:35 - 0:26:50 - Epoch 947: train_loss=14.366118431091309
INFO - 03/11/25 12:00:37 - 0:26:52 - Epoch 948: train_loss=14.363974571228027
INFO - 03/11/25 12:00:38 - 0:26:53 - Epoch 949: train_loss=14.363292694091797
INFO - 03/11/25 12:00:39 - 0:26:54 - Epoch 950: train_loss=14.367085456848145
INFO - 03/11/25 12:00:41 - 0:26:56 - Epoch 951: train_loss=14.366705894470215
INFO - 03/11/25 12:00:42 - 0:26:57 - Epoch 952: train_loss=14.363322257995605
INFO - 03/11/25 12:00:43 - 0:26:59 - Epoch 953: train_loss=14.362717628479004
INFO - 03/11/25 12:00:45 - 0:27:00 - Epoch 954: train_loss=14.36745548248291
INFO - 03/11/25 12:00:46 - 0:27:01 - Epoch 955: train_loss=14.366962432861328
INFO - 03/11/25 12:00:47 - 0:27:03 - Epoch 956: train_loss=14.363072395324707
INFO - 03/11/25 12:00:49 - 0:27:04 - Epoch 957: train_loss=14.362606048583984
INFO - 03/11/25 12:00:50 - 0:27:05 - Epoch 958: train_loss=14.367276191711426
INFO - 03/11/25 12:00:51 - 0:27:07 - Epoch 959: train_loss=14.366618156433105
INFO - 03/11/25 12:00:53 - 0:27:08 - Epoch 960: train_loss=14.363548278808594
INFO - 03/11/25 12:00:54 - 0:27:09 - Epoch 961: train_loss=14.36325454711914
INFO - 03/11/25 12:00:56 - 0:27:11 - Epoch 962: train_loss=14.366300582885742
INFO - 03/11/25 12:00:57 - 0:27:12 - Epoch 963: train_loss=14.365438461303711
INFO - 03/11/25 12:00:58 - 0:27:13 - Epoch 964: train_loss=14.364812850952148
INFO - 03/11/25 12:01:00 - 0:27:15 - Epoch 965: train_loss=14.364529609680176
INFO - 03/11/25 12:01:01 - 0:27:16 - Epoch 966: train_loss=14.365153312683105
INFO - 03/11/25 12:01:02 - 0:27:18 - Epoch 967: train_loss=14.36446475982666
INFO - 03/11/25 12:01:04 - 0:27:19 - Epoch 968: train_loss=14.365482330322266
INFO - 03/11/25 12:01:05 - 0:27:20 - Epoch 969: train_loss=14.364971160888672
INFO - 03/11/25 12:01:06 - 0:27:22 - Epoch 970: train_loss=14.364904403686523
INFO - 03/11/25 12:01:08 - 0:27:23 - Epoch 971: train_loss=14.364415168762207
INFO - 03/11/25 12:01:09 - 0:27:24 - Epoch 972: train_loss=14.365301132202148
INFO - 03/11/25 12:01:10 - 0:27:26 - Epoch 973: train_loss=14.364665985107422
INFO - 03/11/25 12:01:12 - 0:27:27 - Epoch 974: train_loss=14.365211486816406
INFO - 03/11/25 12:01:13 - 0:27:28 - Epoch 975: train_loss=14.36481761932373
INFO - 03/11/25 12:01:15 - 0:27:30 - Epoch 976: train_loss=14.364745140075684
INFO - 03/11/25 12:01:16 - 0:27:31 - Epoch 977: train_loss=14.364099502563477
INFO - 03/11/25 12:01:17 - 0:27:32 - Epoch 978: train_loss=14.365734100341797
INFO - 03/11/25 12:01:19 - 0:27:34 - Epoch 979: train_loss=14.36535358428955
INFO - 03/11/25 12:01:20 - 0:27:35 - Epoch 980: train_loss=14.364126205444336
INFO - 03/11/25 12:01:21 - 0:27:37 - Epoch 981: train_loss=14.36349105834961
INFO - 03/11/25 12:01:23 - 0:27:38 - Epoch 982: train_loss=14.366278648376465
INFO - 03/11/25 12:01:24 - 0:27:39 - Epoch 983: train_loss=14.365904808044434
INFO - 03/11/25 12:01:25 - 0:27:41 - Epoch 984: train_loss=14.363567352294922
INFO - 03/11/25 12:01:27 - 0:27:42 - Epoch 985: train_loss=14.362931251525879
INFO - 03/11/25 12:01:28 - 0:27:43 - Epoch 986: train_loss=14.366629600524902
INFO - 03/11/25 12:01:29 - 0:27:45 - Epoch 987: train_loss=14.366203308105469
INFO - 03/11/25 12:01:31 - 0:27:46 - Epoch 988: train_loss=14.363248825073242
INFO - 03/11/25 12:01:32 - 0:27:47 - Epoch 989: train_loss=14.362674713134766
INFO - 03/11/25 12:01:34 - 0:27:49 - Epoch 990: train_loss=14.366844177246094
INFO - 03/11/25 12:01:35 - 0:27:50 - Epoch 991: train_loss=14.36639404296875
INFO - 03/11/25 12:01:36 - 0:27:51 - Epoch 992: train_loss=14.36299991607666
INFO - 03/11/25 12:01:38 - 0:27:53 - Epoch 993: train_loss=14.362447738647461
INFO - 03/11/25 12:01:39 - 0:27:54 - Epoch 994: train_loss=14.366925239562988
INFO - 03/11/25 12:01:40 - 0:27:56 - Epoch 995: train_loss=14.366439819335938
INFO - 03/11/25 12:01:42 - 0:27:57 - Epoch 996: train_loss=14.362966537475586
INFO - 03/11/25 12:01:43 - 0:27:58 - Epoch 997: train_loss=14.362469673156738
INFO - 03/11/25 12:01:44 - 0:28:00 - Epoch 998: train_loss=14.366840362548828
INFO - 03/11/25 12:01:46 - 0:28:01 - Epoch 999: train_loss=14.36630630493164
INFO - 03/11/25 12:01:47 - 0:28:02 - Epoch 1000: train_loss=14.362998008728027
INFO - 03/11/25 12:01:47 - 0:28:02 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:01:49 - 0:28:04 - Decoding cost time:  1.917 s
INFO - 03/11/25 12:01:51 - 0:28:07 - Epoch 1000: ACC: 0.0, NMI: 0.17815094932608483, F1: 0.0, ARI: 0.031394186864137426
INFO - 03/11/25 12:01:51 - 0:28:07 - -------------------------------------------------------------------------
INFO - 03/11/25 12:01:53 - 0:28:08 - Epoch 1001: train_loss=14.362471580505371
INFO - 03/11/25 12:01:54 - 0:28:09 - Epoch 1002: train_loss=14.366870880126953
INFO - 03/11/25 12:01:56 - 0:28:11 - Epoch 1003: train_loss=14.366471290588379
INFO - 03/11/25 12:01:57 - 0:28:12 - Epoch 1004: train_loss=14.362751007080078
INFO - 03/11/25 12:01:58 - 0:28:13 - Epoch 1005: train_loss=14.36219596862793
INFO - 03/11/25 12:02:00 - 0:28:15 - Epoch 1006: train_loss=14.36705207824707
INFO - 03/11/25 12:02:01 - 0:28:16 - Epoch 1007: train_loss=14.366568565368652
INFO - 03/11/25 12:02:02 - 0:28:18 - Epoch 1008: train_loss=14.362760543823242
INFO - 03/11/25 12:02:04 - 0:28:19 - Epoch 1009: train_loss=14.362335205078125
INFO - 03/11/25 12:02:05 - 0:28:20 - Epoch 1010: train_loss=14.366796493530273
INFO - 03/11/25 12:02:06 - 0:28:22 - Epoch 1011: train_loss=14.366247177124023
INFO - 03/11/25 12:02:08 - 0:28:23 - Epoch 1012: train_loss=14.363066673278809
INFO - 03/11/25 12:02:09 - 0:28:24 - Epoch 1013: train_loss=14.36268138885498
INFO - 03/11/25 12:02:11 - 0:28:26 - Epoch 1014: train_loss=14.36637020111084
INFO - 03/11/25 12:02:12 - 0:28:27 - Epoch 1015: train_loss=14.365741729736328
INFO - 03/11/25 12:02:13 - 0:28:28 - Epoch 1016: train_loss=14.363551139831543
INFO - 03/11/25 12:02:15 - 0:28:30 - Epoch 1017: train_loss=14.363126754760742
INFO - 03/11/25 12:02:16 - 0:28:31 - Epoch 1018: train_loss=14.365950584411621
INFO - 03/11/25 12:02:17 - 0:28:33 - Epoch 1019: train_loss=14.36543083190918
INFO - 03/11/25 12:02:19 - 0:28:34 - Epoch 1020: train_loss=14.363813400268555
INFO - 03/11/25 12:02:20 - 0:28:35 - Epoch 1021: train_loss=14.363404273986816
INFO - 03/11/25 12:02:21 - 0:28:37 - Epoch 1022: train_loss=14.365750312805176
INFO - 03/11/25 12:02:23 - 0:28:38 - Epoch 1023: train_loss=14.365220069885254
INFO - 03/11/25 12:02:24 - 0:28:39 - Epoch 1024: train_loss=14.363945007324219
INFO - 03/11/25 12:02:25 - 0:28:41 - Epoch 1025: train_loss=14.363478660583496
INFO - 03/11/25 12:02:27 - 0:28:42 - Epoch 1026: train_loss=14.365683555603027
INFO - 03/11/25 12:02:28 - 0:28:43 - Epoch 1027: train_loss=14.365278244018555
INFO - 03/11/25 12:02:30 - 0:28:45 - Epoch 1028: train_loss=14.36374568939209
INFO - 03/11/25 12:02:31 - 0:28:46 - Epoch 1029: train_loss=14.363112449645996
INFO - 03/11/25 12:02:32 - 0:28:47 - Epoch 1030: train_loss=14.365964889526367
INFO - 03/11/25 12:02:34 - 0:28:49 - Epoch 1031: train_loss=14.365594863891602
INFO - 03/11/25 12:02:35 - 0:28:50 - Epoch 1032: train_loss=14.363242149353027
INFO - 03/11/25 12:02:36 - 0:28:52 - Epoch 1033: train_loss=14.36263656616211
INFO - 03/11/25 12:02:38 - 0:28:53 - Epoch 1034: train_loss=14.366338729858398
INFO - 03/11/25 12:02:39 - 0:28:54 - Epoch 1035: train_loss=14.365927696228027
INFO - 03/11/25 12:02:40 - 0:28:56 - Epoch 1036: train_loss=14.362897872924805
INFO - 03/11/25 12:02:42 - 0:28:57 - Epoch 1037: train_loss=14.362339973449707
INFO - 03/11/25 12:02:43 - 0:28:58 - Epoch 1038: train_loss=14.366572380065918
INFO - 03/11/25 12:02:44 - 0:29:00 - Epoch 1039: train_loss=14.366199493408203
INFO - 03/11/25 12:02:46 - 0:29:01 - Epoch 1040: train_loss=14.362517356872559
INFO - 03/11/25 12:02:47 - 0:29:02 - Epoch 1041: train_loss=14.361868858337402
INFO - 03/11/25 12:02:49 - 0:29:04 - Epoch 1042: train_loss=14.36701774597168
INFO - 03/11/25 12:02:50 - 0:29:05 - Epoch 1043: train_loss=14.366701126098633
INFO - 03/11/25 12:02:51 - 0:29:06 - Epoch 1044: train_loss=14.36192512512207
INFO - 03/11/25 12:02:53 - 0:29:08 - Epoch 1045: train_loss=14.361578941345215
INFO - 03/11/25 12:02:54 - 0:29:09 - Epoch 1046: train_loss=14.366907119750977
INFO - 03/11/25 12:02:55 - 0:29:11 - Epoch 1047: train_loss=14.365939140319824
INFO - 03/11/25 12:02:57 - 0:29:12 - Epoch 1048: train_loss=14.363637924194336
INFO - 03/11/25 12:02:58 - 0:29:13 - Epoch 1049: train_loss=14.364301681518555
INFO - 03/11/25 12:02:59 - 0:29:15 - Epoch 1050: train_loss=14.363116264343262
INFO - 03/11/25 12:03:01 - 0:29:16 - Epoch 1051: train_loss=14.363179206848145
INFO - 03/11/25 12:03:02 - 0:29:17 - Epoch 1052: train_loss=14.365307807922363
INFO - 03/11/25 12:03:03 - 0:29:19 - Epoch 1053: train_loss=14.362874031066895
INFO - 03/11/25 12:03:05 - 0:29:20 - Epoch 1054: train_loss=14.369059562683105
INFO - 03/11/25 12:03:06 - 0:29:21 - Epoch 1055: train_loss=14.369768142700195
INFO - 03/11/25 12:03:08 - 0:29:23 - Epoch 1056: train_loss=14.363310813903809
INFO - 03/11/25 12:03:09 - 0:29:24 - Epoch 1057: train_loss=14.369580268859863
INFO - 03/11/25 12:03:10 - 0:29:25 - Epoch 1058: train_loss=14.371465682983398
INFO - 03/11/25 12:03:12 - 0:29:27 - Epoch 1059: train_loss=14.365850448608398
INFO - 03/11/25 12:03:13 - 0:29:28 - Epoch 1060: train_loss=14.367789268493652
INFO - 03/11/25 12:03:14 - 0:29:30 - Epoch 1061: train_loss=14.371220588684082
INFO - 03/11/25 12:03:16 - 0:29:31 - Epoch 1062: train_loss=14.366662979125977
INFO - 03/11/25 12:03:17 - 0:29:32 - Epoch 1063: train_loss=14.365866661071777
INFO - 03/11/25 12:03:18 - 0:29:34 - Epoch 1064: train_loss=14.368682861328125
INFO - 03/11/25 12:03:20 - 0:29:35 - Epoch 1065: train_loss=14.363088607788086
INFO - 03/11/25 12:03:21 - 0:29:36 - Epoch 1066: train_loss=14.37009334564209
INFO - 03/11/25 12:03:23 - 0:29:38 - Epoch 1067: train_loss=14.373662948608398
INFO - 03/11/25 12:03:24 - 0:29:39 - Epoch 1068: train_loss=14.368906021118164
INFO - 03/11/25 12:03:25 - 0:29:40 - Epoch 1069: train_loss=14.363713264465332
INFO - 03/11/25 12:03:27 - 0:29:42 - Epoch 1070: train_loss=14.367256164550781
INFO - 03/11/25 12:03:28 - 0:29:43 - Epoch 1071: train_loss=14.363785743713379
INFO - 03/11/25 12:03:29 - 0:29:44 - Epoch 1072: train_loss=14.367881774902344
INFO - 03/11/25 12:03:31 - 0:29:46 - Epoch 1073: train_loss=14.369712829589844
INFO - 03/11/25 12:03:32 - 0:29:47 - Epoch 1074: train_loss=14.363500595092773
INFO - 03/11/25 12:03:33 - 0:29:49 - Epoch 1075: train_loss=14.369973182678223
INFO - 03/11/25 12:03:35 - 0:29:50 - Epoch 1076: train_loss=14.373519897460938
INFO - 03/11/25 12:03:36 - 0:29:51 - Epoch 1077: train_loss=14.369160652160645
INFO - 03/11/25 12:03:37 - 0:29:53 - Epoch 1078: train_loss=14.363724708557129
INFO - 03/11/25 12:03:39 - 0:29:54 - Epoch 1079: train_loss=14.36801528930664
INFO - 03/11/25 12:03:40 - 0:29:55 - Epoch 1080: train_loss=14.366226196289062
INFO - 03/11/25 12:03:42 - 0:29:57 - Epoch 1081: train_loss=14.36446475982666
INFO - 03/11/25 12:03:43 - 0:29:58 - Epoch 1082: train_loss=14.36527156829834
INFO - 03/11/25 12:03:44 - 0:29:59 - Epoch 1083: train_loss=14.363479614257812
INFO - 03/11/25 12:03:46 - 0:30:01 - Epoch 1084: train_loss=14.36361312866211
INFO - 03/11/25 12:03:47 - 0:30:02 - Epoch 1085: train_loss=14.363454818725586
INFO - 03/11/25 12:03:48 - 0:30:03 - Epoch 1086: train_loss=14.362859725952148
INFO - 03/11/25 12:03:50 - 0:30:05 - Epoch 1087: train_loss=14.365436553955078
INFO - 03/11/25 12:03:51 - 0:30:06 - Epoch 1088: train_loss=14.363402366638184
INFO - 03/11/25 12:03:52 - 0:30:08 - Epoch 1089: train_loss=14.36724853515625
INFO - 03/11/25 12:03:54 - 0:30:09 - Epoch 1090: train_loss=14.367350578308105
INFO - 03/11/25 12:03:55 - 0:30:10 - Epoch 1091: train_loss=14.363548278808594
INFO - 03/11/25 12:03:56 - 0:30:12 - Epoch 1092: train_loss=14.3663911819458
INFO - 03/11/25 12:03:58 - 0:30:13 - Epoch 1093: train_loss=14.365819931030273
INFO - 03/11/25 12:03:59 - 0:30:14 - Epoch 1094: train_loss=14.363219261169434
INFO - 03/11/25 12:04:01 - 0:30:16 - Epoch 1095: train_loss=14.362991333007812
INFO - 03/11/25 12:04:02 - 0:30:17 - Epoch 1096: train_loss=14.365464210510254
INFO - 03/11/25 12:04:03 - 0:30:18 - Epoch 1097: train_loss=14.364631652832031
INFO - 03/11/25 12:04:05 - 0:30:20 - Epoch 1098: train_loss=14.364232063293457
INFO - 03/11/25 12:04:06 - 0:30:21 - Epoch 1099: train_loss=14.364413261413574
INFO - 03/11/25 12:04:07 - 0:30:23 - Epoch 1100: train_loss=14.362980842590332
INFO - 03/11/25 12:04:07 - 0:30:23 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:04:09 - 0:30:24 - Decoding cost time:  1.912 s
INFO - 03/11/25 12:04:12 - 0:30:27 - Epoch 1100: ACC: 0.0, NMI: 0.2511403812395195, F1: 0.0, ARI: 0.04457641976651268
INFO - 03/11/25 12:04:12 - 0:30:27 - -------------------------------------------------------------------------
INFO - 03/11/25 12:04:13 - 0:30:28 - Epoch 1101: train_loss=14.36512279510498
INFO - 03/11/25 12:04:15 - 0:30:30 - Epoch 1102: train_loss=14.36175537109375
INFO - 03/11/25 12:04:16 - 0:30:31 - Epoch 1103: train_loss=14.370245933532715
INFO - 03/11/25 12:04:17 - 0:30:33 - Epoch 1104: train_loss=14.370360374450684
INFO - 03/11/25 12:04:19 - 0:30:34 - Epoch 1105: train_loss=14.361610412597656
INFO - 03/11/25 12:04:20 - 0:30:35 - Epoch 1106: train_loss=14.370786666870117
INFO - 03/11/25 12:04:21 - 0:30:37 - Epoch 1107: train_loss=14.371912002563477
INFO - 03/11/25 12:04:23 - 0:30:38 - Epoch 1108: train_loss=14.366496086120605
INFO - 03/11/25 12:04:24 - 0:30:39 - Epoch 1109: train_loss=14.367982864379883
INFO - 03/11/25 12:04:26 - 0:30:41 - Epoch 1110: train_loss=14.368582725524902
INFO - 03/11/25 12:04:27 - 0:30:42 - Epoch 1111: train_loss=14.367932319641113
INFO - 03/11/25 12:04:28 - 0:30:43 - Epoch 1112: train_loss=14.36701774597168
INFO - 03/11/25 12:04:30 - 0:30:45 - Epoch 1113: train_loss=14.364511489868164
INFO - 03/11/25 12:04:31 - 0:30:46 - Epoch 1114: train_loss=14.367218017578125
INFO - 03/11/25 12:04:32 - 0:30:47 - Epoch 1115: train_loss=14.362974166870117
INFO - 03/11/25 12:04:34 - 0:30:49 - Epoch 1116: train_loss=14.370318412780762
INFO - 03/11/25 12:04:35 - 0:30:50 - Epoch 1117: train_loss=14.37250804901123
INFO - 03/11/25 12:04:36 - 0:30:52 - Epoch 1118: train_loss=14.365830421447754
INFO - 03/11/25 12:04:38 - 0:30:53 - Epoch 1119: train_loss=14.369695663452148
INFO - 03/11/25 12:04:39 - 0:30:54 - Epoch 1120: train_loss=14.372682571411133
INFO - 03/11/25 12:04:40 - 0:30:56 - Epoch 1121: train_loss=14.367975234985352
INFO - 03/11/25 12:04:42 - 0:30:57 - Epoch 1122: train_loss=14.36862564086914
INFO - 03/11/25 12:04:43 - 0:30:58 - Epoch 1123: train_loss=14.369828224182129
INFO - 03/11/25 12:04:45 - 0:31:00 - Epoch 1124: train_loss=14.366722106933594
INFO - 03/11/25 12:04:46 - 0:31:01 - Epoch 1125: train_loss=14.369101524353027
INFO - 03/11/25 12:04:47 - 0:31:02 - Epoch 1126: train_loss=14.367897987365723
INFO - 03/11/25 12:04:49 - 0:31:04 - Epoch 1127: train_loss=14.366280555725098
INFO - 03/11/25 12:04:50 - 0:31:05 - Epoch 1128: train_loss=14.367588996887207
INFO - 03/11/25 12:04:51 - 0:31:06 - Epoch 1129: train_loss=14.362092018127441
INFO - 03/11/25 12:04:53 - 0:31:08 - Epoch 1130: train_loss=14.368597984313965
INFO - 03/11/25 12:04:54 - 0:31:09 - Epoch 1131: train_loss=14.368437767028809
INFO - 03/11/25 12:04:55 - 0:31:11 - Epoch 1132: train_loss=14.362776756286621
INFO - 03/11/25 12:04:57 - 0:31:12 - Epoch 1133: train_loss=14.36741828918457
INFO - 03/11/25 12:04:58 - 0:31:13 - Epoch 1134: train_loss=14.367571830749512
INFO - 03/11/25 12:04:59 - 0:31:15 - Epoch 1135: train_loss=14.363468170166016
INFO - 03/11/25 12:05:01 - 0:31:16 - Epoch 1136: train_loss=14.367006301879883
INFO - 03/11/25 12:05:02 - 0:31:17 - Epoch 1137: train_loss=14.366839408874512
INFO - 03/11/25 12:05:04 - 0:31:19 - Epoch 1138: train_loss=14.364437103271484
INFO - 03/11/25 12:05:05 - 0:31:20 - Epoch 1139: train_loss=14.365191459655762
INFO - 03/11/25 12:05:06 - 0:31:21 - Epoch 1140: train_loss=14.363763809204102
INFO - 03/11/25 12:05:08 - 0:31:23 - Epoch 1141: train_loss=14.36410140991211
INFO - 03/11/25 12:05:09 - 0:31:24 - Epoch 1142: train_loss=14.362725257873535
INFO - 03/11/25 12:05:10 - 0:31:26 - Epoch 1143: train_loss=14.362598419189453
INFO - 03/11/25 12:05:12 - 0:31:27 - Epoch 1144: train_loss=14.362889289855957
INFO - 03/11/25 12:05:13 - 0:31:28 - Epoch 1145: train_loss=14.361856460571289
INFO - 03/11/25 12:05:14 - 0:31:30 - Epoch 1146: train_loss=14.362273216247559
INFO - 03/11/25 12:05:16 - 0:31:31 - Epoch 1147: train_loss=14.364191055297852
INFO - 03/11/25 12:05:17 - 0:31:32 - Epoch 1148: train_loss=14.362221717834473
INFO - 03/11/25 12:05:18 - 0:31:34 - Epoch 1149: train_loss=14.365781784057617
INFO - 03/11/25 12:05:20 - 0:31:35 - Epoch 1150: train_loss=14.36450481414795
INFO - 03/11/25 12:05:21 - 0:31:36 - Epoch 1151: train_loss=14.365723609924316
INFO - 03/11/25 12:05:23 - 0:31:38 - Epoch 1152: train_loss=14.364580154418945
INFO - 03/11/25 12:05:24 - 0:31:39 - Epoch 1153: train_loss=14.365533828735352
INFO - 03/11/25 12:05:25 - 0:31:40 - Epoch 1154: train_loss=14.365511894226074
INFO - 03/11/25 12:05:27 - 0:31:42 - Epoch 1155: train_loss=14.363722801208496
INFO - 03/11/25 12:05:28 - 0:31:43 - Epoch 1156: train_loss=14.363441467285156
INFO - 03/11/25 12:05:29 - 0:31:45 - Epoch 1157: train_loss=14.36453628540039
INFO - 03/11/25 12:05:31 - 0:31:46 - Epoch 1158: train_loss=14.363466262817383
INFO - 03/11/25 12:05:32 - 0:31:47 - Epoch 1159: train_loss=14.364745140075684
INFO - 03/11/25 12:05:33 - 0:31:49 - Epoch 1160: train_loss=14.3637113571167
INFO - 03/11/25 12:05:35 - 0:31:50 - Epoch 1161: train_loss=14.365009307861328
INFO - 03/11/25 12:05:36 - 0:31:51 - Epoch 1162: train_loss=14.363884925842285
INFO - 03/11/25 12:05:37 - 0:31:53 - Epoch 1163: train_loss=14.365100860595703
INFO - 03/11/25 12:05:39 - 0:31:54 - Epoch 1164: train_loss=14.364766120910645
INFO - 03/11/25 12:05:40 - 0:31:55 - Epoch 1165: train_loss=14.363991737365723
INFO - 03/11/25 12:05:42 - 0:31:57 - Epoch 1166: train_loss=14.363755226135254
INFO - 03/11/25 12:05:43 - 0:31:58 - Epoch 1167: train_loss=14.364147186279297
INFO - 03/11/25 12:05:44 - 0:31:59 - Epoch 1168: train_loss=14.363497734069824
INFO - 03/11/25 12:05:46 - 0:32:01 - Epoch 1169: train_loss=14.36400318145752
INFO - 03/11/25 12:05:47 - 0:32:02 - Epoch 1170: train_loss=14.363147735595703
INFO - 03/11/25 12:05:48 - 0:32:04 - Epoch 1171: train_loss=14.364273071289062
INFO - 03/11/25 12:05:50 - 0:32:05 - Epoch 1172: train_loss=14.363202095031738
INFO - 03/11/25 12:05:51 - 0:32:06 - Epoch 1173: train_loss=14.364692687988281
INFO - 03/11/25 12:05:52 - 0:32:08 - Epoch 1174: train_loss=14.364036560058594
INFO - 03/11/25 12:05:54 - 0:32:09 - Epoch 1175: train_loss=14.364141464233398
INFO - 03/11/25 12:05:55 - 0:32:10 - Epoch 1176: train_loss=14.363871574401855
INFO - 03/11/25 12:05:56 - 0:32:12 - Epoch 1177: train_loss=14.363804817199707
INFO - 03/11/25 12:05:58 - 0:32:13 - Epoch 1178: train_loss=14.363314628601074
INFO - 03/11/25 12:05:59 - 0:32:14 - Epoch 1179: train_loss=14.364269256591797
INFO - 03/11/25 12:06:01 - 0:32:16 - Epoch 1180: train_loss=14.363628387451172
INFO - 03/11/25 12:06:02 - 0:32:17 - Epoch 1181: train_loss=14.364273071289062
INFO - 03/11/25 12:06:03 - 0:32:18 - Epoch 1182: train_loss=14.363729476928711
INFO - 03/11/25 12:06:05 - 0:32:20 - Epoch 1183: train_loss=14.364201545715332
INFO - 03/11/25 12:06:06 - 0:32:21 - Epoch 1184: train_loss=14.363871574401855
INFO - 03/11/25 12:06:07 - 0:32:23 - Epoch 1185: train_loss=14.36385440826416
INFO - 03/11/25 12:06:09 - 0:32:24 - Epoch 1186: train_loss=14.363225936889648
INFO - 03/11/25 12:06:10 - 0:32:25 - Epoch 1187: train_loss=14.364751815795898
INFO - 03/11/25 12:06:11 - 0:32:27 - Epoch 1188: train_loss=14.364415168762207
INFO - 03/11/25 12:06:13 - 0:32:28 - Epoch 1189: train_loss=14.363324165344238
INFO - 03/11/25 12:06:14 - 0:32:29 - Epoch 1190: train_loss=14.362752914428711
INFO - 03/11/25 12:06:15 - 0:32:31 - Epoch 1191: train_loss=14.365203857421875
INFO - 03/11/25 12:06:17 - 0:32:32 - Epoch 1192: train_loss=14.364863395690918
INFO - 03/11/25 12:06:18 - 0:32:33 - Epoch 1193: train_loss=14.362780570983887
INFO - 03/11/25 12:06:20 - 0:32:35 - Epoch 1194: train_loss=14.362347602844238
INFO - 03/11/25 12:06:21 - 0:32:36 - Epoch 1195: train_loss=14.365195274353027
INFO - 03/11/25 12:06:22 - 0:32:37 - Epoch 1196: train_loss=14.364668846130371
INFO - 03/11/25 12:06:24 - 0:32:39 - Epoch 1197: train_loss=14.362956047058105
INFO - 03/11/25 12:06:25 - 0:32:40 - Epoch 1198: train_loss=14.362520217895508
INFO - 03/11/25 12:06:26 - 0:32:42 - Epoch 1199: train_loss=14.364909172058105
INFO - 03/11/25 12:06:28 - 0:32:43 - Epoch 1200: train_loss=14.364493370056152
INFO - 03/11/25 12:06:28 - 0:32:43 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:06:30 - 0:32:45 - Decoding cost time:  1.918 s
INFO - 03/11/25 12:06:32 - 0:32:47 - Epoch 1200: ACC: 0.0, NMI: 0.1273663001359471, F1: 0.0, ARI: 0.01777070062951339
INFO - 03/11/25 12:06:32 - 0:32:47 - -------------------------------------------------------------------------
INFO - 03/11/25 12:06:34 - 0:32:49 - Epoch 1201: train_loss=14.362975120544434
INFO - 03/11/25 12:06:35 - 0:32:50 - Epoch 1202: train_loss=14.36246395111084
INFO - 03/11/25 12:06:36 - 0:32:52 - Epoch 1203: train_loss=14.364974975585938
INFO - 03/11/25 12:06:38 - 0:32:53 - Epoch 1204: train_loss=14.364563941955566
INFO - 03/11/25 12:06:39 - 0:32:54 - Epoch 1205: train_loss=14.3626708984375
INFO - 03/11/25 12:06:40 - 0:32:56 - Epoch 1206: train_loss=14.36215591430664
INFO - 03/11/25 12:06:42 - 0:32:57 - Epoch 1207: train_loss=14.365212440490723
INFO - 03/11/25 12:06:43 - 0:32:58 - Epoch 1208: train_loss=14.364930152893066
INFO - 03/11/25 12:06:45 - 0:33:00 - Epoch 1209: train_loss=14.362128257751465
INFO - 03/11/25 12:06:46 - 0:33:01 - Epoch 1210: train_loss=14.361572265625
INFO - 03/11/25 12:06:47 - 0:33:02 - Epoch 1211: train_loss=14.36560344696045
INFO - 03/11/25 12:06:49 - 0:33:04 - Epoch 1212: train_loss=14.365174293518066
INFO - 03/11/25 12:06:50 - 0:33:05 - Epoch 1213: train_loss=14.361958503723145
INFO - 03/11/25 12:06:51 - 0:33:07 - Epoch 1214: train_loss=14.361654281616211
INFO - 03/11/25 12:06:53 - 0:33:08 - Epoch 1215: train_loss=14.3652982711792
INFO - 03/11/25 12:06:54 - 0:33:09 - Epoch 1216: train_loss=14.364673614501953
INFO - 03/11/25 12:06:55 - 0:33:11 - Epoch 1217: train_loss=14.362710952758789
INFO - 03/11/25 12:06:57 - 0:33:12 - Epoch 1218: train_loss=14.362628936767578
INFO - 03/11/25 12:06:58 - 0:33:13 - Epoch 1219: train_loss=14.364025115966797
INFO - 03/11/25 12:07:00 - 0:33:15 - Epoch 1220: train_loss=14.363204002380371
INFO - 03/11/25 12:07:01 - 0:33:16 - Epoch 1221: train_loss=14.364236831665039
INFO - 03/11/25 12:07:02 - 0:33:17 - Epoch 1222: train_loss=14.364047050476074
INFO - 03/11/25 12:07:04 - 0:33:19 - Epoch 1223: train_loss=14.363043785095215
INFO - 03/11/25 12:07:05 - 0:33:20 - Epoch 1224: train_loss=14.362608909606934
INFO - 03/11/25 12:07:06 - 0:33:21 - Epoch 1225: train_loss=14.364779472351074
INFO - 03/11/25 12:07:08 - 0:33:23 - Epoch 1226: train_loss=14.364452362060547
INFO - 03/11/25 12:07:09 - 0:33:24 - Epoch 1227: train_loss=14.363336563110352
INFO - 03/11/25 12:07:10 - 0:33:26 - Epoch 1228: train_loss=14.363260269165039
INFO - 03/11/25 12:07:12 - 0:33:27 - Epoch 1229: train_loss=14.364521980285645
INFO - 03/11/25 12:07:13 - 0:33:28 - Epoch 1230: train_loss=14.364079475402832
INFO - 03/11/25 12:07:14 - 0:33:30 - Epoch 1231: train_loss=14.364175796508789
INFO - 03/11/25 12:07:16 - 0:33:31 - Epoch 1232: train_loss=14.364014625549316
INFO - 03/11/25 12:07:17 - 0:33:32 - Epoch 1233: train_loss=14.363286018371582
INFO - 03/11/25 12:07:19 - 0:33:34 - Epoch 1234: train_loss=14.362466812133789
INFO - 03/11/25 12:07:20 - 0:33:35 - Epoch 1235: train_loss=14.364791870117188
INFO - 03/11/25 12:07:21 - 0:33:36 - Epoch 1236: train_loss=14.364635467529297
INFO - 03/11/25 12:07:23 - 0:33:38 - Epoch 1237: train_loss=14.36198616027832
INFO - 03/11/25 12:07:24 - 0:33:39 - Epoch 1238: train_loss=14.361344337463379
INFO - 03/11/25 12:07:25 - 0:33:40 - Epoch 1239: train_loss=14.365728378295898
INFO - 03/11/25 12:07:27 - 0:33:42 - Epoch 1240: train_loss=14.365509033203125
INFO - 03/11/25 12:07:28 - 0:33:43 - Epoch 1241: train_loss=14.361294746398926
INFO - 03/11/25 12:07:29 - 0:33:45 - Epoch 1242: train_loss=14.36237907409668
INFO - 03/11/25 12:07:31 - 0:33:46 - Epoch 1243: train_loss=14.363723754882812
INFO - 03/11/25 12:07:32 - 0:33:47 - Epoch 1244: train_loss=14.361289978027344
INFO - 03/11/25 12:07:33 - 0:33:49 - Epoch 1245: train_loss=14.367337226867676
INFO - 03/11/25 12:07:35 - 0:33:50 - Epoch 1246: train_loss=14.367531776428223
INFO - 03/11/25 12:07:36 - 0:33:51 - Epoch 1247: train_loss=14.362106323242188
INFO - 03/11/25 12:07:38 - 0:33:53 - Epoch 1248: train_loss=14.367711067199707
INFO - 03/11/25 12:07:39 - 0:33:54 - Epoch 1249: train_loss=14.369346618652344
INFO - 03/11/25 12:07:40 - 0:33:55 - Epoch 1250: train_loss=14.364201545715332
INFO - 03/11/25 12:07:42 - 0:33:57 - Epoch 1251: train_loss=14.367084503173828
INFO - 03/11/25 12:07:43 - 0:33:58 - Epoch 1252: train_loss=14.370221138000488
INFO - 03/11/25 12:07:44 - 0:33:59 - Epoch 1253: train_loss=14.36618709564209
INFO - 03/11/25 12:07:46 - 0:34:01 - Epoch 1254: train_loss=14.364352226257324
INFO - 03/11/25 12:07:47 - 0:34:02 - Epoch 1255: train_loss=14.366842269897461
INFO - 03/11/25 12:07:48 - 0:34:04 - Epoch 1256: train_loss=14.363795280456543
INFO - 03/11/25 12:07:50 - 0:34:05 - Epoch 1257: train_loss=14.36543083190918
INFO - 03/11/25 12:07:51 - 0:34:06 - Epoch 1258: train_loss=14.366602897644043
INFO - 03/11/25 12:07:52 - 0:34:08 - Epoch 1259: train_loss=14.362492561340332
INFO - 03/11/25 12:07:54 - 0:34:09 - Epoch 1260: train_loss=14.366168022155762
INFO - 03/11/25 12:07:55 - 0:34:10 - Epoch 1261: train_loss=14.367044448852539
INFO - 03/11/25 12:07:57 - 0:34:12 - Epoch 1262: train_loss=14.36357593536377
INFO - 03/11/25 12:07:58 - 0:34:13 - Epoch 1263: train_loss=14.365167617797852
INFO - 03/11/25 12:07:59 - 0:34:14 - Epoch 1264: train_loss=14.366470336914062
INFO - 03/11/25 12:08:01 - 0:34:16 - Epoch 1265: train_loss=14.363800048828125
INFO - 03/11/25 12:08:02 - 0:34:17 - Epoch 1266: train_loss=14.364307403564453
INFO - 03/11/25 12:08:03 - 0:34:18 - Epoch 1267: train_loss=14.36561393737793
INFO - 03/11/25 12:08:05 - 0:34:20 - Epoch 1268: train_loss=14.363011360168457
INFO - 03/11/25 12:08:06 - 0:34:21 - Epoch 1269: train_loss=14.364845275878906
INFO - 03/11/25 12:08:07 - 0:34:23 - Epoch 1270: train_loss=14.366229057312012
INFO - 03/11/25 12:08:09 - 0:34:24 - Epoch 1271: train_loss=14.361960411071777
INFO - 03/11/25 12:08:10 - 0:34:25 - Epoch 1272: train_loss=14.367409706115723
INFO - 03/11/25 12:08:11 - 0:34:27 - Epoch 1273: train_loss=14.370012283325195
INFO - 03/11/25 12:08:13 - 0:34:28 - Epoch 1274: train_loss=14.367071151733398
INFO - 03/11/25 12:08:14 - 0:34:29 - Epoch 1275: train_loss=14.362585067749023
INFO - 03/11/25 12:08:16 - 0:34:31 - Epoch 1276: train_loss=14.366067886352539
INFO - 03/11/25 12:08:17 - 0:34:32 - Epoch 1277: train_loss=14.366226196289062
INFO - 03/11/25 12:08:18 - 0:34:33 - Epoch 1278: train_loss=14.361950874328613
INFO - 03/11/25 12:08:20 - 0:34:35 - Epoch 1279: train_loss=14.367681503295898
INFO - 03/11/25 12:08:21 - 0:34:36 - Epoch 1280: train_loss=14.370206832885742
INFO - 03/11/25 12:08:22 - 0:34:37 - Epoch 1281: train_loss=14.3670015335083
INFO - 03/11/25 12:08:24 - 0:34:39 - Epoch 1282: train_loss=14.362802505493164
INFO - 03/11/25 12:08:25 - 0:34:40 - Epoch 1283: train_loss=14.365666389465332
INFO - 03/11/25 12:08:26 - 0:34:42 - Epoch 1284: train_loss=14.366340637207031
INFO - 03/11/25 12:08:28 - 0:34:43 - Epoch 1285: train_loss=14.363656044006348
INFO - 03/11/25 12:08:29 - 0:34:44 - Epoch 1286: train_loss=14.363625526428223
INFO - 03/11/25 12:08:30 - 0:34:46 - Epoch 1287: train_loss=14.36530876159668
INFO - 03/11/25 12:08:32 - 0:34:47 - Epoch 1288: train_loss=14.362613677978516
INFO - 03/11/25 12:08:33 - 0:34:48 - Epoch 1289: train_loss=14.364727020263672
INFO - 03/11/25 12:08:35 - 0:34:50 - Epoch 1290: train_loss=14.365815162658691
INFO - 03/11/25 12:08:36 - 0:34:51 - Epoch 1291: train_loss=14.363727569580078
INFO - 03/11/25 12:08:37 - 0:34:52 - Epoch 1292: train_loss=14.362992286682129
INFO - 03/11/25 12:08:39 - 0:34:54 - Epoch 1293: train_loss=14.364298820495605
INFO - 03/11/25 12:08:40 - 0:34:55 - Epoch 1294: train_loss=14.363191604614258
INFO - 03/11/25 12:08:41 - 0:34:56 - Epoch 1295: train_loss=14.36280345916748
INFO - 03/11/25 12:08:43 - 0:34:58 - Epoch 1296: train_loss=14.36346435546875
INFO - 03/11/25 12:08:44 - 0:34:59 - Epoch 1297: train_loss=14.361814498901367
INFO - 03/11/25 12:08:45 - 0:35:01 - Epoch 1298: train_loss=14.362736701965332
INFO - 03/11/25 12:08:47 - 0:35:02 - Epoch 1299: train_loss=14.362933158874512
INFO - 03/11/25 12:08:48 - 0:35:03 - Epoch 1300: train_loss=14.362069129943848
INFO - 03/11/25 12:08:48 - 0:35:03 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:08:50 - 0:35:05 - Decoding cost time:  1.943 s
INFO - 03/11/25 12:08:52 - 0:35:08 - Epoch 1300: ACC: 0.0, NMI: 0.18896602207116556, F1: 0.0, ARI: 0.0325682197819006
INFO - 03/11/25 12:08:52 - 0:35:08 - -------------------------------------------------------------------------
INFO - 03/11/25 12:08:54 - 0:35:09 - Epoch 1301: train_loss=14.361678123474121
INFO - 03/11/25 12:08:55 - 0:35:10 - Epoch 1302: train_loss=14.363078117370605
INFO - 03/11/25 12:08:57 - 0:35:12 - Epoch 1303: train_loss=14.362494468688965
INFO - 03/11/25 12:08:58 - 0:35:13 - Epoch 1304: train_loss=14.36248779296875
INFO - 03/11/25 12:08:59 - 0:35:15 - Epoch 1305: train_loss=14.362666130065918
INFO - 03/11/25 12:09:01 - 0:35:16 - Epoch 1306: train_loss=14.362007141113281
INFO - 03/11/25 12:09:02 - 0:35:17 - Epoch 1307: train_loss=14.363844871520996
INFO - 03/11/25 12:09:03 - 0:35:19 - Epoch 1308: train_loss=14.361605644226074
INFO - 03/11/25 12:09:05 - 0:35:20 - Epoch 1309: train_loss=14.362051963806152
INFO - 03/11/25 12:09:06 - 0:35:21 - Epoch 1310: train_loss=14.36295223236084
INFO - 03/11/25 12:09:07 - 0:35:23 - Epoch 1311: train_loss=14.361639022827148
INFO - 03/11/25 12:09:09 - 0:35:24 - Epoch 1312: train_loss=14.36193561553955
INFO - 03/11/25 12:09:10 - 0:35:25 - Epoch 1313: train_loss=14.362585067749023
INFO - 03/11/25 12:09:12 - 0:35:27 - Epoch 1314: train_loss=14.361385345458984
INFO - 03/11/25 12:09:13 - 0:35:28 - Epoch 1315: train_loss=14.362778663635254
INFO - 03/11/25 12:09:14 - 0:35:29 - Epoch 1316: train_loss=14.361486434936523
INFO - 03/11/25 12:09:16 - 0:35:31 - Epoch 1317: train_loss=14.361800193786621
INFO - 03/11/25 12:09:17 - 0:35:32 - Epoch 1318: train_loss=14.362218856811523
INFO - 03/11/25 12:09:18 - 0:35:34 - Epoch 1319: train_loss=14.361924171447754
INFO - 03/11/25 12:09:20 - 0:35:35 - Epoch 1320: train_loss=14.362258911132812
INFO - 03/11/25 12:09:21 - 0:35:36 - Epoch 1321: train_loss=14.36127758026123
INFO - 03/11/25 12:09:22 - 0:35:38 - Epoch 1322: train_loss=14.363301277160645
INFO - 03/11/25 12:09:24 - 0:35:39 - Epoch 1323: train_loss=14.360830307006836
INFO - 03/11/25 12:09:25 - 0:35:40 - Epoch 1324: train_loss=14.363200187683105
INFO - 03/11/25 12:09:26 - 0:35:42 - Epoch 1325: train_loss=14.362452507019043
INFO - 03/11/25 12:09:28 - 0:35:43 - Epoch 1326: train_loss=14.361390113830566
INFO - 03/11/25 12:09:29 - 0:35:44 - Epoch 1327: train_loss=14.36376953125
INFO - 03/11/25 12:09:31 - 0:35:46 - Epoch 1328: train_loss=14.362550735473633
INFO - 03/11/25 12:09:32 - 0:35:47 - Epoch 1329: train_loss=14.363822937011719
INFO - 03/11/25 12:09:33 - 0:35:48 - Epoch 1330: train_loss=14.36291790008545
INFO - 03/11/25 12:09:35 - 0:35:50 - Epoch 1331: train_loss=14.364275932312012
INFO - 03/11/25 12:09:36 - 0:35:51 - Epoch 1332: train_loss=14.363760948181152
INFO - 03/11/25 12:09:37 - 0:35:53 - Epoch 1333: train_loss=14.363059997558594
INFO - 03/11/25 12:09:39 - 0:35:54 - Epoch 1334: train_loss=14.362828254699707
INFO - 03/11/25 12:09:40 - 0:35:55 - Epoch 1335: train_loss=14.363571166992188
INFO - 03/11/25 12:09:41 - 0:35:57 - Epoch 1336: train_loss=14.362865447998047
INFO - 03/11/25 12:09:43 - 0:35:58 - Epoch 1337: train_loss=14.363859176635742
INFO - 03/11/25 12:09:44 - 0:35:59 - Epoch 1338: train_loss=14.363542556762695
INFO - 03/11/25 12:09:45 - 0:36:01 - Epoch 1339: train_loss=14.363105773925781
INFO - 03/11/25 12:09:47 - 0:36:02 - Epoch 1340: train_loss=14.362723350524902
INFO - 03/11/25 12:09:48 - 0:36:03 - Epoch 1341: train_loss=14.363774299621582
INFO - 03/11/25 12:09:50 - 0:36:05 - Epoch 1342: train_loss=14.363198280334473
INFO - 03/11/25 12:09:51 - 0:36:06 - Epoch 1343: train_loss=14.363473892211914
INFO - 03/11/25 12:09:52 - 0:36:07 - Epoch 1344: train_loss=14.363237380981445
INFO - 03/11/25 12:09:54 - 0:36:09 - Epoch 1345: train_loss=14.363105773925781
INFO - 03/11/25 12:09:55 - 0:36:10 - Epoch 1346: train_loss=14.362566947937012
INFO - 03/11/25 12:09:56 - 0:36:12 - Epoch 1347: train_loss=14.363994598388672
INFO - 03/11/25 12:09:58 - 0:36:13 - Epoch 1348: train_loss=14.363624572753906
INFO - 03/11/25 12:09:59 - 0:36:14 - Epoch 1349: train_loss=14.362852096557617
INFO - 03/11/25 12:10:00 - 0:36:16 - Epoch 1350: train_loss=14.362394332885742
INFO - 03/11/25 12:10:02 - 0:36:17 - Epoch 1351: train_loss=14.36413288116455
INFO - 03/11/25 12:10:03 - 0:36:18 - Epoch 1352: train_loss=14.363838195800781
INFO - 03/11/25 12:10:04 - 0:36:20 - Epoch 1353: train_loss=14.362631797790527
INFO - 03/11/25 12:10:06 - 0:36:21 - Epoch 1354: train_loss=14.362154006958008
INFO - 03/11/25 12:10:07 - 0:36:22 - Epoch 1355: train_loss=14.364521980285645
INFO - 03/11/25 12:10:09 - 0:36:24 - Epoch 1356: train_loss=14.364273071289062
INFO - 03/11/25 12:10:10 - 0:36:25 - Epoch 1357: train_loss=14.362343788146973
INFO - 03/11/25 12:10:11 - 0:36:26 - Epoch 1358: train_loss=14.36210823059082
INFO - 03/11/25 12:10:13 - 0:36:28 - Epoch 1359: train_loss=14.364497184753418
INFO - 03/11/25 12:10:14 - 0:36:29 - Epoch 1360: train_loss=14.36390495300293
INFO - 03/11/25 12:10:15 - 0:36:31 - Epoch 1361: train_loss=14.363012313842773
INFO - 03/11/25 12:10:17 - 0:36:32 - Epoch 1362: train_loss=14.362946510314941
INFO - 03/11/25 12:10:18 - 0:36:33 - Epoch 1363: train_loss=14.363248825073242
INFO - 03/11/25 12:10:19 - 0:36:35 - Epoch 1364: train_loss=14.362541198730469
INFO - 03/11/25 12:10:21 - 0:36:36 - Epoch 1365: train_loss=14.364205360412598
INFO - 03/11/25 12:10:22 - 0:36:37 - Epoch 1366: train_loss=14.363845825195312
INFO - 03/11/25 12:10:23 - 0:36:39 - Epoch 1367: train_loss=14.36258602142334
INFO - 03/11/25 12:10:25 - 0:36:40 - Epoch 1368: train_loss=14.362526893615723
INFO - 03/11/25 12:10:26 - 0:36:41 - Epoch 1369: train_loss=14.363127708435059
INFO - 03/11/25 12:10:28 - 0:36:43 - Epoch 1370: train_loss=14.362070083618164
INFO - 03/11/25 12:10:29 - 0:36:44 - Epoch 1371: train_loss=14.364845275878906
INFO - 03/11/25 12:10:30 - 0:36:45 - Epoch 1372: train_loss=14.364965438842773
INFO - 03/11/25 12:10:32 - 0:36:47 - Epoch 1373: train_loss=14.361164093017578
INFO - 03/11/25 12:10:33 - 0:36:48 - Epoch 1374: train_loss=14.365015983581543
INFO - 03/11/25 12:10:34 - 0:36:50 - Epoch 1375: train_loss=14.361701965332031
INFO - 03/11/25 12:10:36 - 0:36:51 - Epoch 1376: train_loss=14.369218826293945
INFO - 03/11/25 12:10:37 - 0:36:52 - Epoch 1377: train_loss=14.370404243469238
INFO - 03/11/25 12:10:38 - 0:36:54 - Epoch 1378: train_loss=14.363601684570312
INFO - 03/11/25 12:10:40 - 0:36:55 - Epoch 1379: train_loss=14.369699478149414
INFO - 03/11/25 12:10:41 - 0:36:56 - Epoch 1380: train_loss=14.372350692749023
INFO - 03/11/25 12:10:42 - 0:36:58 - Epoch 1381: train_loss=14.36760425567627
INFO - 03/11/25 12:10:44 - 0:36:59 - Epoch 1382: train_loss=14.366506576538086
INFO - 03/11/25 12:10:45 - 0:37:00 - Epoch 1383: train_loss=14.368180274963379
INFO - 03/11/25 12:10:47 - 0:37:02 - Epoch 1384: train_loss=14.3668212890625
INFO - 03/11/25 12:10:48 - 0:37:03 - Epoch 1385: train_loss=14.366215705871582
INFO - 03/11/25 12:10:49 - 0:37:04 - Epoch 1386: train_loss=14.364947319030762
INFO - 03/11/25 12:10:51 - 0:37:06 - Epoch 1387: train_loss=14.366703033447266
INFO - 03/11/25 12:10:52 - 0:37:07 - Epoch 1388: train_loss=14.36428165435791
INFO - 03/11/25 12:10:53 - 0:37:09 - Epoch 1389: train_loss=14.366436004638672
INFO - 03/11/25 12:10:55 - 0:37:10 - Epoch 1390: train_loss=14.367488861083984
INFO - 03/11/25 12:10:56 - 0:37:11 - Epoch 1391: train_loss=14.360577583312988
INFO - 03/11/25 12:10:57 - 0:37:13 - Epoch 1392: train_loss=14.368416786193848
INFO - 03/11/25 12:10:59 - 0:37:14 - Epoch 1393: train_loss=14.368959426879883
INFO - 03/11/25 12:11:00 - 0:37:15 - Epoch 1394: train_loss=14.364373207092285
INFO - 03/11/25 12:11:02 - 0:37:17 - Epoch 1395: train_loss=14.366381645202637
INFO - 03/11/25 12:11:03 - 0:37:18 - Epoch 1396: train_loss=14.367171287536621
INFO - 03/11/25 12:11:04 - 0:37:19 - Epoch 1397: train_loss=14.365792274475098
INFO - 03/11/25 12:11:06 - 0:37:21 - Epoch 1398: train_loss=14.365165710449219
INFO - 03/11/25 12:11:07 - 0:37:22 - Epoch 1399: train_loss=14.365917205810547
INFO - 03/11/25 12:11:08 - 0:37:23 - Epoch 1400: train_loss=14.364604949951172
INFO - 03/11/25 12:11:08 - 0:37:23 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:11:10 - 0:37:25 - Decoding cost time:  1.886 s
INFO - 03/11/25 12:11:13 - 0:37:28 - Epoch 1400: ACC: 0.0, NMI: 0.22719501376138632, F1: 0.0, ARI: 0.041543387477432255
INFO - 03/11/25 12:11:13 - 0:37:28 - -------------------------------------------------------------------------
INFO - 03/11/25 12:11:14 - 0:37:29 - Epoch 1401: train_loss=14.365931510925293
INFO - 03/11/25 12:11:16 - 0:37:31 - Epoch 1402: train_loss=14.36476993560791
INFO - 03/11/25 12:11:17 - 0:37:32 - Epoch 1403: train_loss=14.364481925964355
INFO - 03/11/25 12:11:18 - 0:37:33 - Epoch 1404: train_loss=14.364493370056152
INFO - 03/11/25 12:11:20 - 0:37:35 - Epoch 1405: train_loss=14.362860679626465
INFO - 03/11/25 12:11:21 - 0:37:36 - Epoch 1406: train_loss=14.363205909729004
INFO - 03/11/25 12:11:22 - 0:37:38 - Epoch 1407: train_loss=14.361961364746094
INFO - 03/11/25 12:11:24 - 0:37:39 - Epoch 1408: train_loss=14.364888191223145
INFO - 03/11/25 12:11:25 - 0:37:40 - Epoch 1409: train_loss=14.36322021484375
INFO - 03/11/25 12:11:26 - 0:37:42 - Epoch 1410: train_loss=14.365570068359375
INFO - 03/11/25 12:11:28 - 0:37:43 - Epoch 1411: train_loss=14.36545181274414
INFO - 03/11/25 12:11:29 - 0:37:44 - Epoch 1412: train_loss=14.36340045928955
INFO - 03/11/25 12:11:31 - 0:37:46 - Epoch 1413: train_loss=14.363500595092773
INFO - 03/11/25 12:11:32 - 0:37:47 - Epoch 1414: train_loss=14.364105224609375
INFO - 03/11/25 12:11:33 - 0:37:48 - Epoch 1415: train_loss=14.36291217803955
INFO - 03/11/25 12:11:35 - 0:37:50 - Epoch 1416: train_loss=14.365012168884277
INFO - 03/11/25 12:11:36 - 0:37:51 - Epoch 1417: train_loss=14.363420486450195
INFO - 03/11/25 12:11:37 - 0:37:53 - Epoch 1418: train_loss=14.365845680236816
INFO - 03/11/25 12:11:39 - 0:37:54 - Epoch 1419: train_loss=14.364762306213379
INFO - 03/11/25 12:11:40 - 0:37:55 - Epoch 1420: train_loss=14.364797592163086
INFO - 03/11/25 12:11:41 - 0:37:57 - Epoch 1421: train_loss=14.36470890045166
INFO - 03/11/25 12:11:43 - 0:37:58 - Epoch 1422: train_loss=14.36342716217041
INFO - 03/11/25 12:11:44 - 0:37:59 - Epoch 1423: train_loss=14.362770080566406
INFO - 03/11/25 12:11:45 - 0:38:01 - Epoch 1424: train_loss=14.365418434143066
INFO - 03/11/25 12:11:47 - 0:38:02 - Epoch 1425: train_loss=14.364532470703125
INFO - 03/11/25 12:11:48 - 0:38:03 - Epoch 1426: train_loss=14.364242553710938
INFO - 03/11/25 12:11:50 - 0:38:05 - Epoch 1427: train_loss=14.364121437072754
INFO - 03/11/25 12:11:51 - 0:38:06 - Epoch 1428: train_loss=14.363581657409668
INFO - 03/11/25 12:11:52 - 0:38:07 - Epoch 1429: train_loss=14.362781524658203
INFO - 03/11/25 12:11:54 - 0:38:09 - Epoch 1430: train_loss=14.365388870239258
INFO - 03/11/25 12:11:55 - 0:38:10 - Epoch 1431: train_loss=14.364923477172852
INFO - 03/11/25 12:11:56 - 0:38:12 - Epoch 1432: train_loss=14.363190650939941
INFO - 03/11/25 12:11:58 - 0:38:13 - Epoch 1433: train_loss=14.362865447998047
INFO - 03/11/25 12:11:59 - 0:38:14 - Epoch 1434: train_loss=14.364940643310547
INFO - 03/11/25 12:12:00 - 0:38:16 - Epoch 1435: train_loss=14.36427116394043
INFO - 03/11/25 12:12:02 - 0:38:17 - Epoch 1436: train_loss=14.363760948181152
INFO - 03/11/25 12:12:03 - 0:38:18 - Epoch 1437: train_loss=14.363303184509277
INFO - 03/11/25 12:12:05 - 0:38:20 - Epoch 1438: train_loss=14.364678382873535
INFO - 03/11/25 12:12:06 - 0:38:21 - Epoch 1439: train_loss=14.364218711853027
INFO - 03/11/25 12:12:07 - 0:38:22 - Epoch 1440: train_loss=14.363739967346191
INFO - 03/11/25 12:12:09 - 0:38:24 - Epoch 1441: train_loss=14.363221168518066
INFO - 03/11/25 12:12:10 - 0:38:25 - Epoch 1442: train_loss=14.36486530303955
INFO - 03/11/25 12:12:11 - 0:38:26 - Epoch 1443: train_loss=14.364541053771973
INFO - 03/11/25 12:12:13 - 0:38:28 - Epoch 1444: train_loss=14.363553047180176
INFO - 03/11/25 12:12:14 - 0:38:29 - Epoch 1445: train_loss=14.363122940063477
INFO - 03/11/25 12:12:15 - 0:38:31 - Epoch 1446: train_loss=14.365501403808594
INFO - 03/11/25 12:12:17 - 0:38:32 - Epoch 1447: train_loss=14.365314483642578
INFO - 03/11/25 12:12:18 - 0:38:33 - Epoch 1448: train_loss=14.363191604614258
INFO - 03/11/25 12:12:19 - 0:38:35 - Epoch 1449: train_loss=14.362696647644043
INFO - 03/11/25 12:12:21 - 0:38:36 - Epoch 1450: train_loss=14.36558723449707
INFO - 03/11/25 12:12:22 - 0:38:37 - Epoch 1451: train_loss=14.36497974395752
INFO - 03/11/25 12:12:24 - 0:38:39 - Epoch 1452: train_loss=14.363009452819824
INFO - 03/11/25 12:12:25 - 0:38:40 - Epoch 1453: train_loss=14.362521171569824
INFO - 03/11/25 12:12:26 - 0:38:41 - Epoch 1454: train_loss=14.36496639251709
INFO - 03/11/25 12:12:28 - 0:38:43 - Epoch 1455: train_loss=14.3643798828125
INFO - 03/11/25 12:12:29 - 0:38:44 - Epoch 1456: train_loss=14.363180160522461
INFO - 03/11/25 12:12:30 - 0:38:45 - Epoch 1457: train_loss=14.362805366516113
INFO - 03/11/25 12:12:32 - 0:38:47 - Epoch 1458: train_loss=14.364547729492188
INFO - 03/11/25 12:12:33 - 0:38:48 - Epoch 1459: train_loss=14.364062309265137
INFO - 03/11/25 12:12:34 - 0:38:50 - Epoch 1460: train_loss=14.363301277160645
INFO - 03/11/25 12:12:36 - 0:38:51 - Epoch 1461: train_loss=14.362812042236328
INFO - 03/11/25 12:12:37 - 0:38:52 - Epoch 1462: train_loss=14.364653587341309
INFO - 03/11/25 12:12:38 - 0:38:54 - Epoch 1463: train_loss=14.36426067352295
INFO - 03/11/25 12:12:40 - 0:38:55 - Epoch 1464: train_loss=14.362994194030762
INFO - 03/11/25 12:12:41 - 0:38:56 - Epoch 1465: train_loss=14.36241626739502
INFO - 03/11/25 12:12:43 - 0:38:58 - Epoch 1466: train_loss=14.365068435668945
INFO - 03/11/25 12:12:44 - 0:38:59 - Epoch 1467: train_loss=14.364737510681152
INFO - 03/11/25 12:12:45 - 0:39:00 - Epoch 1468: train_loss=14.362462997436523
INFO - 03/11/25 12:12:47 - 0:39:02 - Epoch 1469: train_loss=14.361943244934082
INFO - 03/11/25 12:12:48 - 0:39:03 - Epoch 1470: train_loss=14.36536979675293
INFO - 03/11/25 12:12:49 - 0:39:05 - Epoch 1471: train_loss=14.364962577819824
INFO - 03/11/25 12:12:51 - 0:39:06 - Epoch 1472: train_loss=14.362187385559082
INFO - 03/11/25 12:12:52 - 0:39:07 - Epoch 1473: train_loss=14.361713409423828
INFO - 03/11/25 12:12:53 - 0:39:09 - Epoch 1474: train_loss=14.365477561950684
INFO - 03/11/25 12:12:55 - 0:39:10 - Epoch 1475: train_loss=14.365012168884277
INFO - 03/11/25 12:12:56 - 0:39:11 - Epoch 1476: train_loss=14.362228393554688
INFO - 03/11/25 12:12:57 - 0:39:13 - Epoch 1477: train_loss=14.361946105957031
INFO - 03/11/25 12:12:59 - 0:39:14 - Epoch 1478: train_loss=14.364812850952148
INFO - 03/11/25 12:13:00 - 0:39:15 - Epoch 1479: train_loss=14.364124298095703
INFO - 03/11/25 12:13:02 - 0:39:17 - Epoch 1480: train_loss=14.363338470458984
INFO - 03/11/25 12:13:03 - 0:39:18 - Epoch 1481: train_loss=14.363115310668945
INFO - 03/11/25 12:13:04 - 0:39:19 - Epoch 1482: train_loss=14.363871574401855
INFO - 03/11/25 12:13:06 - 0:39:21 - Epoch 1483: train_loss=14.36330509185791
INFO - 03/11/25 12:13:07 - 0:39:22 - Epoch 1484: train_loss=14.36410140991211
INFO - 03/11/25 12:13:08 - 0:39:24 - Epoch 1485: train_loss=14.363659858703613
INFO - 03/11/25 12:13:10 - 0:39:25 - Epoch 1486: train_loss=14.363605499267578
INFO - 03/11/25 12:13:11 - 0:39:26 - Epoch 1487: train_loss=14.36313533782959
INFO - 03/11/25 12:13:12 - 0:39:28 - Epoch 1488: train_loss=14.363929748535156
INFO - 03/11/25 12:13:14 - 0:39:29 - Epoch 1489: train_loss=14.363320350646973
INFO - 03/11/25 12:13:15 - 0:39:30 - Epoch 1490: train_loss=14.363851547241211
INFO - 03/11/25 12:13:16 - 0:39:32 - Epoch 1491: train_loss=14.36351203918457
INFO - 03/11/25 12:13:18 - 0:39:33 - Epoch 1492: train_loss=14.363303184509277
INFO - 03/11/25 12:13:19 - 0:39:34 - Epoch 1493: train_loss=14.362746238708496
INFO - 03/11/25 12:13:21 - 0:39:36 - Epoch 1494: train_loss=14.364126205444336
INFO - 03/11/25 12:13:22 - 0:39:37 - Epoch 1495: train_loss=14.363664627075195
INFO - 03/11/25 12:13:23 - 0:39:38 - Epoch 1496: train_loss=14.363054275512695
INFO - 03/11/25 12:13:25 - 0:39:40 - Epoch 1497: train_loss=14.362653732299805
INFO - 03/11/25 12:13:26 - 0:39:41 - Epoch 1498: train_loss=14.3639554977417
INFO - 03/11/25 12:13:27 - 0:39:43 - Epoch 1499: train_loss=14.36345386505127
INFO - 03/11/25 12:13:29 - 0:39:44 - Epoch 1500: train_loss=14.363198280334473
INFO - 03/11/25 12:13:29 - 0:39:44 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:13:31 - 0:39:46 - Decoding cost time:  1.903 s
INFO - 03/11/25 12:13:33 - 0:39:48 - Epoch 1500: ACC: 0.0, NMI: 0.1512606029149834, F1: 0.0, ARI: 0.015041454274710573
INFO - 03/11/25 12:13:33 - 0:39:48 - -------------------------------------------------------------------------
INFO - 03/11/25 12:13:33 - 0:39:48 - ------------------Loading best model-------------------
INFO - 03/11/25 12:31:14 - 0:57:30 - Best Results according to nmi: ACC: 0.0, NMI: 0.32671084876622253, F1: 0.0, ARI: 0.16718497081169964 
                                     
INFO - 03/11/25 12:31:14 - 0:57:30 - Best Results according to ari: ACC: 0.0, NMI: 0.32671084876622253, F1: 0.0, ARI: 0.16718497081169964 
                                     
INFO - 03/11/25 12:31:15 - 0:57:30 - 
                                     train iters 1
INFO - 03/11/25 12:31:16 - 0:57:31 - Epoch 1: train_loss=6.017188549041748
INFO - 03/11/25 12:31:17 - 0:57:32 - Epoch 2: train_loss=2.5530893802642822
INFO - 03/11/25 12:31:18 - 0:57:33 - Epoch 3: train_loss=2.9593052864074707
INFO - 03/11/25 12:31:19 - 0:57:34 - Epoch 4: train_loss=2.8141965866088867
INFO - 03/11/25 12:31:20 - 0:57:35 - Epoch 5: train_loss=2.6517961025238037
INFO - 03/11/25 12:31:21 - 0:57:36 - Epoch 6: train_loss=2.513711452484131
INFO - 03/11/25 12:31:22 - 0:57:37 - Epoch 7: train_loss=2.3964712619781494
INFO - 03/11/25 12:31:23 - 0:57:39 - Epoch 8: train_loss=2.2948007583618164
INFO - 03/11/25 12:31:24 - 0:57:40 - Epoch 9: train_loss=2.20452618598938
INFO - 03/11/25 12:31:26 - 0:57:41 - Epoch 10: train_loss=2.12239670753479
INFO - 03/11/25 12:31:27 - 0:57:42 - Epoch 11: train_loss=2.045572280883789
INFO - 03/11/25 12:31:28 - 0:57:43 - Epoch 12: train_loss=1.9709672927856445
INFO - 03/11/25 12:31:29 - 0:57:44 - Epoch 13: train_loss=1.894027829170227
INFO - 03/11/25 12:31:30 - 0:57:45 - Epoch 14: train_loss=1.8313472270965576
INFO - 03/11/25 12:31:31 - 0:57:46 - Epoch 15: train_loss=1.7545835971832275
INFO - 03/11/25 12:31:32 - 0:57:47 - Epoch 16: train_loss=1.6814124584197998
INFO - 03/11/25 12:31:33 - 0:57:48 - Epoch 17: train_loss=1.6004242897033691
INFO - 03/11/25 12:31:34 - 0:57:49 - Epoch 18: train_loss=1.5069695711135864
INFO - 03/11/25 12:31:35 - 0:57:51 - Epoch 19: train_loss=1.3887345790863037
INFO - 03/11/25 12:31:36 - 0:57:52 - Epoch 20: train_loss=1.2597436904907227
INFO - 03/11/25 12:31:38 - 0:57:53 - Epoch 21: train_loss=1.1813275814056396
INFO - 03/11/25 12:31:39 - 0:57:54 - Epoch 22: train_loss=1.3128619194030762
INFO - 03/11/25 12:31:40 - 0:57:55 - Epoch 23: train_loss=1.169030785560608
INFO - 03/11/25 12:31:41 - 0:57:56 - Epoch 24: train_loss=1.0223344564437866
INFO - 03/11/25 12:31:42 - 0:57:57 - Epoch 25: train_loss=0.9240725636482239
INFO - 03/11/25 12:31:43 - 0:57:58 - Epoch 26: train_loss=0.8760865926742554
INFO - 03/11/25 12:31:44 - 0:57:59 - Epoch 27: train_loss=0.8972337245941162
INFO - 03/11/25 12:31:45 - 0:58:00 - Epoch 28: train_loss=0.9243661761283875
INFO - 03/11/25 12:31:46 - 0:58:02 - Epoch 29: train_loss=0.9381512403488159
INFO - 03/11/25 12:31:47 - 0:58:03 - Epoch 30: train_loss=0.9402481913566589
INFO - 03/11/25 12:31:49 - 0:58:04 - Epoch 31: train_loss=0.9348834156990051
INFO - 03/11/25 12:31:50 - 0:58:05 - Epoch 32: train_loss=0.9241877198219299
INFO - 03/11/25 12:31:51 - 0:58:06 - Epoch 33: train_loss=0.9087045192718506
INFO - 03/11/25 12:31:52 - 0:58:07 - Epoch 34: train_loss=0.8888763189315796
INFO - 03/11/25 12:31:53 - 0:58:08 - Epoch 35: train_loss=0.86583012342453
INFO - 03/11/25 12:31:54 - 0:58:09 - Epoch 36: train_loss=0.8417394757270813
INFO - 03/11/25 12:31:55 - 0:58:10 - Epoch 37: train_loss=0.8194147944450378
INFO - 03/11/25 12:31:56 - 0:58:11 - Epoch 38: train_loss=0.8016120195388794
INFO - 03/11/25 12:31:57 - 0:58:13 - Epoch 39: train_loss=0.7914714217185974
INFO - 03/11/25 12:31:58 - 0:58:14 - Epoch 40: train_loss=0.7917114496231079
INFO - 03/11/25 12:32:00 - 0:58:15 - Epoch 41: train_loss=0.7987195253372192
INFO - 03/11/25 12:32:01 - 0:58:16 - Epoch 42: train_loss=0.8049789071083069
INFO - 03/11/25 12:32:02 - 0:58:17 - Epoch 43: train_loss=0.8065089583396912
INFO - 03/11/25 12:32:03 - 0:58:18 - Epoch 44: train_loss=0.8030339479446411
INFO - 03/11/25 12:32:04 - 0:58:19 - Epoch 45: train_loss=0.7961859703063965
INFO - 03/11/25 12:32:05 - 0:58:20 - Epoch 46: train_loss=0.7881335020065308
INFO - 03/11/25 12:32:06 - 0:58:21 - Epoch 47: train_loss=0.7802544832229614
INFO - 03/11/25 12:32:07 - 0:58:22 - Epoch 48: train_loss=0.7723142504692078
INFO - 03/11/25 12:32:08 - 0:58:24 - Epoch 49: train_loss=0.7639235854148865
INFO - 03/11/25 12:32:09 - 0:58:25 - Epoch 50: train_loss=0.7581583857536316
INFO - 03/11/25 12:32:11 - 0:58:26 - Epoch 51: train_loss=0.7603482604026794
INFO - 03/11/25 12:32:12 - 0:58:27 - Epoch 52: train_loss=0.7664090394973755
INFO - 03/11/25 12:32:13 - 0:58:28 - Epoch 53: train_loss=0.7693724036216736
INFO - 03/11/25 12:32:14 - 0:58:29 - Epoch 54: train_loss=0.7666183710098267
INFO - 03/11/25 12:32:15 - 0:58:30 - Epoch 55: train_loss=0.7583090662956238
INFO - 03/11/25 12:32:16 - 0:58:31 - Epoch 56: train_loss=0.747657835483551
INFO - 03/11/25 12:32:17 - 0:58:32 - Epoch 57: train_loss=0.7436333894729614
INFO - 03/11/25 12:32:18 - 0:58:33 - Epoch 58: train_loss=0.7459375262260437
INFO - 03/11/25 12:32:19 - 0:58:34 - Epoch 59: train_loss=0.7481836676597595
INFO - 03/11/25 12:32:20 - 0:58:36 - Epoch 60: train_loss=0.7482842803001404
INFO - 03/11/25 12:32:21 - 0:58:37 - Epoch 61: train_loss=0.7456420063972473
INFO - 03/11/25 12:32:23 - 0:58:38 - Epoch 62: train_loss=0.7401407957077026
INFO - 03/11/25 12:32:24 - 0:58:39 - Epoch 63: train_loss=0.7338383793830872
INFO - 03/11/25 12:32:25 - 0:58:40 - Epoch 64: train_loss=0.733982503414154
INFO - 03/11/25 12:32:26 - 0:58:41 - Epoch 65: train_loss=0.7360602021217346
INFO - 03/11/25 12:32:27 - 0:58:42 - Epoch 66: train_loss=0.7350395321846008
INFO - 03/11/25 12:32:28 - 0:58:43 - Epoch 67: train_loss=0.7314175963401794
INFO - 03/11/25 12:32:29 - 0:58:44 - Epoch 68: train_loss=0.726954996585846
INFO - 03/11/25 12:32:30 - 0:58:45 - Epoch 69: train_loss=0.7256541848182678
INFO - 03/11/25 12:32:31 - 0:58:47 - Epoch 70: train_loss=0.7259063720703125
INFO - 03/11/25 12:32:32 - 0:58:48 - Epoch 71: train_loss=0.7244459986686707
INFO - 03/11/25 12:32:34 - 0:58:49 - Epoch 72: train_loss=0.7221715450286865
INFO - 03/11/25 12:32:35 - 0:58:50 - Epoch 73: train_loss=0.7188136577606201
INFO - 03/11/25 12:32:36 - 0:58:51 - Epoch 74: train_loss=0.7182222008705139
INFO - 03/11/25 12:32:37 - 0:58:52 - Epoch 75: train_loss=0.7179477214813232
INFO - 03/11/25 12:32:38 - 0:58:53 - Epoch 76: train_loss=0.7158734798431396
INFO - 03/11/25 12:32:39 - 0:58:54 - Epoch 77: train_loss=0.7123501300811768
INFO - 03/11/25 12:32:40 - 0:58:55 - Epoch 78: train_loss=0.7121725678443909
INFO - 03/11/25 12:32:41 - 0:58:56 - Epoch 79: train_loss=0.7123399376869202
INFO - 03/11/25 12:32:42 - 0:58:58 - Epoch 80: train_loss=0.7089472413063049
INFO - 03/11/25 12:32:43 - 0:58:59 - Epoch 81: train_loss=0.7074475884437561
INFO - 03/11/25 12:32:45 - 0:59:00 - Epoch 82: train_loss=0.7095452547073364
INFO - 03/11/25 12:32:46 - 0:59:01 - Epoch 83: train_loss=0.705935001373291
INFO - 03/11/25 12:32:47 - 0:59:02 - Epoch 84: train_loss=0.7043918967247009
INFO - 03/11/25 12:32:48 - 0:59:03 - Epoch 85: train_loss=0.7041903734207153
INFO - 03/11/25 12:32:49 - 0:59:04 - Epoch 86: train_loss=0.7014212608337402
INFO - 03/11/25 12:32:50 - 0:59:05 - Epoch 87: train_loss=0.7007352709770203
INFO - 03/11/25 12:32:51 - 0:59:06 - Epoch 88: train_loss=0.6974340081214905
INFO - 03/11/25 12:32:52 - 0:59:07 - Epoch 89: train_loss=0.6986554265022278
INFO - 03/11/25 12:32:53 - 0:59:09 - Epoch 90: train_loss=0.6938616633415222
INFO - 03/11/25 12:32:54 - 0:59:10 - Epoch 91: train_loss=0.6944023370742798
INFO - 03/11/25 12:32:56 - 0:59:11 - Epoch 92: train_loss=0.6917058825492859
INFO - 03/11/25 12:32:57 - 0:59:12 - Epoch 93: train_loss=0.6912250518798828
INFO - 03/11/25 12:32:58 - 0:59:13 - Epoch 94: train_loss=0.6883645057678223
INFO - 03/11/25 12:32:59 - 0:59:14 - Epoch 95: train_loss=0.6867678165435791
INFO - 03/11/25 12:33:00 - 0:59:15 - Epoch 96: train_loss=0.6857721209526062
INFO - 03/11/25 12:33:01 - 0:59:16 - Epoch 97: train_loss=0.6837161779403687
INFO - 03/11/25 12:33:02 - 0:59:17 - Epoch 98: train_loss=0.6827674508094788
INFO - 03/11/25 12:33:03 - 0:59:18 - Epoch 99: train_loss=0.6820542216300964
INFO - 03/11/25 12:33:04 - 0:59:20 - Epoch 100: train_loss=0.6783867478370667
INFO - 03/11/25 12:33:05 - 0:59:21 - Epoch 101: train_loss=0.6785073280334473
INFO - 03/11/25 12:33:07 - 0:59:22 - Epoch 102: train_loss=0.6757490634918213
INFO - 03/11/25 12:33:08 - 0:59:23 - Epoch 103: train_loss=0.6735793948173523
INFO - 03/11/25 12:33:09 - 0:59:24 - Epoch 104: train_loss=0.674454391002655
INFO - 03/11/25 12:33:10 - 0:59:25 - Epoch 105: train_loss=0.6698026657104492
INFO - 03/11/25 12:33:11 - 0:59:26 - Epoch 106: train_loss=0.6708824634552002
INFO - 03/11/25 12:33:12 - 0:59:27 - Epoch 107: train_loss=0.667904257774353
INFO - 03/11/25 12:33:13 - 0:59:28 - Epoch 108: train_loss=0.6655069589614868
INFO - 03/11/25 12:33:14 - 0:59:29 - Epoch 109: train_loss=0.6661637425422668
INFO - 03/11/25 12:33:15 - 0:59:30 - Epoch 110: train_loss=0.6611422300338745
INFO - 03/11/25 12:33:16 - 0:59:32 - Epoch 111: train_loss=0.6636139154434204
INFO - 03/11/25 12:33:18 - 0:59:33 - Epoch 112: train_loss=0.6592443585395813
INFO - 03/11/25 12:33:19 - 0:59:34 - Epoch 113: train_loss=0.6580628752708435
INFO - 03/11/25 12:33:20 - 0:59:35 - Epoch 114: train_loss=0.6567403674125671
INFO - 03/11/25 12:33:21 - 0:59:36 - Epoch 115: train_loss=0.654417872428894
INFO - 03/11/25 12:33:22 - 0:59:37 - Epoch 116: train_loss=0.6539049744606018
INFO - 03/11/25 12:33:23 - 0:59:38 - Epoch 117: train_loss=0.6504901647567749
INFO - 03/11/25 12:33:24 - 0:59:39 - Epoch 118: train_loss=0.6506029963493347
INFO - 03/11/25 12:33:25 - 0:59:40 - Epoch 119: train_loss=0.6468296051025391
INFO - 03/11/25 12:33:26 - 0:59:41 - Epoch 120: train_loss=0.647438108921051
INFO - 03/11/25 12:33:27 - 0:59:43 - Epoch 121: train_loss=0.644141674041748
INFO - 03/11/25 12:33:28 - 0:59:44 - Epoch 122: train_loss=0.6421974301338196
INFO - 03/11/25 12:33:30 - 0:59:45 - Epoch 123: train_loss=0.644034206867218
INFO - 03/11/25 12:33:31 - 0:59:46 - Epoch 124: train_loss=0.639236569404602
INFO - 03/11/25 12:33:32 - 0:59:47 - Epoch 125: train_loss=0.6393790245056152
INFO - 03/11/25 12:33:33 - 0:59:48 - Epoch 126: train_loss=0.6369879245758057
INFO - 03/11/25 12:33:34 - 0:59:49 - Epoch 127: train_loss=0.635252058506012
INFO - 03/11/25 12:33:35 - 0:59:50 - Epoch 128: train_loss=0.631024956703186
INFO - 03/11/25 12:33:36 - 0:59:51 - Epoch 129: train_loss=0.6348543167114258
INFO - 03/11/25 12:33:37 - 0:59:52 - Epoch 130: train_loss=0.6280767321586609
INFO - 03/11/25 12:33:38 - 0:59:54 - Epoch 131: train_loss=0.634441614151001
INFO - 03/11/25 12:33:39 - 0:59:55 - Epoch 132: train_loss=0.6317289471626282
INFO - 03/11/25 12:33:41 - 0:59:56 - Epoch 133: train_loss=0.6254011988639832
INFO - 03/11/25 12:33:42 - 0:59:57 - Epoch 134: train_loss=0.6268115639686584
INFO - 03/11/25 12:33:43 - 0:59:58 - Epoch 135: train_loss=0.6224731802940369
INFO - 03/11/25 12:33:44 - 0:59:59 - Epoch 136: train_loss=0.6233779788017273
INFO - 03/11/25 12:33:45 - 1:00:00 - Epoch 137: train_loss=0.6210250854492188
INFO - 03/11/25 12:33:46 - 1:00:01 - Epoch 138: train_loss=0.6168285608291626
INFO - 03/11/25 12:33:47 - 1:00:02 - Epoch 139: train_loss=0.6163821816444397
INFO - 03/11/25 12:33:48 - 1:00:03 - Epoch 140: train_loss=0.6110253930091858
INFO - 03/11/25 12:33:49 - 1:00:05 - Epoch 141: train_loss=0.616986095905304
INFO - 03/11/25 12:33:50 - 1:00:06 - Epoch 142: train_loss=0.6125237941741943
INFO - 03/11/25 12:33:52 - 1:00:07 - Epoch 143: train_loss=0.6105419993400574
INFO - 03/11/25 12:33:53 - 1:00:08 - Epoch 144: train_loss=0.6083207130432129
INFO - 03/11/25 12:33:54 - 1:00:09 - Epoch 145: train_loss=0.6062256097793579
INFO - 03/11/25 12:33:55 - 1:00:10 - Epoch 146: train_loss=0.6024307608604431
INFO - 03/11/25 12:33:56 - 1:00:11 - Epoch 147: train_loss=0.6030027866363525
INFO - 03/11/25 12:33:57 - 1:00:12 - Epoch 148: train_loss=0.5971088409423828
INFO - 03/11/25 12:33:58 - 1:00:13 - Epoch 149: train_loss=0.6013703346252441
INFO - 03/11/25 12:33:59 - 1:00:14 - Epoch 150: train_loss=0.5955644845962524
INFO - 03/11/25 12:34:00 - 1:00:16 - Epoch 151: train_loss=0.5986725687980652
INFO - 03/11/25 12:34:01 - 1:00:17 - Epoch 152: train_loss=0.5967031121253967
INFO - 03/11/25 12:34:03 - 1:00:18 - Epoch 153: train_loss=0.5877878665924072
INFO - 03/11/25 12:34:04 - 1:00:19 - Epoch 154: train_loss=0.5889446139335632
INFO - 03/11/25 12:34:05 - 1:00:20 - Epoch 155: train_loss=0.583470344543457
INFO - 03/11/25 12:34:06 - 1:00:21 - Epoch 156: train_loss=0.5830352902412415
INFO - 03/11/25 12:34:07 - 1:00:22 - Epoch 157: train_loss=0.5807299613952637
INFO - 03/11/25 12:34:08 - 1:00:23 - Epoch 158: train_loss=0.5785765051841736
INFO - 03/11/25 12:34:09 - 1:00:24 - Epoch 159: train_loss=0.5750823020935059
INFO - 03/11/25 12:34:10 - 1:00:25 - Epoch 160: train_loss=0.5795445442199707
INFO - 03/11/25 12:34:11 - 1:00:27 - Epoch 161: train_loss=0.5729312300682068
INFO - 03/11/25 12:34:12 - 1:00:28 - Epoch 162: train_loss=0.5788400769233704
INFO - 03/11/25 12:34:14 - 1:00:29 - Epoch 163: train_loss=0.5766200423240662
INFO - 03/11/25 12:34:15 - 1:00:30 - Epoch 164: train_loss=0.5685304999351501
INFO - 03/11/25 12:34:16 - 1:00:31 - Epoch 165: train_loss=0.5718160271644592
INFO - 03/11/25 12:34:17 - 1:00:32 - Epoch 166: train_loss=0.5673910975456238
INFO - 03/11/25 12:34:18 - 1:00:33 - Epoch 167: train_loss=0.5656875967979431
INFO - 03/11/25 12:34:19 - 1:00:34 - Epoch 168: train_loss=0.5635026693344116
INFO - 03/11/25 12:34:20 - 1:00:35 - Epoch 169: train_loss=0.5601372122764587
INFO - 03/11/25 12:34:21 - 1:00:36 - Epoch 170: train_loss=0.5587412118911743
INFO - 03/11/25 12:34:22 - 1:00:38 - Epoch 171: train_loss=0.5544648766517639
INFO - 03/11/25 12:34:23 - 1:00:39 - Epoch 172: train_loss=0.5565858483314514
INFO - 03/11/25 12:34:25 - 1:00:40 - Epoch 173: train_loss=0.5515269041061401
INFO - 03/11/25 12:34:26 - 1:00:41 - Epoch 174: train_loss=0.5520869493484497
INFO - 03/11/25 12:34:27 - 1:00:42 - Epoch 175: train_loss=0.5491889119148254
INFO - 03/11/25 12:34:28 - 1:00:43 - Epoch 176: train_loss=0.544968843460083
INFO - 03/11/25 12:34:29 - 1:00:44 - Epoch 177: train_loss=0.5489530563354492
INFO - 03/11/25 12:34:30 - 1:00:45 - Epoch 178: train_loss=0.5442941784858704
INFO - 03/11/25 12:34:31 - 1:00:46 - Epoch 179: train_loss=0.5434901118278503
INFO - 03/11/25 12:34:32 - 1:00:47 - Epoch 180: train_loss=0.540412962436676
INFO - 03/11/25 12:34:33 - 1:00:49 - Epoch 181: train_loss=0.5391843318939209
INFO - 03/11/25 12:34:34 - 1:00:50 - Epoch 182: train_loss=0.5340027213096619
INFO - 03/11/25 12:34:36 - 1:00:51 - Epoch 183: train_loss=0.5364260673522949
INFO - 03/11/25 12:34:37 - 1:00:52 - Epoch 184: train_loss=0.5287084579467773
INFO - 03/11/25 12:34:38 - 1:00:53 - Epoch 185: train_loss=0.5357083678245544
INFO - 03/11/25 12:34:39 - 1:00:54 - Epoch 186: train_loss=0.5309367775917053
INFO - 03/11/25 12:34:40 - 1:00:55 - Epoch 187: train_loss=0.5271390080451965
INFO - 03/11/25 12:34:41 - 1:00:56 - Epoch 188: train_loss=0.5253425240516663
INFO - 03/11/25 12:34:42 - 1:00:57 - Epoch 189: train_loss=0.521811306476593
INFO - 03/11/25 12:34:43 - 1:00:58 - Epoch 190: train_loss=0.5199751853942871
INFO - 03/11/25 12:34:44 - 1:01:00 - Epoch 191: train_loss=0.5163678526878357
INFO - 03/11/25 12:34:45 - 1:01:01 - Epoch 192: train_loss=0.5156662464141846
INFO - 03/11/25 12:34:47 - 1:01:02 - Epoch 193: train_loss=0.5106131434440613
INFO - 03/11/25 12:34:48 - 1:01:03 - Epoch 194: train_loss=0.5133829712867737
INFO - 03/11/25 12:34:49 - 1:01:04 - Epoch 195: train_loss=0.5065209865570068
INFO - 03/11/25 12:34:50 - 1:01:05 - Epoch 196: train_loss=0.5128881931304932
INFO - 03/11/25 12:34:51 - 1:01:06 - Epoch 197: train_loss=0.5086492300033569
INFO - 03/11/25 12:34:52 - 1:01:07 - Epoch 198: train_loss=0.504832923412323
INFO - 03/11/25 12:34:53 - 1:01:08 - Epoch 199: train_loss=0.5027227401733398
INFO - 03/11/25 12:34:53 - 1:01:08 - --------------------------Training Start-------------------------
INFO - 03/11/25 12:34:55 - 1:01:10 - Epoch 1: train_loss=14.726410865783691
INFO - 03/11/25 12:34:56 - 1:01:11 - Epoch 2: train_loss=14.923574447631836
INFO - 03/11/25 12:34:57 - 1:01:12 - Epoch 3: train_loss=14.77530288696289
INFO - 03/11/25 12:34:59 - 1:01:14 - Epoch 4: train_loss=14.700732231140137
INFO - 03/11/25 12:35:00 - 1:01:15 - Epoch 5: train_loss=14.669661521911621
INFO - 03/11/25 12:35:01 - 1:01:17 - Epoch 6: train_loss=14.613153457641602
INFO - 03/11/25 12:35:03 - 1:01:18 - Epoch 7: train_loss=14.585559844970703
INFO - 03/11/25 12:35:04 - 1:01:19 - Epoch 8: train_loss=14.582499504089355
INFO - 03/11/25 12:35:05 - 1:01:21 - Epoch 9: train_loss=14.585819244384766
INFO - 03/11/25 12:35:07 - 1:01:22 - Epoch 10: train_loss=14.579597473144531
INFO - 03/11/25 12:35:08 - 1:01:23 - Epoch 11: train_loss=14.568268775939941
INFO - 03/11/25 12:35:09 - 1:01:25 - Epoch 12: train_loss=14.553508758544922
INFO - 03/11/25 12:35:11 - 1:01:26 - Epoch 13: train_loss=14.53492546081543
INFO - 03/11/25 12:35:12 - 1:01:27 - Epoch 14: train_loss=14.524896621704102
INFO - 03/11/25 12:35:14 - 1:01:29 - Epoch 15: train_loss=14.521586418151855
INFO - 03/11/25 12:35:15 - 1:01:30 - Epoch 16: train_loss=14.52501106262207
INFO - 03/11/25 12:35:16 - 1:01:31 - Epoch 17: train_loss=14.523483276367188
INFO - 03/11/25 12:35:18 - 1:01:33 - Epoch 18: train_loss=14.516382217407227
INFO - 03/11/25 12:35:19 - 1:01:34 - Epoch 19: train_loss=14.505236625671387
INFO - 03/11/25 12:35:20 - 1:01:36 - Epoch 20: train_loss=14.502625465393066
INFO - 03/11/25 12:35:22 - 1:01:37 - Epoch 21: train_loss=14.501387596130371
INFO - 03/11/25 12:35:23 - 1:01:38 - Epoch 22: train_loss=14.499460220336914
INFO - 03/11/25 12:35:24 - 1:01:40 - Epoch 23: train_loss=14.500776290893555
INFO - 03/11/25 12:35:26 - 1:01:41 - Epoch 24: train_loss=14.495604515075684
INFO - 03/11/25 12:35:27 - 1:01:42 - Epoch 25: train_loss=14.488595962524414
INFO - 03/11/25 12:35:29 - 1:01:44 - Epoch 26: train_loss=14.483963012695312
INFO - 03/11/25 12:35:30 - 1:01:45 - Epoch 27: train_loss=14.48556900024414
INFO - 03/11/25 12:35:31 - 1:01:46 - Epoch 28: train_loss=14.479681015014648
INFO - 03/11/25 12:35:33 - 1:01:48 - Epoch 29: train_loss=14.481035232543945
INFO - 03/11/25 12:35:34 - 1:01:49 - Epoch 30: train_loss=14.481128692626953
INFO - 03/11/25 12:35:35 - 1:01:50 - Epoch 31: train_loss=14.47390365600586
INFO - 03/11/25 12:35:37 - 1:01:52 - Epoch 32: train_loss=14.47336483001709
INFO - 03/11/25 12:35:38 - 1:01:53 - Epoch 33: train_loss=14.470808982849121
INFO - 03/11/25 12:35:39 - 1:01:55 - Epoch 34: train_loss=14.47021770477295
INFO - 03/11/25 12:35:41 - 1:01:56 - Epoch 35: train_loss=14.470000267028809
INFO - 03/11/25 12:35:42 - 1:01:57 - Epoch 36: train_loss=14.46778392791748
INFO - 03/11/25 12:35:43 - 1:01:59 - Epoch 37: train_loss=14.468997955322266
INFO - 03/11/25 12:35:45 - 1:02:00 - Epoch 38: train_loss=14.459450721740723
INFO - 03/11/25 12:35:46 - 1:02:01 - Epoch 39: train_loss=14.461326599121094
INFO - 03/11/25 12:35:48 - 1:02:03 - Epoch 40: train_loss=14.465222358703613
INFO - 03/11/25 12:35:49 - 1:02:04 - Epoch 41: train_loss=14.458892822265625
INFO - 03/11/25 12:35:50 - 1:02:05 - Epoch 42: train_loss=14.467856407165527
INFO - 03/11/25 12:35:52 - 1:02:07 - Epoch 43: train_loss=14.460320472717285
INFO - 03/11/25 12:35:53 - 1:02:08 - Epoch 44: train_loss=14.464604377746582
INFO - 03/11/25 12:35:54 - 1:02:10 - Epoch 45: train_loss=14.464073181152344
INFO - 03/11/25 12:35:56 - 1:02:11 - Epoch 46: train_loss=14.452733993530273
INFO - 03/11/25 12:35:57 - 1:02:12 - Epoch 47: train_loss=14.457415580749512
INFO - 03/11/25 12:35:58 - 1:02:14 - Epoch 48: train_loss=14.449095726013184
INFO - 03/11/25 12:36:00 - 1:02:15 - Epoch 49: train_loss=14.459711074829102
INFO - 03/11/25 12:36:01 - 1:02:16 - Epoch 50: train_loss=14.45541763305664
INFO - 03/11/25 12:36:02 - 1:02:18 - Epoch 51: train_loss=14.450830459594727
INFO - 03/11/25 12:36:04 - 1:02:19 - Epoch 52: train_loss=14.450918197631836
INFO - 03/11/25 12:36:05 - 1:02:20 - Epoch 53: train_loss=14.44641399383545
INFO - 03/11/25 12:36:07 - 1:02:22 - Epoch 54: train_loss=14.443504333496094
INFO - 03/11/25 12:36:08 - 1:02:23 - Epoch 55: train_loss=14.449865341186523
INFO - 03/11/25 12:36:09 - 1:02:24 - Epoch 56: train_loss=14.44711971282959
INFO - 03/11/25 12:36:11 - 1:02:26 - Epoch 57: train_loss=14.443572998046875
INFO - 03/11/25 12:36:12 - 1:02:27 - Epoch 58: train_loss=14.444769859313965
INFO - 03/11/25 12:36:13 - 1:02:29 - Epoch 59: train_loss=14.437893867492676
INFO - 03/11/25 12:36:15 - 1:02:30 - Epoch 60: train_loss=14.43842887878418
INFO - 03/11/25 12:36:16 - 1:02:31 - Epoch 61: train_loss=14.442841529846191
INFO - 03/11/25 12:36:17 - 1:02:33 - Epoch 62: train_loss=14.436975479125977
INFO - 03/11/25 12:36:19 - 1:02:34 - Epoch 63: train_loss=14.443065643310547
INFO - 03/11/25 12:36:20 - 1:02:35 - Epoch 64: train_loss=14.440237045288086
INFO - 03/11/25 12:36:21 - 1:02:37 - Epoch 65: train_loss=14.437281608581543
INFO - 03/11/25 12:36:23 - 1:02:38 - Epoch 66: train_loss=14.43823528289795
INFO - 03/11/25 12:36:24 - 1:02:39 - Epoch 67: train_loss=14.433746337890625
INFO - 03/11/25 12:36:26 - 1:02:41 - Epoch 68: train_loss=14.436460494995117
INFO - 03/11/25 12:36:27 - 1:02:42 - Epoch 69: train_loss=14.433417320251465
INFO - 03/11/25 12:36:28 - 1:02:43 - Epoch 70: train_loss=14.433642387390137
INFO - 03/11/25 12:36:30 - 1:02:45 - Epoch 71: train_loss=14.432476997375488
INFO - 03/11/25 12:36:31 - 1:02:46 - Epoch 72: train_loss=14.430453300476074
INFO - 03/11/25 12:36:32 - 1:02:48 - Epoch 73: train_loss=14.427700996398926
INFO - 03/11/25 12:36:34 - 1:02:49 - Epoch 74: train_loss=14.430380821228027
INFO - 03/11/25 12:36:35 - 1:02:50 - Epoch 75: train_loss=14.428321838378906
INFO - 03/11/25 12:36:36 - 1:02:52 - Epoch 76: train_loss=14.425531387329102
INFO - 03/11/25 12:36:38 - 1:02:53 - Epoch 77: train_loss=14.42573070526123
INFO - 03/11/25 12:36:39 - 1:02:54 - Epoch 78: train_loss=14.418453216552734
INFO - 03/11/25 12:36:41 - 1:02:56 - Epoch 79: train_loss=14.427083015441895
INFO - 03/11/25 12:36:42 - 1:02:57 - Epoch 80: train_loss=14.422698020935059
INFO - 03/11/25 12:36:43 - 1:02:58 - Epoch 81: train_loss=14.414851188659668
INFO - 03/11/25 12:36:45 - 1:03:00 - Epoch 82: train_loss=14.417854309082031
INFO - 03/11/25 12:36:46 - 1:03:01 - Epoch 83: train_loss=14.415489196777344
INFO - 03/11/25 12:36:47 - 1:03:03 - Epoch 84: train_loss=14.411545753479004
INFO - 03/11/25 12:36:49 - 1:03:04 - Epoch 85: train_loss=14.411050796508789
INFO - 03/11/25 12:36:50 - 1:03:05 - Epoch 86: train_loss=14.409490585327148
INFO - 03/11/25 12:36:51 - 1:03:07 - Epoch 87: train_loss=14.413187980651855
INFO - 03/11/25 12:36:53 - 1:03:08 - Epoch 88: train_loss=14.407626152038574
INFO - 03/11/25 12:36:54 - 1:03:09 - Epoch 89: train_loss=14.409943580627441
INFO - 03/11/25 12:36:55 - 1:03:11 - Epoch 90: train_loss=14.408223152160645
INFO - 03/11/25 12:36:57 - 1:03:12 - Epoch 91: train_loss=14.401656150817871
INFO - 03/11/25 12:36:58 - 1:03:13 - Epoch 92: train_loss=14.413031578063965
INFO - 03/11/25 12:37:00 - 1:03:15 - Epoch 93: train_loss=14.405189514160156
INFO - 03/11/25 12:37:01 - 1:03:16 - Epoch 94: train_loss=14.40510368347168
INFO - 03/11/25 12:37:02 - 1:03:17 - Epoch 95: train_loss=14.404826164245605
INFO - 03/11/25 12:37:04 - 1:03:19 - Epoch 96: train_loss=14.39976978302002
INFO - 03/11/25 12:37:05 - 1:03:20 - Epoch 97: train_loss=14.400912284851074
INFO - 03/11/25 12:37:06 - 1:03:22 - Epoch 98: train_loss=14.397293090820312
INFO - 03/11/25 12:37:08 - 1:03:23 - Epoch 99: train_loss=14.393204689025879
INFO - 03/11/25 12:37:09 - 1:03:24 - Epoch 100: train_loss=14.390018463134766
INFO - 03/11/25 12:37:09 - 1:03:24 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:37:11 - 1:03:26 - Decoding cost time:  1.723 s
INFO - 03/11/25 12:37:13 - 1:03:28 - ------------------Saving best model-------------------
INFO - 03/11/25 12:37:13 - 1:03:29 - Epoch 100: ACC: 0.0, NMI: 0.023078441617189877, F1: 0.0, ARI: 0.00024360442896587608
INFO - 03/11/25 12:37:13 - 1:03:29 - -------------------------------------------------------------------------
INFO - 03/11/25 12:37:15 - 1:03:30 - Epoch 101: train_loss=14.40092658996582
INFO - 03/11/25 12:37:16 - 1:03:31 - Epoch 102: train_loss=14.392990112304688
INFO - 03/11/25 12:37:18 - 1:03:33 - Epoch 103: train_loss=14.394918441772461
INFO - 03/11/25 12:37:19 - 1:03:34 - Epoch 104: train_loss=14.39273452758789
INFO - 03/11/25 12:37:20 - 1:03:35 - Epoch 105: train_loss=14.38455867767334
INFO - 03/11/25 12:37:22 - 1:03:37 - Epoch 106: train_loss=14.381531715393066
INFO - 03/11/25 12:37:23 - 1:03:38 - Epoch 107: train_loss=14.381026268005371
INFO - 03/11/25 12:37:24 - 1:03:40 - Epoch 108: train_loss=14.37303352355957
INFO - 03/11/25 12:37:26 - 1:03:41 - Epoch 109: train_loss=14.376287460327148
INFO - 03/11/25 12:37:27 - 1:03:42 - Epoch 110: train_loss=14.372360229492188
INFO - 03/11/25 12:37:28 - 1:03:44 - Epoch 111: train_loss=14.373504638671875
INFO - 03/11/25 12:37:30 - 1:03:45 - Epoch 112: train_loss=14.368834495544434
INFO - 03/11/25 12:37:31 - 1:03:46 - Epoch 113: train_loss=14.370967864990234
INFO - 03/11/25 12:37:33 - 1:03:48 - Epoch 114: train_loss=14.38498306274414
INFO - 03/11/25 12:37:34 - 1:03:49 - Epoch 115: train_loss=14.366597175598145
INFO - 03/11/25 12:37:35 - 1:03:50 - Epoch 116: train_loss=14.373977661132812
INFO - 03/11/25 12:37:37 - 1:03:52 - Epoch 117: train_loss=14.366555213928223
INFO - 03/11/25 12:37:38 - 1:03:53 - Epoch 118: train_loss=14.400123596191406
INFO - 03/11/25 12:37:39 - 1:03:55 - Epoch 119: train_loss=14.390547752380371
INFO - 03/11/25 12:37:41 - 1:03:56 - Epoch 120: train_loss=14.386260986328125
INFO - 03/11/25 12:37:42 - 1:03:57 - Epoch 121: train_loss=14.379110336303711
INFO - 03/11/25 12:37:43 - 1:03:59 - Epoch 122: train_loss=14.380417823791504
INFO - 03/11/25 12:37:45 - 1:04:00 - Epoch 123: train_loss=14.37698745727539
INFO - 03/11/25 12:37:46 - 1:04:01 - Epoch 124: train_loss=14.37922477722168
INFO - 03/11/25 12:37:47 - 1:04:03 - Epoch 125: train_loss=14.374757766723633
INFO - 03/11/25 12:37:49 - 1:04:04 - Epoch 126: train_loss=14.398747444152832
INFO - 03/11/25 12:37:50 - 1:04:05 - Epoch 127: train_loss=14.434754371643066
INFO - 03/11/25 12:37:52 - 1:04:07 - Epoch 128: train_loss=14.475312232971191
INFO - 03/11/25 12:37:53 - 1:04:08 - Epoch 129: train_loss=14.396669387817383
INFO - 03/11/25 12:37:54 - 1:04:09 - Epoch 130: train_loss=14.329889297485352
INFO - 03/11/25 12:37:56 - 1:04:11 - Epoch 131: train_loss=14.298660278320312
INFO - 03/11/25 12:37:57 - 1:04:12 - Epoch 132: train_loss=14.268059730529785
INFO - 03/11/25 12:37:58 - 1:04:14 - Epoch 133: train_loss=14.233892440795898
INFO - 03/11/25 12:38:00 - 1:04:15 - Epoch 134: train_loss=14.203102111816406
INFO - 03/11/25 12:38:01 - 1:04:16 - Epoch 135: train_loss=14.192551612854004
INFO - 03/11/25 12:38:02 - 1:04:18 - Epoch 136: train_loss=14.179856300354004
INFO - 03/11/25 12:38:04 - 1:04:19 - Epoch 137: train_loss=14.173406600952148
INFO - 03/11/25 12:38:05 - 1:04:20 - Epoch 138: train_loss=14.159899711608887
INFO - 03/11/25 12:38:06 - 1:04:22 - Epoch 139: train_loss=14.158634185791016
INFO - 03/11/25 12:38:08 - 1:04:23 - Epoch 140: train_loss=14.157201766967773
INFO - 03/11/25 12:38:09 - 1:04:24 - Epoch 141: train_loss=14.142066955566406
INFO - 03/11/25 12:38:11 - 1:04:26 - Epoch 142: train_loss=14.132369041442871
INFO - 03/11/25 12:38:12 - 1:04:27 - Epoch 143: train_loss=14.13546085357666
INFO - 03/11/25 12:38:13 - 1:04:28 - Epoch 144: train_loss=14.130590438842773
INFO - 03/11/25 12:38:15 - 1:04:30 - Epoch 145: train_loss=14.126258850097656
INFO - 03/11/25 12:38:16 - 1:04:31 - Epoch 146: train_loss=14.121699333190918
INFO - 03/11/25 12:38:17 - 1:04:33 - Epoch 147: train_loss=14.114995956420898
INFO - 03/11/25 12:38:19 - 1:04:34 - Epoch 148: train_loss=14.116113662719727
INFO - 03/11/25 12:38:20 - 1:04:35 - Epoch 149: train_loss=14.111340522766113
INFO - 03/11/25 12:38:21 - 1:04:37 - Epoch 150: train_loss=14.10326099395752
INFO - 03/11/25 12:38:23 - 1:04:38 - Epoch 151: train_loss=14.102601051330566
INFO - 03/11/25 12:38:24 - 1:04:39 - Epoch 152: train_loss=14.092689514160156
INFO - 03/11/25 12:38:25 - 1:04:41 - Epoch 153: train_loss=14.09317684173584
INFO - 03/11/25 12:38:27 - 1:04:42 - Epoch 154: train_loss=14.085301399230957
INFO - 03/11/25 12:38:28 - 1:04:43 - Epoch 155: train_loss=14.087980270385742
INFO - 03/11/25 12:38:30 - 1:04:45 - Epoch 156: train_loss=14.086775779724121
INFO - 03/11/25 12:38:31 - 1:04:46 - Epoch 157: train_loss=14.080429077148438
INFO - 03/11/25 12:38:32 - 1:04:47 - Epoch 158: train_loss=14.091405868530273
INFO - 03/11/25 12:38:34 - 1:04:49 - Epoch 159: train_loss=14.087593078613281
INFO - 03/11/25 12:38:35 - 1:04:50 - Epoch 160: train_loss=14.076741218566895
INFO - 03/11/25 12:38:36 - 1:04:52 - Epoch 161: train_loss=14.078354835510254
INFO - 03/11/25 12:38:38 - 1:04:53 - Epoch 162: train_loss=14.064727783203125
INFO - 03/11/25 12:38:39 - 1:04:54 - Epoch 163: train_loss=14.055482864379883
INFO - 03/11/25 12:38:40 - 1:04:56 - Epoch 164: train_loss=14.051645278930664
INFO - 03/11/25 12:38:42 - 1:04:57 - Epoch 165: train_loss=14.059348106384277
INFO - 03/11/25 12:38:43 - 1:04:58 - Epoch 166: train_loss=14.057122230529785
INFO - 03/11/25 12:38:44 - 1:05:00 - Epoch 167: train_loss=14.052879333496094
INFO - 03/11/25 12:38:46 - 1:05:01 - Epoch 168: train_loss=14.053532600402832
INFO - 03/11/25 12:38:47 - 1:05:02 - Epoch 169: train_loss=14.0368070602417
INFO - 03/11/25 12:38:49 - 1:05:04 - Epoch 170: train_loss=14.042290687561035
INFO - 03/11/25 12:38:50 - 1:05:05 - Epoch 171: train_loss=14.03486442565918
INFO - 03/11/25 12:38:51 - 1:05:06 - Epoch 172: train_loss=14.026307106018066
INFO - 03/11/25 12:38:53 - 1:05:08 - Epoch 173: train_loss=14.021246910095215
INFO - 03/11/25 12:38:54 - 1:05:09 - Epoch 174: train_loss=14.0145263671875
INFO - 03/11/25 12:38:55 - 1:05:11 - Epoch 175: train_loss=14.01723861694336
INFO - 03/11/25 12:38:57 - 1:05:12 - Epoch 176: train_loss=14.012309074401855
INFO - 03/11/25 12:38:58 - 1:05:13 - Epoch 177: train_loss=14.006722450256348
INFO - 03/11/25 12:38:59 - 1:05:15 - Epoch 178: train_loss=14.01207447052002
INFO - 03/11/25 12:39:01 - 1:05:16 - Epoch 179: train_loss=14.016437530517578
INFO - 03/11/25 12:39:02 - 1:05:17 - Epoch 180: train_loss=14.001997947692871
INFO - 03/11/25 12:39:04 - 1:05:19 - Epoch 181: train_loss=14.004404067993164
INFO - 03/11/25 12:39:05 - 1:05:20 - Epoch 182: train_loss=13.995973587036133
INFO - 03/11/25 12:39:06 - 1:05:21 - Epoch 183: train_loss=13.997580528259277
INFO - 03/11/25 12:39:08 - 1:05:23 - Epoch 184: train_loss=13.996817588806152
INFO - 03/11/25 12:39:09 - 1:05:24 - Epoch 185: train_loss=13.995030403137207
INFO - 03/11/25 12:39:10 - 1:05:25 - Epoch 186: train_loss=13.990809440612793
INFO - 03/11/25 12:39:12 - 1:05:27 - Epoch 187: train_loss=13.985733032226562
INFO - 03/11/25 12:39:13 - 1:05:28 - Epoch 188: train_loss=13.990845680236816
INFO - 03/11/25 12:39:14 - 1:05:30 - Epoch 189: train_loss=13.989566802978516
INFO - 03/11/25 12:39:16 - 1:05:31 - Epoch 190: train_loss=13.991435050964355
INFO - 03/11/25 12:39:17 - 1:05:32 - Epoch 191: train_loss=13.989022254943848
INFO - 03/11/25 12:39:18 - 1:05:34 - Epoch 192: train_loss=13.990866661071777
INFO - 03/11/25 12:39:20 - 1:05:35 - Epoch 193: train_loss=14.005424499511719
INFO - 03/11/25 12:39:21 - 1:05:36 - Epoch 194: train_loss=14.014983177185059
INFO - 03/11/25 12:39:23 - 1:05:38 - Epoch 195: train_loss=14.011579513549805
INFO - 03/11/25 12:39:24 - 1:05:39 - Epoch 196: train_loss=14.00302505493164
INFO - 03/11/25 12:39:25 - 1:05:40 - Epoch 197: train_loss=13.998429298400879
INFO - 03/11/25 12:39:27 - 1:05:42 - Epoch 198: train_loss=13.99531078338623
INFO - 03/11/25 12:39:28 - 1:05:43 - Epoch 199: train_loss=13.990185737609863
INFO - 03/11/25 12:39:29 - 1:05:45 - Epoch 200: train_loss=13.986103057861328
INFO - 03/11/25 12:39:29 - 1:05:45 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:39:31 - 1:05:46 - Decoding cost time:  1.702 s
INFO - 03/11/25 12:39:33 - 1:05:49 - ------------------Saving best model-------------------
INFO - 03/11/25 12:39:34 - 1:05:49 - Epoch 200: ACC: 0.0, NMI: 0.12546871435152543, F1: 0.0, ARI: 0.039924037842403066
INFO - 03/11/25 12:39:34 - 1:05:49 - -------------------------------------------------------------------------
INFO - 03/11/25 12:39:35 - 1:05:50 - Epoch 201: train_loss=13.990303039550781
INFO - 03/11/25 12:39:36 - 1:05:51 - Epoch 202: train_loss=13.985159873962402
INFO - 03/11/25 12:39:38 - 1:05:53 - Epoch 203: train_loss=13.984567642211914
INFO - 03/11/25 12:39:39 - 1:05:54 - Epoch 204: train_loss=13.98741340637207
INFO - 03/11/25 12:39:40 - 1:05:56 - Epoch 205: train_loss=13.979008674621582
INFO - 03/11/25 12:39:42 - 1:05:57 - Epoch 206: train_loss=13.976031303405762
INFO - 03/11/25 12:39:43 - 1:05:58 - Epoch 207: train_loss=13.970025062561035
INFO - 03/11/25 12:39:44 - 1:06:00 - Epoch 208: train_loss=13.984100341796875
INFO - 03/11/25 12:39:46 - 1:06:01 - Epoch 209: train_loss=13.977282524108887
INFO - 03/11/25 12:39:47 - 1:06:02 - Epoch 210: train_loss=13.954691886901855
INFO - 03/11/25 12:39:48 - 1:06:04 - Epoch 211: train_loss=13.962624549865723
INFO - 03/11/25 12:39:50 - 1:06:05 - Epoch 212: train_loss=13.952388763427734
INFO - 03/11/25 12:39:51 - 1:06:06 - Epoch 213: train_loss=13.937260627746582
INFO - 03/11/25 12:39:53 - 1:06:08 - Epoch 214: train_loss=13.923538208007812
INFO - 03/11/25 12:39:54 - 1:06:09 - Epoch 215: train_loss=13.955759048461914
INFO - 03/11/25 12:39:55 - 1:06:10 - Epoch 216: train_loss=13.97420883178711
INFO - 03/11/25 12:39:57 - 1:06:12 - Epoch 217: train_loss=13.956855773925781
INFO - 03/11/25 12:39:58 - 1:06:13 - Epoch 218: train_loss=13.988913536071777
INFO - 03/11/25 12:39:59 - 1:06:15 - Epoch 219: train_loss=14.01331901550293
INFO - 03/11/25 12:40:01 - 1:06:16 - Epoch 220: train_loss=14.023168563842773
INFO - 03/11/25 12:40:02 - 1:06:17 - Epoch 221: train_loss=13.976516723632812
INFO - 03/11/25 12:40:03 - 1:06:19 - Epoch 222: train_loss=13.948846817016602
INFO - 03/11/25 12:40:05 - 1:06:20 - Epoch 223: train_loss=13.926156997680664
INFO - 03/11/25 12:40:06 - 1:06:21 - Epoch 224: train_loss=13.899842262268066
INFO - 03/11/25 12:40:08 - 1:06:23 - Epoch 225: train_loss=13.88504409790039
INFO - 03/11/25 12:40:09 - 1:06:24 - Epoch 226: train_loss=13.875829696655273
INFO - 03/11/25 12:40:10 - 1:06:25 - Epoch 227: train_loss=13.882322311401367
INFO - 03/11/25 12:40:12 - 1:06:27 - Epoch 228: train_loss=13.878002166748047
INFO - 03/11/25 12:40:13 - 1:06:28 - Epoch 229: train_loss=13.869900703430176
INFO - 03/11/25 12:40:14 - 1:06:29 - Epoch 230: train_loss=13.86923885345459
INFO - 03/11/25 12:40:16 - 1:06:31 - Epoch 231: train_loss=13.870521545410156
INFO - 03/11/25 12:40:17 - 1:06:32 - Epoch 232: train_loss=13.880949974060059
INFO - 03/11/25 12:40:18 - 1:06:34 - Epoch 233: train_loss=13.894145965576172
INFO - 03/11/25 12:40:20 - 1:06:35 - Epoch 234: train_loss=13.88228702545166
INFO - 03/11/25 12:40:21 - 1:06:36 - Epoch 235: train_loss=13.871710777282715
INFO - 03/11/25 12:40:22 - 1:06:38 - Epoch 236: train_loss=13.873679161071777
INFO - 03/11/25 12:40:24 - 1:06:39 - Epoch 237: train_loss=13.859601020812988
INFO - 03/11/25 12:40:25 - 1:06:40 - Epoch 238: train_loss=13.856099128723145
INFO - 03/11/25 12:40:27 - 1:06:42 - Epoch 239: train_loss=13.858490943908691
INFO - 03/11/25 12:40:28 - 1:06:43 - Epoch 240: train_loss=13.840616226196289
INFO - 03/11/25 12:40:29 - 1:06:44 - Epoch 241: train_loss=13.840571403503418
INFO - 03/11/25 12:40:31 - 1:06:46 - Epoch 242: train_loss=13.82321834564209
INFO - 03/11/25 12:40:32 - 1:06:47 - Epoch 243: train_loss=13.817235946655273
INFO - 03/11/25 12:40:33 - 1:06:49 - Epoch 244: train_loss=13.793754577636719
INFO - 03/11/25 12:40:35 - 1:06:50 - Epoch 245: train_loss=13.753992080688477
INFO - 03/11/25 12:40:36 - 1:06:51 - Epoch 246: train_loss=13.713691711425781
INFO - 03/11/25 12:40:37 - 1:06:53 - Epoch 247: train_loss=13.684577941894531
INFO - 03/11/25 12:40:39 - 1:06:54 - Epoch 248: train_loss=13.667512893676758
INFO - 03/11/25 12:40:40 - 1:06:55 - Epoch 249: train_loss=13.654829025268555
INFO - 03/11/25 12:40:41 - 1:06:57 - Epoch 250: train_loss=13.596739768981934
INFO - 03/11/25 12:40:43 - 1:06:58 - Epoch 251: train_loss=13.52698802947998
INFO - 03/11/25 12:40:44 - 1:06:59 - Epoch 252: train_loss=13.468626976013184
INFO - 03/11/25 12:40:46 - 1:07:01 - Epoch 253: train_loss=13.398109436035156
INFO - 03/11/25 12:40:47 - 1:07:02 - Epoch 254: train_loss=13.342451095581055
INFO - 03/11/25 12:40:48 - 1:07:03 - Epoch 255: train_loss=13.319318771362305
INFO - 03/11/25 12:40:50 - 1:07:05 - Epoch 256: train_loss=13.265295028686523
INFO - 03/11/25 12:40:51 - 1:07:06 - Epoch 257: train_loss=13.292724609375
INFO - 03/11/25 12:40:52 - 1:07:08 - Epoch 258: train_loss=13.231979370117188
INFO - 03/11/25 12:40:54 - 1:07:09 - Epoch 259: train_loss=13.211458206176758
INFO - 03/11/25 12:40:55 - 1:07:10 - Epoch 260: train_loss=13.217079162597656
INFO - 03/11/25 12:40:56 - 1:07:12 - Epoch 261: train_loss=13.221778869628906
INFO - 03/11/25 12:40:58 - 1:07:13 - Epoch 262: train_loss=13.214885711669922
INFO - 03/11/25 12:40:59 - 1:07:14 - Epoch 263: train_loss=13.215509414672852
INFO - 03/11/25 12:41:00 - 1:07:16 - Epoch 264: train_loss=13.211248397827148
INFO - 03/11/25 12:41:02 - 1:07:17 - Epoch 265: train_loss=13.203556060791016
INFO - 03/11/25 12:41:03 - 1:07:18 - Epoch 266: train_loss=13.197760581970215
INFO - 03/11/25 12:41:05 - 1:07:20 - Epoch 267: train_loss=13.196268081665039
INFO - 03/11/25 12:41:06 - 1:07:21 - Epoch 268: train_loss=13.214092254638672
INFO - 03/11/25 12:41:07 - 1:07:22 - Epoch 269: train_loss=13.216048240661621
INFO - 03/11/25 12:41:09 - 1:07:24 - Epoch 270: train_loss=13.208110809326172
INFO - 03/11/25 12:41:10 - 1:07:25 - Epoch 271: train_loss=13.207139015197754
INFO - 03/11/25 12:41:11 - 1:07:27 - Epoch 272: train_loss=13.187766075134277
INFO - 03/11/25 12:41:13 - 1:07:28 - Epoch 273: train_loss=13.185980796813965
INFO - 03/11/25 12:41:14 - 1:07:29 - Epoch 274: train_loss=13.179019927978516
INFO - 03/11/25 12:41:15 - 1:07:31 - Epoch 275: train_loss=13.172441482543945
INFO - 03/11/25 12:41:17 - 1:07:32 - Epoch 276: train_loss=13.172292709350586
INFO - 03/11/25 12:41:18 - 1:07:33 - Epoch 277: train_loss=13.167890548706055
INFO - 03/11/25 12:41:20 - 1:07:35 - Epoch 278: train_loss=13.160168647766113
INFO - 03/11/25 12:41:21 - 1:07:36 - Epoch 279: train_loss=13.160534858703613
INFO - 03/11/25 12:41:22 - 1:07:37 - Epoch 280: train_loss=13.158116340637207
INFO - 03/11/25 12:41:24 - 1:07:39 - Epoch 281: train_loss=13.155139923095703
INFO - 03/11/25 12:41:25 - 1:07:40 - Epoch 282: train_loss=13.150405883789062
INFO - 03/11/25 12:41:26 - 1:07:41 - Epoch 283: train_loss=13.149898529052734
INFO - 03/11/25 12:41:28 - 1:07:43 - Epoch 284: train_loss=13.143058776855469
INFO - 03/11/25 12:41:29 - 1:07:44 - Epoch 285: train_loss=13.143482208251953
INFO - 03/11/25 12:41:30 - 1:07:46 - Epoch 286: train_loss=13.139374732971191
INFO - 03/11/25 12:41:32 - 1:07:47 - Epoch 287: train_loss=13.145509719848633
INFO - 03/11/25 12:41:33 - 1:07:48 - Epoch 288: train_loss=13.132833480834961
INFO - 03/11/25 12:41:34 - 1:07:50 - Epoch 289: train_loss=13.147089958190918
INFO - 03/11/25 12:41:36 - 1:07:51 - Epoch 290: train_loss=13.144486427307129
INFO - 03/11/25 12:41:37 - 1:07:52 - Epoch 291: train_loss=13.136451721191406
INFO - 03/11/25 12:41:39 - 1:07:54 - Epoch 292: train_loss=13.134797096252441
INFO - 03/11/25 12:41:40 - 1:07:55 - Epoch 293: train_loss=13.134072303771973
INFO - 03/11/25 12:41:41 - 1:07:56 - Epoch 294: train_loss=13.124870300292969
INFO - 03/11/25 12:41:43 - 1:07:58 - Epoch 295: train_loss=13.140745162963867
INFO - 03/11/25 12:41:44 - 1:07:59 - Epoch 296: train_loss=13.13321590423584
INFO - 03/11/25 12:41:45 - 1:08:00 - Epoch 297: train_loss=13.131036758422852
INFO - 03/11/25 12:41:47 - 1:08:02 - Epoch 298: train_loss=13.128385543823242
INFO - 03/11/25 12:41:48 - 1:08:03 - Epoch 299: train_loss=13.130269050598145
INFO - 03/11/25 12:41:49 - 1:08:05 - Epoch 300: train_loss=13.123887062072754
INFO - 03/11/25 12:41:49 - 1:08:05 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:41:51 - 1:08:06 - Decoding cost time:  1.700 s
INFO - 03/11/25 12:41:54 - 1:08:09 - ------------------Saving best model-------------------
INFO - 03/11/25 12:41:54 - 1:08:09 - Epoch 300: ACC: 0.0, NMI: 0.2220431344431232, F1: 0.0, ARI: 0.09792242924916317
INFO - 03/11/25 12:41:54 - 1:08:09 - -------------------------------------------------------------------------
INFO - 03/11/25 12:41:55 - 1:08:10 - Epoch 301: train_loss=13.139506340026855
INFO - 03/11/25 12:41:56 - 1:08:12 - Epoch 302: train_loss=13.123342514038086
INFO - 03/11/25 12:41:58 - 1:08:13 - Epoch 303: train_loss=13.132917404174805
INFO - 03/11/25 12:41:59 - 1:08:14 - Epoch 304: train_loss=13.12399959564209
INFO - 03/11/25 12:42:00 - 1:08:16 - Epoch 305: train_loss=13.131064414978027
INFO - 03/11/25 12:42:02 - 1:08:17 - Epoch 306: train_loss=13.12008285522461
INFO - 03/11/25 12:42:03 - 1:08:18 - Epoch 307: train_loss=13.147089004516602
INFO - 03/11/25 12:42:04 - 1:08:20 - Epoch 308: train_loss=13.141349792480469
INFO - 03/11/25 12:42:06 - 1:08:21 - Epoch 309: train_loss=13.158645629882812
INFO - 03/11/25 12:42:07 - 1:08:22 - Epoch 310: train_loss=13.155570983886719
INFO - 03/11/25 12:42:09 - 1:08:24 - Epoch 311: train_loss=13.142444610595703
INFO - 03/11/25 12:42:10 - 1:08:25 - Epoch 312: train_loss=13.137248039245605
INFO - 03/11/25 12:42:11 - 1:08:26 - Epoch 313: train_loss=13.138531684875488
INFO - 03/11/25 12:42:13 - 1:08:28 - Epoch 314: train_loss=13.132789611816406
INFO - 03/11/25 12:42:14 - 1:08:29 - Epoch 315: train_loss=13.133194923400879
INFO - 03/11/25 12:42:15 - 1:08:31 - Epoch 316: train_loss=13.128558158874512
INFO - 03/11/25 12:42:17 - 1:08:32 - Epoch 317: train_loss=13.131797790527344
INFO - 03/11/25 12:42:18 - 1:08:33 - Epoch 318: train_loss=13.129692077636719
INFO - 03/11/25 12:42:19 - 1:08:35 - Epoch 319: train_loss=13.121785163879395
INFO - 03/11/25 12:42:21 - 1:08:36 - Epoch 320: train_loss=13.124051094055176
INFO - 03/11/25 12:42:22 - 1:08:37 - Epoch 321: train_loss=13.111284255981445
INFO - 03/11/25 12:42:23 - 1:08:39 - Epoch 322: train_loss=13.114585876464844
INFO - 03/11/25 12:42:25 - 1:08:40 - Epoch 323: train_loss=13.112264633178711
INFO - 03/11/25 12:42:26 - 1:08:41 - Epoch 324: train_loss=13.107338905334473
INFO - 03/11/25 12:42:28 - 1:08:43 - Epoch 325: train_loss=13.108358383178711
INFO - 03/11/25 12:42:29 - 1:08:44 - Epoch 326: train_loss=13.103590965270996
INFO - 03/11/25 12:42:30 - 1:08:45 - Epoch 327: train_loss=13.108349800109863
INFO - 03/11/25 12:42:32 - 1:08:47 - Epoch 328: train_loss=13.105011940002441
INFO - 03/11/25 12:42:33 - 1:08:48 - Epoch 329: train_loss=13.09965991973877
INFO - 03/11/25 12:42:34 - 1:08:50 - Epoch 330: train_loss=13.093779563903809
INFO - 03/11/25 12:42:36 - 1:08:51 - Epoch 331: train_loss=13.108543395996094
INFO - 03/11/25 12:42:37 - 1:08:52 - Epoch 332: train_loss=13.095447540283203
INFO - 03/11/25 12:42:38 - 1:08:54 - Epoch 333: train_loss=13.110038757324219
INFO - 03/11/25 12:42:40 - 1:08:55 - Epoch 334: train_loss=13.109615325927734
INFO - 03/11/25 12:42:41 - 1:08:56 - Epoch 335: train_loss=13.095795631408691
INFO - 03/11/25 12:42:42 - 1:08:58 - Epoch 336: train_loss=13.098374366760254
INFO - 03/11/25 12:42:44 - 1:08:59 - Epoch 337: train_loss=13.096532821655273
INFO - 03/11/25 12:42:45 - 1:09:00 - Epoch 338: train_loss=13.092951774597168
INFO - 03/11/25 12:42:47 - 1:09:02 - Epoch 339: train_loss=13.095650672912598
INFO - 03/11/25 12:42:48 - 1:09:03 - Epoch 340: train_loss=13.088493347167969
INFO - 03/11/25 12:42:49 - 1:09:04 - Epoch 341: train_loss=13.091856956481934
INFO - 03/11/25 12:42:51 - 1:09:06 - Epoch 342: train_loss=13.09197998046875
INFO - 03/11/25 12:42:52 - 1:09:07 - Epoch 343: train_loss=13.085803031921387
INFO - 03/11/25 12:42:53 - 1:09:09 - Epoch 344: train_loss=13.084446907043457
INFO - 03/11/25 12:42:55 - 1:09:10 - Epoch 345: train_loss=13.07925033569336
INFO - 03/11/25 12:42:56 - 1:09:11 - Epoch 346: train_loss=13.083722114562988
INFO - 03/11/25 12:42:57 - 1:09:13 - Epoch 347: train_loss=13.078285217285156
INFO - 03/11/25 12:42:59 - 1:09:14 - Epoch 348: train_loss=13.076947212219238
INFO - 03/11/25 12:43:00 - 1:09:15 - Epoch 349: train_loss=13.079015731811523
INFO - 03/11/25 12:43:01 - 1:09:17 - Epoch 350: train_loss=13.07431411743164
INFO - 03/11/25 12:43:03 - 1:09:18 - Epoch 351: train_loss=13.075474739074707
INFO - 03/11/25 12:43:04 - 1:09:19 - Epoch 352: train_loss=13.07565689086914
INFO - 03/11/25 12:43:06 - 1:09:21 - Epoch 353: train_loss=13.068929672241211
INFO - 03/11/25 12:43:07 - 1:09:22 - Epoch 354: train_loss=13.07268238067627
INFO - 03/11/25 12:43:08 - 1:09:23 - Epoch 355: train_loss=13.071220397949219
INFO - 03/11/25 12:43:10 - 1:09:25 - Epoch 356: train_loss=13.067859649658203
INFO - 03/11/25 12:43:11 - 1:09:26 - Epoch 357: train_loss=13.0737886428833
INFO - 03/11/25 12:43:12 - 1:09:28 - Epoch 358: train_loss=13.069567680358887
INFO - 03/11/25 12:43:14 - 1:09:29 - Epoch 359: train_loss=13.069774627685547
INFO - 03/11/25 12:43:15 - 1:09:30 - Epoch 360: train_loss=13.068692207336426
INFO - 03/11/25 12:43:16 - 1:09:32 - Epoch 361: train_loss=13.064030647277832
INFO - 03/11/25 12:43:18 - 1:09:33 - Epoch 362: train_loss=13.083174705505371
INFO - 03/11/25 12:43:19 - 1:09:34 - Epoch 363: train_loss=13.072394371032715
INFO - 03/11/25 12:43:20 - 1:09:36 - Epoch 364: train_loss=13.073677062988281
INFO - 03/11/25 12:43:22 - 1:09:37 - Epoch 365: train_loss=13.073071479797363
INFO - 03/11/25 12:43:23 - 1:09:38 - Epoch 366: train_loss=13.066184997558594
INFO - 03/11/25 12:43:25 - 1:09:40 - Epoch 367: train_loss=13.061513900756836
INFO - 03/11/25 12:43:26 - 1:09:41 - Epoch 368: train_loss=13.071660041809082
INFO - 03/11/25 12:43:27 - 1:09:42 - Epoch 369: train_loss=13.06446361541748
INFO - 03/11/25 12:43:29 - 1:09:44 - Epoch 370: train_loss=13.06016731262207
INFO - 03/11/25 12:43:30 - 1:09:45 - Epoch 371: train_loss=13.063192367553711
INFO - 03/11/25 12:43:31 - 1:09:47 - Epoch 372: train_loss=13.064386367797852
INFO - 03/11/25 12:43:33 - 1:09:48 - Epoch 373: train_loss=13.059837341308594
INFO - 03/11/25 12:43:34 - 1:09:49 - Epoch 374: train_loss=13.062458992004395
INFO - 03/11/25 12:43:35 - 1:09:51 - Epoch 375: train_loss=13.062458038330078
INFO - 03/11/25 12:43:37 - 1:09:52 - Epoch 376: train_loss=13.058353424072266
INFO - 03/11/25 12:43:38 - 1:09:53 - Epoch 377: train_loss=13.056873321533203
INFO - 03/11/25 12:43:40 - 1:09:55 - Epoch 378: train_loss=13.06380558013916
INFO - 03/11/25 12:43:41 - 1:09:56 - Epoch 379: train_loss=13.058506965637207
INFO - 03/11/25 12:43:42 - 1:09:57 - Epoch 380: train_loss=13.0613431930542
INFO - 03/11/25 12:43:44 - 1:09:59 - Epoch 381: train_loss=13.058393478393555
INFO - 03/11/25 12:43:45 - 1:10:00 - Epoch 382: train_loss=13.055012702941895
INFO - 03/11/25 12:43:46 - 1:10:01 - Epoch 383: train_loss=13.0553617477417
INFO - 03/11/25 12:43:48 - 1:10:03 - Epoch 384: train_loss=13.054558753967285
INFO - 03/11/25 12:43:49 - 1:10:04 - Epoch 385: train_loss=13.055286407470703
INFO - 03/11/25 12:43:50 - 1:10:06 - Epoch 386: train_loss=13.05120849609375
INFO - 03/11/25 12:43:52 - 1:10:07 - Epoch 387: train_loss=13.052600860595703
INFO - 03/11/25 12:43:53 - 1:10:08 - Epoch 388: train_loss=13.04862117767334
INFO - 03/11/25 12:43:54 - 1:10:10 - Epoch 389: train_loss=13.055562973022461
INFO - 03/11/25 12:43:56 - 1:10:11 - Epoch 390: train_loss=13.052297592163086
INFO - 03/11/25 12:43:57 - 1:10:12 - Epoch 391: train_loss=13.049833297729492
INFO - 03/11/25 12:43:59 - 1:10:14 - Epoch 392: train_loss=13.048688888549805
INFO - 03/11/25 12:44:00 - 1:10:15 - Epoch 393: train_loss=13.055651664733887
INFO - 03/11/25 12:44:01 - 1:10:16 - Epoch 394: train_loss=13.046151161193848
INFO - 03/11/25 12:44:03 - 1:10:18 - Epoch 395: train_loss=13.061585426330566
INFO - 03/11/25 12:44:04 - 1:10:19 - Epoch 396: train_loss=13.055745124816895
INFO - 03/11/25 12:44:05 - 1:10:21 - Epoch 397: train_loss=13.058761596679688
INFO - 03/11/25 12:44:07 - 1:10:22 - Epoch 398: train_loss=13.050683975219727
INFO - 03/11/25 12:44:08 - 1:10:23 - Epoch 399: train_loss=13.053901672363281
INFO - 03/11/25 12:44:09 - 1:10:25 - Epoch 400: train_loss=13.046586036682129
INFO - 03/11/25 12:44:09 - 1:10:25 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:44:11 - 1:10:26 - Decoding cost time:  1.703 s
INFO - 03/11/25 12:44:18 - 1:10:33 - ------------------Saving best model-------------------
INFO - 03/11/25 12:44:18 - 1:10:33 - Epoch 400: ACC: 0.0, NMI: 0.227717381035739, F1: 0.0, ARI: 0.095696265026696
INFO - 03/11/25 12:44:18 - 1:10:33 - -------------------------------------------------------------------------
INFO - 03/11/25 12:44:19 - 1:10:34 - Epoch 401: train_loss=13.053084373474121
INFO - 03/11/25 12:44:20 - 1:10:36 - Epoch 402: train_loss=13.048218727111816
INFO - 03/11/25 12:44:22 - 1:10:37 - Epoch 403: train_loss=13.062949180603027
INFO - 03/11/25 12:44:23 - 1:10:38 - Epoch 404: train_loss=13.057583808898926
INFO - 03/11/25 12:44:24 - 1:10:40 - Epoch 405: train_loss=13.073996543884277
INFO - 03/11/25 12:44:26 - 1:10:41 - Epoch 406: train_loss=13.067182540893555
INFO - 03/11/25 12:44:27 - 1:10:42 - Epoch 407: train_loss=13.076861381530762
INFO - 03/11/25 12:44:29 - 1:10:44 - Epoch 408: train_loss=13.07596492767334
INFO - 03/11/25 12:44:30 - 1:10:45 - Epoch 409: train_loss=13.061208724975586
INFO - 03/11/25 12:44:31 - 1:10:46 - Epoch 410: train_loss=13.07412338256836
INFO - 03/11/25 12:44:33 - 1:10:48 - Epoch 411: train_loss=13.072078704833984
INFO - 03/11/25 12:44:34 - 1:10:49 - Epoch 412: train_loss=13.055524826049805
INFO - 03/11/25 12:44:35 - 1:10:51 - Epoch 413: train_loss=13.065074920654297
INFO - 03/11/25 12:44:37 - 1:10:52 - Epoch 414: train_loss=13.069759368896484
INFO - 03/11/25 12:44:38 - 1:10:53 - Epoch 415: train_loss=13.065149307250977
INFO - 03/11/25 12:44:39 - 1:10:55 - Epoch 416: train_loss=13.06296157836914
INFO - 03/11/25 12:44:41 - 1:10:56 - Epoch 417: train_loss=13.056791305541992
INFO - 03/11/25 12:44:42 - 1:10:57 - Epoch 418: train_loss=13.054049491882324
INFO - 03/11/25 12:44:43 - 1:10:59 - Epoch 419: train_loss=13.0592622756958
INFO - 03/11/25 12:44:45 - 1:11:00 - Epoch 420: train_loss=13.054990768432617
INFO - 03/11/25 12:44:46 - 1:11:01 - Epoch 421: train_loss=13.061885833740234
INFO - 03/11/25 12:44:48 - 1:11:03 - Epoch 422: train_loss=13.05612564086914
INFO - 03/11/25 12:44:49 - 1:11:04 - Epoch 423: train_loss=13.0602445602417
INFO - 03/11/25 12:44:50 - 1:11:05 - Epoch 424: train_loss=13.059796333312988
INFO - 03/11/25 12:44:52 - 1:11:07 - Epoch 425: train_loss=13.052552223205566
INFO - 03/11/25 12:44:53 - 1:11:08 - Epoch 426: train_loss=13.060247421264648
INFO - 03/11/25 12:44:54 - 1:11:10 - Epoch 427: train_loss=13.058944702148438
INFO - 03/11/25 12:44:56 - 1:11:11 - Epoch 428: train_loss=13.050806045532227
INFO - 03/11/25 12:44:57 - 1:11:12 - Epoch 429: train_loss=13.057841300964355
INFO - 03/11/25 12:44:58 - 1:11:14 - Epoch 430: train_loss=13.053529739379883
INFO - 03/11/25 12:45:00 - 1:11:15 - Epoch 431: train_loss=13.0549898147583
INFO - 03/11/25 12:45:01 - 1:11:16 - Epoch 432: train_loss=13.05417251586914
INFO - 03/11/25 12:45:02 - 1:11:18 - Epoch 433: train_loss=13.049766540527344
INFO - 03/11/25 12:45:04 - 1:11:19 - Epoch 434: train_loss=13.05318832397461
INFO - 03/11/25 12:45:05 - 1:11:20 - Epoch 435: train_loss=13.046797752380371
INFO - 03/11/25 12:45:07 - 1:11:22 - Epoch 436: train_loss=13.056200981140137
INFO - 03/11/25 12:45:08 - 1:11:23 - Epoch 437: train_loss=13.05224609375
INFO - 03/11/25 12:45:09 - 1:11:24 - Epoch 438: train_loss=13.052359580993652
INFO - 03/11/25 12:45:11 - 1:11:26 - Epoch 439: train_loss=13.05069637298584
INFO - 03/11/25 12:45:12 - 1:11:27 - Epoch 440: train_loss=13.048643112182617
INFO - 03/11/25 12:45:13 - 1:11:29 - Epoch 441: train_loss=13.04775619506836
INFO - 03/11/25 12:45:15 - 1:11:30 - Epoch 442: train_loss=13.047662734985352
INFO - 03/11/25 12:45:16 - 1:11:31 - Epoch 443: train_loss=13.043107986450195
INFO - 03/11/25 12:45:17 - 1:11:33 - Epoch 444: train_loss=13.043574333190918
INFO - 03/11/25 12:45:19 - 1:11:34 - Epoch 445: train_loss=13.04468822479248
INFO - 03/11/25 12:45:20 - 1:11:35 - Epoch 446: train_loss=13.04072380065918
INFO - 03/11/25 12:45:22 - 1:11:37 - Epoch 447: train_loss=13.052724838256836
INFO - 03/11/25 12:45:23 - 1:11:38 - Epoch 448: train_loss=13.049782752990723
INFO - 03/11/25 12:45:24 - 1:11:39 - Epoch 449: train_loss=13.044363975524902
INFO - 03/11/25 12:45:26 - 1:11:41 - Epoch 450: train_loss=13.045944213867188
INFO - 03/11/25 12:45:27 - 1:11:42 - Epoch 451: train_loss=13.045083045959473
INFO - 03/11/25 12:45:28 - 1:11:43 - Epoch 452: train_loss=13.04163932800293
INFO - 03/11/25 12:45:30 - 1:11:45 - Epoch 453: train_loss=13.041810035705566
INFO - 03/11/25 12:45:31 - 1:11:46 - Epoch 454: train_loss=13.039051055908203
INFO - 03/11/25 12:45:32 - 1:11:48 - Epoch 455: train_loss=13.045242309570312
INFO - 03/11/25 12:45:34 - 1:11:49 - Epoch 456: train_loss=13.039108276367188
INFO - 03/11/25 12:45:35 - 1:11:50 - Epoch 457: train_loss=13.044881820678711
INFO - 03/11/25 12:45:36 - 1:11:52 - Epoch 458: train_loss=13.041868209838867
INFO - 03/11/25 12:45:38 - 1:11:53 - Epoch 459: train_loss=13.040230751037598
INFO - 03/11/25 12:45:39 - 1:11:54 - Epoch 460: train_loss=13.04602336883545
INFO - 03/11/25 12:45:41 - 1:11:56 - Epoch 461: train_loss=13.038822174072266
INFO - 03/11/25 12:45:42 - 1:11:57 - Epoch 462: train_loss=13.04861831665039
INFO - 03/11/25 12:45:43 - 1:11:58 - Epoch 463: train_loss=13.041982650756836
INFO - 03/11/25 12:45:45 - 1:12:00 - Epoch 464: train_loss=13.046278953552246
INFO - 03/11/25 12:45:46 - 1:12:01 - Epoch 465: train_loss=13.054327011108398
INFO - 03/11/25 12:45:47 - 1:12:03 - Epoch 466: train_loss=13.051466941833496
INFO - 03/11/25 12:45:49 - 1:12:04 - Epoch 467: train_loss=13.053223609924316
INFO - 03/11/25 12:45:50 - 1:12:05 - Epoch 468: train_loss=13.050901412963867
INFO - 03/11/25 12:45:51 - 1:12:07 - Epoch 469: train_loss=13.04952621459961
INFO - 03/11/25 12:45:53 - 1:12:08 - Epoch 470: train_loss=13.04988956451416
INFO - 03/11/25 12:45:54 - 1:12:09 - Epoch 471: train_loss=13.04685115814209
INFO - 03/11/25 12:45:55 - 1:12:11 - Epoch 472: train_loss=13.04626178741455
INFO - 03/11/25 12:45:57 - 1:12:12 - Epoch 473: train_loss=13.048770904541016
INFO - 03/11/25 12:45:58 - 1:12:13 - Epoch 474: train_loss=13.043636322021484
INFO - 03/11/25 12:46:00 - 1:12:15 - Epoch 475: train_loss=13.043878555297852
INFO - 03/11/25 12:46:01 - 1:12:16 - Epoch 476: train_loss=13.039653778076172
INFO - 03/11/25 12:46:02 - 1:12:17 - Epoch 477: train_loss=13.048497200012207
INFO - 03/11/25 12:46:04 - 1:12:19 - Epoch 478: train_loss=13.043444633483887
INFO - 03/11/25 12:46:05 - 1:12:20 - Epoch 479: train_loss=13.047544479370117
INFO - 03/11/25 12:46:06 - 1:12:22 - Epoch 480: train_loss=13.049260139465332
INFO - 03/11/25 12:46:08 - 1:12:23 - Epoch 481: train_loss=13.04082202911377
INFO - 03/11/25 12:46:09 - 1:12:24 - Epoch 482: train_loss=13.047266960144043
INFO - 03/11/25 12:46:10 - 1:12:26 - Epoch 483: train_loss=13.04804801940918
INFO - 03/11/25 12:46:12 - 1:12:27 - Epoch 484: train_loss=13.039691925048828
INFO - 03/11/25 12:46:13 - 1:12:28 - Epoch 485: train_loss=13.043240547180176
INFO - 03/11/25 12:46:15 - 1:12:30 - Epoch 486: train_loss=13.036808013916016
INFO - 03/11/25 12:46:16 - 1:12:31 - Epoch 487: train_loss=13.042106628417969
INFO - 03/11/25 12:46:17 - 1:12:32 - Epoch 488: train_loss=13.04191780090332
INFO - 03/11/25 12:46:19 - 1:12:34 - Epoch 489: train_loss=13.055794715881348
INFO - 03/11/25 12:46:20 - 1:12:35 - Epoch 490: train_loss=13.043058395385742
INFO - 03/11/25 12:46:21 - 1:12:36 - Epoch 491: train_loss=13.049139022827148
INFO - 03/11/25 12:46:23 - 1:12:38 - Epoch 492: train_loss=13.047603607177734
INFO - 03/11/25 12:46:24 - 1:12:39 - Epoch 493: train_loss=13.043524742126465
INFO - 03/11/25 12:46:25 - 1:12:41 - Epoch 494: train_loss=13.049837112426758
INFO - 03/11/25 12:46:27 - 1:12:42 - Epoch 495: train_loss=13.0498628616333
INFO - 03/11/25 12:46:28 - 1:12:43 - Epoch 496: train_loss=13.044787406921387
INFO - 03/11/25 12:46:29 - 1:12:45 - Epoch 497: train_loss=13.044950485229492
INFO - 03/11/25 12:46:31 - 1:12:46 - Epoch 498: train_loss=13.046119689941406
INFO - 03/11/25 12:46:32 - 1:12:47 - Epoch 499: train_loss=13.04272747039795
INFO - 03/11/25 12:46:34 - 1:12:49 - Epoch 500: train_loss=13.046246528625488
INFO - 03/11/25 12:46:34 - 1:12:49 - -----------------------Evaluation Start---------------------
INFO - 03/11/25 12:46:35 - 1:12:50 - Decoding cost time:  1.716 s
INFO - 03/11/25 12:46:38 - 1:12:53 - ------------------Saving best model-------------------
INFO - 03/11/25 12:46:38 - 1:12:53 - Epoch 500: ACC: 0.0, NMI: 0.2990934300862232, F1: 0.0, ARI: 0.13771325612181456
INFO - 03/11/25 12:46:38 - 1:12:53 - -------------------------------------------------------------------------
INFO - 03/11/25 12:46:39 - 1:12:54 - Epoch 501: train_loss=13.038206100463867
INFO - 03/11/25 12:46:40 - 1:12:56 - Epoch 502: train_loss=13.051809310913086
INFO - 03/11/25 12:46:42 - 1:12:57 - Epoch 503: train_loss=13.048456192016602
INFO - 03/11/25 12:46:43 - 1:12:58 - Epoch 504: train_loss=13.0491361618042
INFO - 03/11/25 12:46:45 - 1:13:00 - Epoch 505: train_loss=13.051161766052246
INFO - 03/11/25 12:46:46 - 1:13:01 - Epoch 506: train_loss=13.06622314453125
INFO - 03/11/25 12:46:47 - 1:13:02 - Epoch 507: train_loss=13.04355239868164
INFO - 03/11/25 12:46:49 - 1:13:04 - Epoch 508: train_loss=13.068634033203125
INFO - 03/11/25 12:46:50 - 1:13:05 - Epoch 509: train_loss=13.06179141998291
INFO - 03/11/25 12:46:51 - 1:13:07 - Epoch 510: train_loss=13.053144454956055
INFO - 03/11/25 12:46:53 - 1:13:08 - Epoch 511: train_loss=13.063521385192871
INFO - 03/11/25 12:46:54 - 1:13:09 - Epoch 512: train_loss=13.058304786682129
INFO - 03/11/25 12:46:55 - 1:13:11 - Epoch 513: train_loss=13.04598331451416
INFO - 03/11/25 12:46:57 - 1:13:12 - Epoch 514: train_loss=13.049800872802734
INFO - 03/11/25 12:46:58 - 1:13:13 - Epoch 515: train_loss=13.062027931213379
INFO - 03/11/25 12:47:00 - 1:13:15 - Epoch 516: train_loss=13.04963207244873
INFO - 03/11/25 12:47:01 - 1:13:16 - Epoch 517: train_loss=13.05012035369873
INFO - 03/11/25 12:47:02 - 1:13:17 - Epoch 518: train_loss=13.04736614227295
INFO - 03/11/25 12:47:04 - 1:13:19 - Epoch 519: train_loss=13.05108642578125
INFO - 03/11/25 12:47:05 - 1:13:20 - Epoch 520: train_loss=13.048176765441895
INFO - 03/11/25 12:47:06 - 1:13:21 - Epoch 521: train_loss=13.04289722442627
INFO - 03/11/25 12:47:08 - 1:13:23 - Epoch 522: train_loss=13.044310569763184
INFO - 03/11/25 12:47:09 - 1:13:24 - Epoch 523: train_loss=13.038847923278809
INFO - 03/11/25 12:47:10 - 1:13:26 - Epoch 524: train_loss=13.0601167678833
INFO - 03/11/25 12:47:12 - 1:13:27 - Epoch 525: train_loss=13.050989151000977
INFO - 03/11/25 12:47:13 - 1:13:28 - Epoch 526: train_loss=13.041749000549316
INFO - 03/11/25 12:47:14 - 1:13:30 - Epoch 527: train_loss=13.042780876159668
INFO - 03/11/25 12:47:16 - 1:13:31 - Epoch 528: train_loss=13.053525924682617
INFO - 03/11/25 12:47:17 - 1:13:32 - Epoch 529: train_loss=13.04898452758789
INFO - 03/11/25 12:47:19 - 1:13:34 - Epoch 530: train_loss=13.048799514770508
INFO - 03/11/25 12:47:20 - 1:13:35 - Epoch 531: train_loss=13.049139976501465
INFO - 03/11/25 12:47:21 - 1:13:36 - Epoch 532: train_loss=13.052746772766113
INFO - 03/11/25 12:47:23 - 1:13:38 - Epoch 533: train_loss=13.050606727600098
INFO - 03/11/25 12:47:24 - 1:13:39 - Epoch 534: train_loss=13.053072929382324
INFO - 03/11/25 12:47:25 - 1:13:41 - Epoch 535: train_loss=13.056180953979492
INFO - 03/11/25 12:47:27 - 1:13:42 - Epoch 536: train_loss=13.048591613769531
INFO - 03/11/25 12:47:28 - 1:13:43 - Epoch 537: train_loss=13.061674118041992
INFO - 03/11/25 12:47:29 - 1:13:45 - Epoch 538: train_loss=13.051685333251953
INFO - 03/11/25 12:47:31 - 1:13:46 - Epoch 539: train_loss=13.05807876586914
INFO - 03/11/25 12:47:32 - 1:13:47 - Epoch 540: train_loss=13.057829856872559
INFO - 03/11/25 12:47:33 - 1:13:49 - Epoch 541: train_loss=13.062846183776855
INFO - 03/11/25 12:47:35 - 1:13:50 - Epoch 542: train_loss=13.058451652526855
INFO - 03/11/25 12:47:36 - 1:13:51 - Epoch 543: train_loss=13.064123153686523
INFO - 03/11/25 12:47:38 - 1:13:53 - Epoch 544: train_loss=13.054611206054688
INFO - 03/11/25 12:47:39 - 1:13:54 - Epoch 545: train_loss=13.056766510009766
INFO - 03/11/25 12:47:40 - 1:13:55 - Epoch 546: train_loss=13.05068302154541
INFO - 03/11/25 12:47:42 - 1:13:57 - Epoch 547: train_loss=13.065105438232422
INFO - 03/11/25 12:47:43 - 1:13:58 - Epoch 548: train_loss=13.057411193847656
INFO - 03/11/25 12:47:44 - 1:14:00 - Epoch 549: train_loss=13.05229377746582
INFO - 03/11/25 12:47:46 - 1:14:01 - Epoch 550: train_loss=13.069180488586426
INFO - 03/11/25 12:47:47 - 1:14:02 - Epoch 551: train_loss=13.064523696899414
INFO - 03/11/25 12:47:48 - 1:14:04 - Epoch 552: train_loss=13.057990074157715
INFO - 03/11/25 12:47:50 - 1:14:05 - Epoch 553: train_loss=13.055901527404785
INFO - 03/11/25 12:47:51 - 1:14:06 - Epoch 554: train_loss=13.057522773742676
INFO - 03/11/25 12:47:52 - 1:14:08 - Epoch 555: train_loss=13.062003135681152
INFO - 03/11/25 12:47:54 - 1:14:09 - Epoch 556: train_loss=13.075645446777344
INFO - 03/11/25 12:47:55 - 1:14:10 - Epoch 557: train_loss=13.109686851501465
INFO - 03/11/25 12:47:57 - 1:14:12 - Epoch 558: train_loss=13.081488609313965
INFO - 03/11/25 12:47:58 - 1:14:13 - Epoch 559: train_loss=13.081818580627441
INFO - 03/11/25 12:47:59 - 1:14:14 - Epoch 560: train_loss=13.081269264221191
INFO - 03/11/25 12:48:01 - 1:14:16 - Epoch 561: train_loss=13.099889755249023
INFO - 03/11/25 12:48:02 - 1:14:17 - Epoch 562: train_loss=13.092650413513184
INFO - 03/11/25 12:48:03 - 1:14:19 - Epoch 563: train_loss=13.101633071899414
INFO - 03/11/25 12:48:05 - 1:14:20 - Epoch 564: train_loss=13.099617958068848
INFO - 03/11/25 12:48:06 - 1:14:21 - Epoch 565: train_loss=13.083917617797852
INFO - 03/11/25 12:48:07 - 1:14:23 - Epoch 566: train_loss=13.10738754272461
INFO - 03/11/25 12:48:09 - 1:14:24 - Epoch 567: train_loss=13.105607986450195
INFO - 03/11/25 12:48:10 - 1:14:25 - Epoch 568: train_loss=13.095574378967285
INFO - 03/11/25 12:48:12 - 1:14:27 - Epoch 569: train_loss=13.10932445526123
INFO - 03/11/25 12:48:13 - 1:14:28 - Epoch 570: train_loss=13.114253044128418
INFO - 03/11/25 12:48:14 - 1:14:29 - Epoch 571: train_loss=13.111200332641602
INFO - 03/11/25 12:48:16 - 1:14:31 - Epoch 572: train_loss=13.103523254394531
INFO - 03/11/25 12:48:17 - 1:14:32 - Epoch 573: train_loss=13.108617782592773
INFO - 03/11/25 12:48:18 - 1:14:34 - Epoch 574: train_loss=13.108397483825684
INFO - 03/11/25 12:48:20 - 1:14:35 - Epoch 575: train_loss=13.094075202941895
INFO - 03/11/25 12:48:21 - 1:14:36 - Epoch 576: train_loss=13.1101713180542
INFO - 03/11/25 12:48:22 - 1:14:38 - Epoch 577: train_loss=13.112747192382812
INFO - 03/11/25 12:48:24 - 1:14:39 - Epoch 578: train_loss=13.11235523223877
INFO - 03/11/25 12:48:25 - 1:14:40 - Epoch 579: train_loss=13.107020378112793
INFO - 03/11/25 12:48:26 - 1:14:42 - Epoch 580: train_loss=13.105554580688477
INFO - 03/11/25 12:48:28 - 1:14:43 - Epoch 581: train_loss=13.112994194030762
INFO - 03/11/25 12:48:29 - 1:14:44 - Epoch 582: train_loss=13.1331787109375
INFO - 03/11/25 12:48:31 - 1:14:46 - Epoch 583: train_loss=13.104063034057617
INFO - 03/11/25 12:48:32 - 1:14:47 - Epoch 584: train_loss=13.097662925720215
INFO - 03/11/25 12:48:33 - 1:14:48 - Epoch 585: train_loss=13.097477912902832
INFO - 03/11/25 12:48:35 - 1:14:50 - Epoch 586: train_loss=13.095534324645996
INFO - 03/11/25 12:48:36 - 1:14:51 - Epoch 587: train_loss=13.07801342010498
INFO - 03/11/25 12:48:37 - 1:14:53 - Epoch 588: train_loss=nan
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [347,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [347,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [347,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [347,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [64,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [65,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [66,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [67,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [68,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [69,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [70,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [71,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [72,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [73,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [74,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [75,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [76,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [77,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [78,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [79,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [80,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [81,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [82,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [83,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [84,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [85,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [86,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [87,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [88,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [89,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [90,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [91,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [92,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [93,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [94,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [48,0,0], thread: [95,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [164,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [64,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [65,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [66,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [67,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [68,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [69,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [70,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [71,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [72,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [73,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [74,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [75,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [76,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [77,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [78,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [79,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [80,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [81,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [82,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [83,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [84,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [85,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [86,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [87,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [88,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [89,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [90,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [91,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [92,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [93,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [94,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [171,0,0], thread: [95,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [22,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [25,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [27,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [150,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [56,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [84,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.
WARNING - 03/11/25 12:48:38 - 1:14:54 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:38 - 1:14:54 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 55, in train
                                            nmi, ari = self.train_clu(data, model, optimizer, logger, device, exp_iter)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 96, in train_clu
                                            loss.backward()
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/_tensor.py", line 626, in backward
                                            torch.autograd.backward(
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
                                            _engine_run_backward(
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
                                            return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:41 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:41 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:42 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:42 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:44 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:44 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:45 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:45 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:47 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:47 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:48 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:48 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:50 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:50 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:51 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:51 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:53 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:53 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:54 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:54 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:56 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:56 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:57 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:57 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:48:59 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:48:59 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:00 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:00 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:02 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:02 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:03 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:03 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:06 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:06 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:09 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:09 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:13 - 0:00:02 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:13 - 0:00:02 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:15 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:15 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:17 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:17 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:18 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:18 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:20 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:20 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:22 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:22 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:23 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:23 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:31 - 0:00:07 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:31 - 0:00:07 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:33 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:33 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:34 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:34 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:36 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:36 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:37 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:37 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:39 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:39 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:40 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:40 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:42 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:42 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:43 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:43 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:45 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:45 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:46 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:46 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:48 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:48 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:49 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:49 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:50 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:50 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:52 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:52 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:53 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:53 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:55 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:55 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:56 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:56 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:58 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:58 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:49:59 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:49:59 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:01 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:01 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:02 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:02 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
INFO - 03/11/25 12:50:02 - 0:00:01 - Finished 50 trials.
WARNING - 03/11/25 12:50:04 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:04 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:05 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:05 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:07 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:07 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:08 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:08 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:10 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:10 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:11 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:11 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:13 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:13 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:14 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:14 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:22 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:22 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:23 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:23 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:25 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:25 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:26 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:26 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:28 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:28 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:29 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:29 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:31 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:31 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:32 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:32 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:34 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:34 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:35 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:35 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:37 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:37 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:38 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:38 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:40 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:40 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:41 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:41 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:43 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:43 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:44 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:44 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:45 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:45 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:47 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:47 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:48 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:48 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:50 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:50 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:51 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:51 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:53 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:53 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:54 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:54 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:56 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:56 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:57 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:57 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:50:59 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:50:59 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:00 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:00 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:02 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:02 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:03 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:03 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:05 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:05 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:12 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:12 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:14 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:14 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:15 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:15 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:17 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:17 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:18 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:18 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:20 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:20 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:21 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:21 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:23 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:23 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:24 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:24 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:26 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:26 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:27 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:27 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:29 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:29 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
INFO - 03/11/25 12:51:29 - 0:00:01 - Finished 100 trials.
WARNING - 03/11/25 12:51:30 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:30 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:32 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:32 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:33 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:33 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:35 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:35 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:36 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:36 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:38 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:38 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:39 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:39 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:41 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:41 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:42 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:42 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:44 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:44 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:45 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:45 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:46 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:46 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:48 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:48 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:49 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:49 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:51 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:51 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:52 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:52 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:54 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:54 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:51:55 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:51:55 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:03 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:03 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:05 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:05 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:06 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:06 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:08 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:08 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:09 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:09 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:11 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:11 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:12 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:12 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:14 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:14 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:15 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:15 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:17 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:17 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:18 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:18 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:20 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:20 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:21 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:21 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:23 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:23 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:24 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:24 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:25 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:25 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:27 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:27 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:28 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:28 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:30 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:30 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:31 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:31 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:33 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:33 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:34 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:34 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:36 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:36 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:37 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:37 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:39 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:39 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:40 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:40 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:42 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:42 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:43 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:43 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:45 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:45 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:46 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:46 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:52:54 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:54 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:4779: RuntimeWarning: invalid value encountered in subtract
  diff_b_a = subtract(b, a)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
WARNING - 03/11/25 12:52:58 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:58 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
INFO - 03/11/25 12:52:58 - 0:00:01 - Finished 150 trials.
WARNING - 03/11/25 12:52:59 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:52:59 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:01 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:01 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:02 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:02 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:04 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:04 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:05 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:05 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:07 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:07 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:08 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:08 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:10 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:10 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:11 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:11 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:13 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:13 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:14 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:14 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:16 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:16 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:17 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:17 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:18 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:18 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:20 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:20 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:21 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:21 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:23 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:23 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:24 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:24 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:26 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:26 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:27 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:27 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:29 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:29 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:30 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:30 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:32 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:32 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:4779: RuntimeWarning: invalid value encountered in subtract
  diff_b_a = subtract(b, a)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
WARNING - 03/11/25 12:53:36 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:36 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:37 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:37 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:39 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:39 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:46 - 0:00:08 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:46 - 0:00:08 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:48 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:48 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:49 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:49 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:51 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:51 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:52 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:52 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:54 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:54 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:55 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:55 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:57 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:57 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:53:58 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:53:58 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:00 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:00 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:01 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:01 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:03 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:03 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:04 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:04 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:06 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:06 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:07 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:07 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:09 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:09 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:10 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:10 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:12 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:12 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:13 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:13 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:14 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:14 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:16 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:16 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:4779: RuntimeWarning: invalid value encountered in subtract
  diff_b_a = subtract(b, a)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/model/random_forest/random_forest.py:222: RuntimeWarning: Mean of empty slice
  preds_as_array = np.log(np.nanmean(np.exp(preds_as_array), axis=2) + VERY_SMALL_NUMBER)
WARNING - 03/11/25 12:54:17 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:17 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:19 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:19 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
WARNING - 03/11/25 12:54:20 - 0:00:01 - Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.
WARNING - 03/11/25 12:54:20 - 0:00:01 - Traceback: Traceback (most recent call last):
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 190, in run
                                            rval = self(config_copy, target_function, kwargs)
                                          File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/site-packages/smac/runner/target_function_runner.py", line 264, in __call__
                                            return algorithm(config, **algorithm_kwargs)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 103, in train
                                            ari = exp.train()
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 36, in train
                                            self.send_device(data)
                                          File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/exp.py", line 30, in send_device
                                            data[k] = v.to(self.device)
                                        RuntimeError: CUDA error: device-side assert triggered
                                        CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
                                        For debugging consider passing CUDA_LAUNCH_BLOCKING=1
                                        Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
                                        
                                        
                                        
INFO - 03/11/25 12:54:20 - 0:00:01 - Finished 200 trials.
INFO - 03/11/25 12:54:20 - 0:00:01 - Configuration budget is exhausted:
INFO - 03/11/25 12:54:20 - 0:00:01 - --- Remaining wallclock time: inf
INFO - 03/11/25 12:54:20 - 0:00:01 - --- Remaining cpu time: inf
INFO - 03/11/25 12:54:20 - 0:00:01 - --- Remaining trials: 0
Traceback (most recent call last):
  File "/home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/./main_with_smac.py", line 118, in <module>
    json.dump(dict(incumbent), fp)
  File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/json/__init__.py", line 179, in dump
    for chunk in iterable:
  File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/json/encoder.py", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/json/encoder.py", line 438, in _iterencode
    o = _default(o)
  File "/home/hpc/v100dd/v100dd15/.conda/envs/clustering_env/lib/python3.9/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
=== JOB_STATISTICS ===
=== current date     : Tue Mar 11 12:54:34 CET 2025
= Job-ID             : 2448128 on alex
= Job-Name           : lsenet_hpo
= Job-Command        : /home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering/fau_alex_job_script_hpo.sh
= Initial workdir    : /home/atuin/v100dd/v100dd15/python_code/deep_graph_clustering/deep_graph_clustering
= Queue/Partition    : a100
= Slurm account      : v100dd with QOS=normal
= Features           : a100_80
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 03:21:10
= Total RAM usage    : 8.7 GiB of assigned  GiB (%)
= Node list          : a0931
= Subm/Elig/Start/End: 2025-03-11T09:33:19 / 2025-03-11T09:33:19 / 2025-03-11T09:33:23 / 2025-03-11T12:54:33
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           30.1G   104.9G   209.7G        N/A      51K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-80GB, 00000000:0E:00.0, 21469, 70 %, 14 %, 81138 MiB, 12058031 ms
